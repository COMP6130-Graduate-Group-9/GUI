{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The notebook contains\n",
    "### Code for _Bulyan_ aggregation algorithm, *when gradient updates of benign clients are unknown to adversary*\n",
    "### Evaluation of all the attacks (Fang, LIE, and our AGR-agnstic) on Multi-krum, except our AGR-tailored attack on Bulyan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse, os, sys, csv, shutil, time, random, operator, pickle, ast, math, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.optim import Optimizer\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "sys.path.insert(0,'./../utils/')\n",
    "from logger import *\n",
    "from eval import *\n",
    "from misc import *\n",
    "\n",
    "from femnist_normal_train import *\n",
    "from femnist_util import *\n",
    "from adam import Adam\n",
    "from sgd import SGD\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the FEMNIST dataset; we use [LEAF framework](https://leaf.cmu.edu/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_tr_data = []\n",
    "user_tr_labels = []\n",
    "\n",
    "for i in range(34):\n",
    "    f = '/mnt/nfs/work1/amir/vshejwalkar/leaf/data/femnist/data/train/all_data_%d_niid_0_keep_0_train_9.json'%i\n",
    "    with open(f, 'r') as myfile:\n",
    "        data=myfile.read()\n",
    "    obj = json.loads(data)\n",
    "    \n",
    "    for user in obj['users']:\n",
    "        user_tr_data.append(obj['user_data'][user]['x'])\n",
    "        user_tr_labels.append(obj['user_data'][user]['y'])\n",
    "\n",
    "user_te_data = []\n",
    "user_te_labels = []\n",
    "\n",
    "for i in range(34):\n",
    "    f = '/mnt/nfs/work1/amir/vshejwalkar/leaf/data/femnist/data/test/all_data_%d_niid_0_keep_0_test_9.json'%i\n",
    "    with open(f, 'r') as myfile:\n",
    "        data=myfile.read()\n",
    "    obj = json.loads(data)\n",
    "    \n",
    "    for user in obj['users']:\n",
    "        user_te_data.append(obj['user_data'][user]['x'])\n",
    "        user_te_labels.append(obj['user_data'][user]['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of clients:  3400\n"
     ]
    }
   ],
   "source": [
    "user_tr_data_tensors=[]\n",
    "user_tr_label_tensors=[]\n",
    "\n",
    "for i in range(len(user_tr_data)):\n",
    "    \n",
    "    user_tr_data_tensor=torch.from_numpy(np.array(user_tr_data[i])).type(torch.FloatTensor)\n",
    "    user_tr_label_tensor=torch.from_numpy(np.array(user_tr_labels[i])).type(torch.LongTensor)\n",
    "\n",
    "    user_tr_data_tensors.append(user_tr_data_tensor)\n",
    "    user_tr_label_tensors.append(user_tr_label_tensor)\n",
    "    \n",
    "    # print('user %d tr len %d'%(i,len(user_tr_data_tensor)))\n",
    "print(\"number of clients: \", len(user_tr_data_tensors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_data = np.concatenate(user_te_data, 0)\n",
    "te_labels = np.concatenate(user_te_labels)\n",
    "te_len = len(te_labels)\n",
    "\n",
    "te_data_tensor = torch.from_numpy(te_data[:(te_len//2)]).type(torch.FloatTensor)\n",
    "te_label_tensor = torch.from_numpy(te_labels[:(te_len//2)]).type(torch.LongTensor)\n",
    "\n",
    "val_data_tensor = torch.from_numpy(te_data[(te_len//2):]).type(torch.FloatTensor)\n",
    "val_label_tensor = torch.from_numpy(te_labels[(te_len//2):]).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Bulyan aggregation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulyan(all_updates, n_attackers):\n",
    "    nusers = all_updates.shape[0]\n",
    "    bulyan_cluster = []\n",
    "    candidate_indices = []\n",
    "    remaining_updates = all_updates\n",
    "    all_indices = np.arange(len(all_updates))\n",
    "\n",
    "    while len(bulyan_cluster) < (nusers - 2 * n_attackers):\n",
    "        distances = []\n",
    "        for update in remaining_updates:\n",
    "            distance = torch.norm((remaining_updates - update), dim=1) ** 2\n",
    "            distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)\n",
    "\n",
    "        distances = torch.sort(distances, dim=1)[0]\n",
    "\n",
    "        scores = torch.sum(distances[:, :len(remaining_updates) - 2 - n_attackers], dim=1)\n",
    "        indices = torch.argsort(scores)[:len(remaining_updates) - 2 - n_attackers]\n",
    "\n",
    "        candidate_indices.append(all_indices[indices[0].cpu().numpy()])\n",
    "        all_indices = np.delete(all_indices, indices[0].cpu().numpy())\n",
    "        bulyan_cluster = remaining_updates[indices[0]][None, :] if not len(bulyan_cluster) else torch.cat((bulyan_cluster, remaining_updates[indices[0]][None, :]), 0)\n",
    "        remaining_updates = torch.cat((remaining_updates[:indices[0]], remaining_updates[indices[0] + 1:]), 0)\n",
    "\n",
    "    # print('dim of bulyan cluster ', bulyan_cluster.shape)\n",
    "\n",
    "    n, d = bulyan_cluster.shape\n",
    "    param_med = torch.median(bulyan_cluster, dim=0)[0]\n",
    "    sort_idx = torch.argsort(torch.abs(bulyan_cluster - param_med), dim=0)\n",
    "    sorted_params = bulyan_cluster[sort_idx, torch.arange(d)[None, :]]\n",
    "\n",
    "    return torch.mean(sorted_params[:n - 2 * n_attackers], dim=0), np.array(candidate_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Multi-krum aggregation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_krum(all_updates, n_attackers, multi_k=False):\n",
    "    nusers = all_updates.shape[0]\n",
    "    candidates = []\n",
    "    candidate_indices = []\n",
    "    remaining_updates = all_updates\n",
    "    all_indices = np.arange(len(all_updates))\n",
    "\n",
    "    while len(remaining_updates) > 2 * n_attackers + 2:\n",
    "        distances = []\n",
    "        for update in remaining_updates:\n",
    "            distance = torch.norm((remaining_updates - update), dim=1) ** 2\n",
    "            distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)\n",
    "\n",
    "        distances = torch.sort(distances, dim=1)[0]\n",
    "        scores = torch.sum(distances[:, :len(remaining_updates) - 2 - n_attackers], dim=1)\n",
    "        indices = torch.argsort(scores)[:len(remaining_updates) - 2 - n_attackers]\n",
    "\n",
    "        candidate_indices.append(all_indices[indices[0].cpu().numpy()])\n",
    "        all_indices = np.delete(all_indices, indices[0].cpu().numpy())\n",
    "        candidates = remaining_updates[indices[0]][None, :] if not len(candidates) else torch.cat((candidates, remaining_updates[indices[0]][None, :]), 0)\n",
    "        remaining_updates = torch.cat((remaining_updates[:indices[0]], remaining_updates[indices[0] + 1:]), 0)\n",
    "        if not multi_k:\n",
    "            break\n",
    "    # print(len(remaining_updates))\n",
    "    aggregate = torch.mean(candidates, dim=0)\n",
    "    return aggregate, np.array(candidate_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Fang attack on Bulyan\n",
    "### Fang attacks on MKrum and Bulyan are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lambda(all_updates, model_re, n_attackers):\n",
    "\n",
    "    distances = []\n",
    "    n_benign, d = all_updates.shape\n",
    "    for update in all_updates:\n",
    "        distance = torch.norm((all_updates - update), dim=1)\n",
    "        distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)\n",
    "\n",
    "    distances[distances == 0] = 10000\n",
    "    distances = torch.sort(distances, dim=1)[0]\n",
    "    scores = torch.sum(distances[:, :n_benign - 2 - n_attackers], dim=1)\n",
    "    min_score = torch.min(scores)\n",
    "    term_1 = min_score / ((n_benign - n_attackers - 1) * torch.sqrt(torch.Tensor([d]))[0])\n",
    "    max_wre_dist = torch.max(torch.norm((all_updates - model_re), dim=1)) / (torch.sqrt(torch.Tensor([d]))[0])\n",
    "\n",
    "    return (term_1 + max_wre_dist)\n",
    "\n",
    "\n",
    "def get_malicious_updates_fang(all_updates, model_re, deviation, n_attackers):\n",
    "\n",
    "    lamda = compute_lambda(all_updates, model_re, n_attackers)\n",
    "\n",
    "    threshold = 1e-5\n",
    "    mal_update = []\n",
    "\n",
    "    while lamda > threshold:\n",
    "        mal_update = (-lamda * deviation)\n",
    "        mal_updates = torch.stack([mal_update] * n_attackers)\n",
    "        mal_updates = torch.cat((mal_updates, all_updates), 0)\n",
    "\n",
    "        # print(mal_updates.shape, n_attackers)\n",
    "        agg_grads, krum_candidate = multi_krum(mal_updates, n_attackers, multi_k=False)\n",
    "        if krum_candidate < n_attackers:\n",
    "            # print('successful lamda is ', lamda)\n",
    "            return mal_update\n",
    "        else:\n",
    "            mal_update = []\n",
    "\n",
    "        lamda *= 0.5\n",
    "\n",
    "    if not len(mal_update):\n",
    "        mal_update = (model_re - lamda * deviation)\n",
    "        \n",
    "    return mal_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Initializing global model\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 0 fed_model val loss 4.0347 val acc 5.1457 best val_acc 5.145696 te_acc 5.397961\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 9 e 20 fed_model val loss 3.8071 val acc 16.5852 best val_acc 19.836285 te_acc 22.371808\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 40 fed_model val loss 3.4244 val acc 27.2086 best val_acc 28.467360 te_acc 31.576915\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 60 fed_model val loss 3.0778 val acc 27.1031 best val_acc 30.905066 te_acc 34.480540\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 80 fed_model val loss 2.7368 val acc 33.3170 best val_acc 33.317030 te_acc 36.771520\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 100 fed_model val loss 2.6942 val acc 34.3029 best val_acc 35.124073 te_acc 38.992998\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 120 fed_model val loss 2.4517 val acc 38.1976 best val_acc 38.753604 te_acc 42.787273\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 140 fed_model val loss 2.3466 val acc 40.4551 best val_acc 42.290465 te_acc 45.899403\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 160 fed_model val loss 2.2778 val acc 41.3149 best val_acc 45.085976 te_acc 48.154345\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 180 fed_model val loss 2.1935 val acc 44.5016 best val_acc 45.572488 te_acc 47.670408\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 200 fed_model val loss 2.0596 val acc 44.9985 best val_acc 48.352554 te_acc 50.790259\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 220 fed_model val loss 2.0734 val acc 47.2277 best val_acc 48.352554 te_acc 50.790259\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 240 fed_model val loss 1.8974 val acc 51.3849 best val_acc 51.384885 te_acc 52.697694\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 260 fed_model val loss 2.0065 val acc 48.6203 best val_acc 51.384885 te_acc 52.697694\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 280 fed_model val loss 1.8546 val acc 51.7710 best val_acc 54.775021 te_acc 55.966845\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 300 fed_model val loss 1.7200 val acc 55.2126 best val_acc 56.008031 te_acc 56.896108\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 320 fed_model val loss 1.6825 val acc 55.0762 best val_acc 57.851112 te_acc 58.970861\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 340 fed_model val loss 1.6001 val acc 58.5822 best val_acc 58.641371 te_acc 59.542319\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 360 fed_model val loss 1.5995 val acc 57.9541 best val_acc 59.133031 te_acc 59.990218\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 380 fed_model val loss 1.7992 val acc 52.7234 best val_acc 59.133031 te_acc 59.990218\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 400 fed_model val loss 1.6160 val acc 58.6388 best val_acc 59.133031 te_acc 59.990218\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 420 fed_model val loss 1.6210 val acc 57.5293 best val_acc 59.426483 te_acc 60.790774\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 440 fed_model val loss 1.5986 val acc 59.0790 best val_acc 59.426483 te_acc 60.790774\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 460 fed_model val loss 1.6732 val acc 57.2797 best val_acc 60.162685 te_acc 60.875721\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 480 fed_model val loss 1.5587 val acc 58.3325 best val_acc 60.456137 te_acc 61.303027\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 500 fed_model val loss 1.5050 val acc 59.7508 best val_acc 61.686573 te_acc 62.641577\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 520 fed_model val loss 1.5959 val acc 60.3403 best val_acc 62.644152 te_acc 63.624897\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 540 fed_model val loss 1.5091 val acc 62.0624 best val_acc 62.644152 te_acc 63.624897\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 560 fed_model val loss 1.4505 val acc 62.3867 best val_acc 62.644152 te_acc 63.624897\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 580 fed_model val loss 1.4800 val acc 60.9195 best val_acc 62.644152 te_acc 63.624897\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 600 fed_model val loss 1.4988 val acc 60.8706 best val_acc 62.644152 te_acc 63.624897\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 620 fed_model val loss 1.4558 val acc 62.0984 best val_acc 62.644152 te_acc 63.624897\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 640 fed_model val loss 1.6922 val acc 57.6040 best val_acc 62.644152 te_acc 63.624897\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 660 fed_model val loss 1.5268 val acc 61.4961 best val_acc 62.986512 te_acc 64.062500\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 680 fed_model val loss 1.7190 val acc 58.5744 best val_acc 62.986512 te_acc 64.062500\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 700 fed_model val loss 1.5699 val acc 60.5591 best val_acc 62.986512 te_acc 64.062500\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 720 fed_model val loss 1.5705 val acc 59.0481 best val_acc 62.999382 te_acc 63.841124\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 740 fed_model val loss 1.5565 val acc 60.0185 best val_acc 63.709843 te_acc 64.471787\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 760 fed_model val loss 1.6492 val acc 56.8240 best val_acc 64.278727 te_acc 65.020078\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 780 fed_model val loss 1.4453 val acc 62.1345 best val_acc 64.278727 te_acc 65.020078\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 800 fed_model val loss 1.3661 val acc 64.2762 best val_acc 64.314765 te_acc 65.437088\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 820 fed_model val loss 1.3913 val acc 63.8154 best val_acc 64.399712 te_acc 65.367586\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 840 fed_model val loss 1.3991 val acc 62.9093 best val_acc 64.399712 te_acc 65.367586\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 860 fed_model val loss 1.4934 val acc 60.9684 best val_acc 64.399712 te_acc 65.367586\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 880 fed_model val loss 1.6271 val acc 60.0160 best val_acc 64.399712 te_acc 65.367586\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 900 fed_model val loss 1.4101 val acc 63.0740 best val_acc 64.399712 te_acc 65.367586\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 920 fed_model val loss 1.3674 val acc 63.7356 best val_acc 65.200268 te_acc 66.379222\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 940 fed_model val loss 1.3428 val acc 64.6314 best val_acc 65.449959 te_acc 66.783361\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 960 fed_model val loss 1.3295 val acc 64.7138 best val_acc 65.449959 te_acc 66.783361\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 980 fed_model val loss 1.3349 val acc 64.5773 best val_acc 65.449959 te_acc 66.783361\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 1000 fed_model val loss 1.3570 val acc 64.6082 best val_acc 65.449959 te_acc 66.783361\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 1020 fed_model val loss 1.3821 val acc 64.1577 best val_acc 65.449959 te_acc 66.783361\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 1040 fed_model val loss 1.3740 val acc 64.2144 best val_acc 65.449959 te_acc 66.783361\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 1060 fed_model val loss 1.4097 val acc 63.6069 best val_acc 65.457681 te_acc 66.469316\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 1080 fed_model val loss 1.4988 val acc 60.8577 best val_acc 65.457681 te_acc 66.469316\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 1100 fed_model val loss 1.3861 val acc 64.0857 best val_acc 65.457681 te_acc 66.469316\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 1120 fed_model val loss 1.4221 val acc 63.0148 best val_acc 65.457681 te_acc 66.469316\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 1140 fed_model val loss 1.4797 val acc 61.9672 best val_acc 65.457681 te_acc 66.469316\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 1160 fed_model val loss 1.5232 val acc 61.2104 best val_acc 65.457681 te_acc 66.469316\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 1180 fed_model val loss 1.5124 val acc 61.7818 best val_acc 65.457681 te_acc 66.469316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 1200 fed_model val loss 1.4577 val acc 62.6442 best val_acc 65.457681 te_acc 66.469316\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 1220 fed_model val loss 1.5330 val acc 60.8912 best val_acc 65.457681 te_acc 66.469316\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 1240 fed_model val loss 1.6787 val acc 58.3093 best val_acc 65.457681 te_acc 66.469316\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 1260 fed_model val loss 1.7443 val acc 57.4727 best val_acc 65.457681 te_acc 66.469316\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 1280 fed_model val loss 1.8586 val acc 55.6914 best val_acc 65.457681 te_acc 66.469316\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 1300 fed_model val loss 2.0829 val acc 52.0619 best val_acc 65.457681 te_acc 66.469316\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 1320 fed_model val loss 2.0442 val acc 52.8393 best val_acc 65.457681 te_acc 66.469316\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 1340 fed_model val loss 2.1063 val acc 51.2150 best val_acc 65.457681 te_acc 66.469316\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 1360 fed_model val loss 2.2055 val acc 50.3655 best val_acc 65.457681 te_acc 66.469316\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 1380 fed_model val loss 2.4042 val acc 48.7387 best val_acc 65.457681 te_acc 66.469316\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 1400 fed_model val loss 2.3890 val acc 47.6601 best val_acc 65.457681 te_acc 66.469316\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 1420 fed_model val loss 2.3627 val acc 48.3345 best val_acc 65.457681 te_acc 66.469316\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 1440 fed_model val loss 2.4515 val acc 47.1144 best val_acc 65.457681 te_acc 66.469316\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 1460 fed_model val loss 2.5685 val acc 45.6703 best val_acc 65.457681 te_acc 66.469316\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 1480 fed_model val loss 2.7207 val acc 44.2288 best val_acc 65.457681 te_acc 66.469316\n",
      "bulyan: at fang at_frac 20.0 n_at 13 n_mal_sel 13 e 1500 fed_model val loss 2.7370 val acc 43.8092 best val_acc 65.457681 te_acc 66.469316\n"
     ]
    }
   ],
   "source": [
    "resume=0\n",
    "nepochs=1500\n",
    "gamma=.1\n",
    "fed_lr=0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "batch_size = 100\n",
    "schedule = [2000]\n",
    "\n",
    "aggregation = 'bulyan'\n",
    "at_type = 'fang'\n",
    "chkpt = './' + aggregation\n",
    "epoch_num = 0\n",
    "\n",
    "at_fractions = [20]\n",
    "\n",
    "for at_fraction in at_fractions:\n",
    "\n",
    "    fed_model = mnist_conv().cuda()\n",
    "    fed_model.apply(weights_init)\n",
    "    optimizer_fed = Adam(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "    print('==> Initializing global model')\n",
    "    epoch_num = 0\n",
    "    n_attacker = 0\n",
    "    best_global_acc=0\n",
    "    best_global_te_acc=0\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        user_grads = []\n",
    "\n",
    "        # The following condition is necessary for Krum/Multi-krum/Bulyan to work\n",
    "        while n_attacker < 4:\n",
    "            round_users = np.random.choice(3400, 60)\n",
    "            n_attacker = np.sum(round_users < (34*at_fraction))\n",
    "\n",
    "        if n_attacker > 14:\n",
    "            print ('n_attackers actual %d adjusted 14' % n_attacker)\n",
    "            n_attacker = 14\n",
    "\n",
    "        at_idx = []\n",
    "        attacker_count = 0\n",
    "        for i in round_users:\n",
    "            if i < (34*at_fraction) and attacker_count < n_attacker:\n",
    "                at_idx.append(i)\n",
    "                attacker_count += 1\n",
    "                continue\n",
    "\n",
    "            inputs = user_tr_data_tensors[i]\n",
    "            targets = user_tr_label_tensors[i]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "            outputs = fed_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            optimizer_fed.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            param_grad=[]\n",
    "            for param in fed_model.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "            user_grads=param_grad[None,:] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]),0)    \n",
    "\n",
    "        if n_attacker > 0:\n",
    "            attacker_grads = []\n",
    "            n_attacker_ = max(1, n_attacker**2//60)\n",
    "            for i in at_idx:\n",
    "\n",
    "                inputs = user_tr_data_tensors[i]\n",
    "                targets = user_tr_label_tensors[i]\n",
    "\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "                outputs = fed_model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                optimizer_fed.zero_grad()\n",
    "                loss.backward(retain_graph=True)\n",
    "\n",
    "                param_grad=[]\n",
    "                for param in fed_model.parameters():\n",
    "                    param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "                attacker_grads=param_grad[None,:] if len(attacker_grads)==0 else torch.cat((attacker_grads,param_grad[None,:]),0)\n",
    "\n",
    "            mal_updates = []\n",
    "            if at_type == 'fang':\n",
    "                agg_grads = torch.mean(attacker_grads, 0)\n",
    "                deviation = torch.sign(agg_grads)\n",
    "                mal_update = get_malicious_updates_fang(attacker_grads, agg_grads, deviation, n_attacker_)\n",
    "\n",
    "            elif at_type == 'our-agr':\n",
    "                agg_grads = torch.mean(attacker_grads, 0)\n",
    "                mal_update = our_attack_mkrum(attacker_grads, agg_grads, n_attacker_, dev_type='sign')\n",
    "            \n",
    "        if not len(mal_updates):\n",
    "            mal_updates = torch.stack([mal_update] * n_attacker)\n",
    "        malicious_grads = torch.cat((mal_updates, user_grads), 0)    \n",
    "        \n",
    "        if malicious_grads.shape[0] != 60: \n",
    "            print('malicious grads shape ', malicious_grads.shape)\n",
    "            sys.exit()\n",
    "\n",
    "        agg_grads, krum_candidate = bulyan(malicious_grads, n_attacker)\n",
    "            \n",
    "        start_idx=0\n",
    "\n",
    "        if epoch_num in schedule:\n",
    "            for param_group in optimizer_fed.param_groups:\n",
    "                param_group['lr'] *= gamma\n",
    "                print('New learnin rate ', param_group['lr'])\n",
    "\n",
    "        optimizer_fed.zero_grad()\n",
    "\n",
    "        model_grads=[]\n",
    "\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "\n",
    "        optimizer_fed.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if is_best:\n",
    "            best_global_te_acc = te_acc\n",
    "\n",
    "        if epoch_num % 20 == 0:\n",
    "            print('%s: at %s at_frac %.1f n_at %d n_mal_sel %d e %d fed_model val loss %.4f val acc %.4f best val_acc %f te_acc %f'%(aggregation, at_type, at_fraction, n_attacker, np.sum(krum_candidate < n_attacker), epoch_num, val_loss, val_acc, best_global_acc,best_global_te_acc))\n",
    "\n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our first AGR-agnostic attack - Min-Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_attack_dist(all_updates, model_re, n_attackers, dev_type='unit_vec'):\n",
    "\n",
    "    if dev_type == 'unit_vec':\n",
    "        deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "    elif dev_type == 'sign':\n",
    "        deviation = torch.sign(model_re)\n",
    "    elif dev_type == 'std':\n",
    "        deviation = torch.std(all_updates, 0)\n",
    "\n",
    "    lamda = torch.Tensor([50.0]).float().cuda()\n",
    "    # print(lamda)\n",
    "    threshold_diff = 1e-5\n",
    "    lamda_fail = lamda\n",
    "    lamda_succ = 0\n",
    "    \n",
    "    distances = []\n",
    "    for update in all_updates:\n",
    "        distance = torch.norm((all_updates - update), dim=1) ** 2\n",
    "        distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)\n",
    "    \n",
    "    max_distance = torch.max(distances)\n",
    "    del distances\n",
    "\n",
    "    while torch.abs(lamda_succ - lamda) > threshold_diff:\n",
    "        mal_update = (model_re - lamda * deviation)\n",
    "        distance = torch.norm((all_updates - mal_update), dim=1) ** 2\n",
    "        max_d = torch.max(distance)\n",
    "        \n",
    "        if max_d <= max_distance:\n",
    "            # print('successful lamda is ', lamda)\n",
    "            lamda_succ = lamda\n",
    "            lamda = lamda + lamda_fail / 2\n",
    "        else:\n",
    "            lamda = lamda - lamda_fail / 2\n",
    "\n",
    "        lamda_fail = lamda_fail / 2\n",
    "\n",
    "    mal_update = (model_re - lamda_succ * deviation)\n",
    "    \n",
    "    return mal_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Initializing global model\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 2 e 0 fed_model val loss 3.9931 val acc 4.6129 best val_acc 4.612850 te_acc 5.369646\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 5 e 20 fed_model val loss 3.8769 val acc 6.8343 best val_acc 7.943781 te_acc 8.700577\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 5 e 40 fed_model val loss 3.8638 val acc 26.1609 best val_acc 26.160935 te_acc 28.995058\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 4 e 60 fed_model val loss 3.5651 val acc 26.7401 best val_acc 29.404345 te_acc 32.822797\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 4 e 80 fed_model val loss 3.0735 val acc 29.2962 best val_acc 31.224259 te_acc 34.686470\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 4 e 100 fed_model val loss 2.7741 val acc 31.4997 best val_acc 33.113674 te_acc 36.686573\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 4 e 120 fed_model val loss 2.5854 val acc 36.8951 best val_acc 36.972302 te_acc 39.731775\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 6 e 140 fed_model val loss 2.6543 val acc 37.4228 best val_acc 39.039333 te_acc 41.953254\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 6 e 160 fed_model val loss 2.4941 val acc 40.9751 best val_acc 41.188736 te_acc 44.064044\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 6 e 180 fed_model val loss 2.5994 val acc 40.6121 best val_acc 42.444914 te_acc 44.841433\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 5 e 200 fed_model val loss 2.3508 val acc 42.6431 best val_acc 43.096170 te_acc 46.427100\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 5 e 220 fed_model val loss 2.1321 val acc 46.6150 best val_acc 46.615012 te_acc 49.768328\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 6 e 240 fed_model val loss 2.1309 val acc 47.0552 best val_acc 49.582990 te_acc 52.278110\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 6 e 260 fed_model val loss 2.0235 val acc 47.0758 best val_acc 50.136429 te_acc 51.989806\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 9 e 280 fed_model val loss 2.3431 val acc 46.2083 best val_acc 50.136429 te_acc 51.989806\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 11 e 300 fed_model val loss 2.2635 val acc 43.0395 best val_acc 50.136429 te_acc 51.989806\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 7 e 320 fed_model val loss 2.0249 val acc 48.8005 best val_acc 50.136429 te_acc 51.989806\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 7 e 340 fed_model val loss 1.9634 val acc 51.2922 best val_acc 52.491763 te_acc 53.763386\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 8 e 360 fed_model val loss 1.9966 val acc 51.1223 best val_acc 53.269152 te_acc 55.601318\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 8 e 380 fed_model val loss 1.7715 val acc 53.5858 best val_acc 54.504736 te_acc 56.358114\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 7 e 400 fed_model val loss 1.8554 val acc 53.2640 best val_acc 55.081343 te_acc 56.906404\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 4 e 420 fed_model val loss 1.6937 val acc 56.6438 best val_acc 57.063427 te_acc 58.921952\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 6 e 440 fed_model val loss 2.2535 val acc 51.6577 best val_acc 57.279654 te_acc 59.138180\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 6 e 460 fed_model val loss 1.8273 val acc 56.5460 best val_acc 57.279654 te_acc 59.138180\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 7 e 480 fed_model val loss 1.7296 val acc 57.9258 best val_acc 57.925762 te_acc 59.148476\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 10 e 500 fed_model val loss 3.9023 val acc 42.6303 best val_acc 57.925762 te_acc 59.148476\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 4 e 520 fed_model val loss 1.8080 val acc 55.0582 best val_acc 57.925762 te_acc 59.148476\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 6 e 540 fed_model val loss 1.6880 val acc 58.5358 best val_acc 58.780375 te_acc 60.600288\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 5 e 560 fed_model val loss 1.6617 val acc 58.9554 best val_acc 59.243719 te_acc 61.084226\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 12 e 580 fed_model val loss 2.1956 val acc 54.0157 best val_acc 59.864086 te_acc 61.246396\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 6 e 600 fed_model val loss 2.4362 val acc 49.6216 best val_acc 59.864086 te_acc 61.246396\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 6 e 620 fed_model val loss 1.6813 val acc 57.5963 best val_acc 59.864086 te_acc 61.246396\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 6 e 640 fed_model val loss 1.6159 val acc 59.5732 best val_acc 60.172982 te_acc 62.142195\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 6 e 660 fed_model val loss 1.6035 val acc 60.0288 best val_acc 60.172982 te_acc 62.142195\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 7 e 680 fed_model val loss 1.6286 val acc 59.5989 best val_acc 60.201297 te_acc 62.458814\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 7 e 700 fed_model val loss 1.7079 val acc 59.0352 best val_acc 60.201297 te_acc 62.458814\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 7 e 720 fed_model val loss 1.8187 val acc 59.0610 best val_acc 60.201297 te_acc 62.458814\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 8 e 740 fed_model val loss 1.8464 val acc 57.9386 best val_acc 60.674938 te_acc 62.463962\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 11 e 760 fed_model val loss 2.5927 val acc 49.0501 best val_acc 60.883443 te_acc 62.345552\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 7 e 780 fed_model val loss 1.6894 val acc 56.7082 best val_acc 60.883443 te_acc 62.345552\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 10 e 800 fed_model val loss 1.7960 val acc 57.0377 best val_acc 60.883443 te_acc 62.345552\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 7 e 820 fed_model val loss 1.6634 val acc 59.5269 best val_acc 60.883443 te_acc 62.345552\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 7 e 840 fed_model val loss 1.5605 val acc 60.5874 best val_acc 61.665980 te_acc 63.323723\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 10 e 860 fed_model val loss 1.6655 val acc 58.5693 best val_acc 61.665980 te_acc 63.323723\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 9 e 880 fed_model val loss 2.5519 val acc 53.5394 best val_acc 61.665980 te_acc 63.323723\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 5 e 900 fed_model val loss 1.6224 val acc 57.6529 best val_acc 61.665980 te_acc 63.323723\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 6 e 920 fed_model val loss 1.6419 val acc 58.9966 best val_acc 61.665980 te_acc 63.323723\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 6 e 940 fed_model val loss 1.5810 val acc 59.9773 best val_acc 61.665980 te_acc 63.323723\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 6 e 960 fed_model val loss 1.5678 val acc 60.3274 best val_acc 61.665980 te_acc 63.323723\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 5 e 980 fed_model val loss 1.5514 val acc 60.7753 best val_acc 61.665980 te_acc 63.323723\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 6 e 1000 fed_model val loss 1.5564 val acc 61.0662 best val_acc 61.665980 te_acc 63.323723\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 6 e 1020 fed_model val loss 1.5451 val acc 60.8551 best val_acc 61.665980 te_acc 63.323723\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 6 e 1040 fed_model val loss 1.5327 val acc 60.6235 best val_acc 61.665980 te_acc 63.323723\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 6 e 1060 fed_model val loss 1.5255 val acc 60.9066 best val_acc 61.665980 te_acc 63.323723\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 7 e 1080 fed_model val loss 1.5816 val acc 60.2348 best val_acc 61.665980 te_acc 63.323723\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 6 e 1100 fed_model val loss 1.5631 val acc 60.5179 best val_acc 61.665980 te_acc 63.323723\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 6 e 1120 fed_model val loss 1.8274 val acc 57.7198 best val_acc 61.665980 te_acc 63.323723\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 5 e 1140 fed_model val loss 1.9526 val acc 52.5407 best val_acc 61.665980 te_acc 63.323723\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 5 e 1160 fed_model val loss 1.7987 val acc 58.3016 best val_acc 61.665980 te_acc 63.323723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 6 e 1180 fed_model val loss 1.5827 val acc 60.6338 best val_acc 61.665980 te_acc 63.323723\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 7 e 1200 fed_model val loss 1.7114 val acc 58.4818 best val_acc 61.665980 te_acc 63.323723\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 7 e 1220 fed_model val loss 1.6701 val acc 58.6542 best val_acc 61.665980 te_acc 63.323723\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 6 e 1240 fed_model val loss 1.6507 val acc 59.6710 best val_acc 61.665980 te_acc 63.323723\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 6 e 1260 fed_model val loss 1.6338 val acc 59.8770 best val_acc 61.665980 te_acc 63.323723\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 7 e 1280 fed_model val loss 1.6091 val acc 60.4021 best val_acc 61.665980 te_acc 63.323723\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 8 e 1320 fed_model val loss 1.7948 val acc 57.9592 best val_acc 61.665980 te_acc 63.323723\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 11 e 1340 fed_model val loss 2.2237 val acc 54.8600 best val_acc 61.665980 te_acc 63.323723\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 9 e 1360 fed_model val loss 2.1658 val acc 54.3323 best val_acc 61.665980 te_acc 63.323723\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 7 e 1380 fed_model val loss 2.1192 val acc 54.4275 best val_acc 61.665980 te_acc 63.323723\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 9 e 1400 fed_model val loss 2.0131 val acc 54.2036 best val_acc 61.665980 te_acc 63.323723\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 6 e 1420 fed_model val loss 1.8042 val acc 57.2900 best val_acc 61.665980 te_acc 63.323723\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 6 e 1440 fed_model val loss 1.7397 val acc 57.8202 best val_acc 61.665980 te_acc 63.323723\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 7 e 1460 fed_model val loss 1.9173 val acc 57.2359 best val_acc 61.665980 te_acc 63.323723\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 7 e 1480 fed_model val loss 1.8827 val acc 57.2282 best val_acc 61.665980 te_acc 63.323723\n",
      "bulyan: at min-max at_frac 20.0 n_at 14 n_mal_sel 7 e 1500 fed_model val loss 1.9507 val acc 58.0519 best val_acc 61.665980 te_acc 63.323723\n"
     ]
    }
   ],
   "source": [
    "resume=0\n",
    "nepochs=1500\n",
    "gamma=.1\n",
    "fed_lr=0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "batch_size = 100\n",
    "schedule = [2000]\n",
    "\n",
    "aggregation = 'bulyan'\n",
    "at_type = 'min-max'\n",
    "chkpt = './' + aggregation\n",
    "epoch_num = 0\n",
    "\n",
    "at_fractions = [20]\n",
    "\n",
    "for at_fraction in at_fractions:\n",
    "\n",
    "    fed_model = mnist_conv().cuda()\n",
    "    fed_model.apply(weights_init)\n",
    "    optimizer_fed = Adam(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "    print('==> Initializing global model')\n",
    "    epoch_num = 0\n",
    "    n_attacker = 0\n",
    "    best_global_acc=0\n",
    "    best_global_te_acc=0\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        user_grads = []\n",
    "\n",
    "        # The following condition is necessary for Krum/Multi-krum/Bulyan to work\n",
    "        while n_attacker < 4:\n",
    "            round_users = np.random.choice(3400, 60)\n",
    "            n_attacker = np.sum(round_users < (34*at_fraction))\n",
    "\n",
    "        if n_attacker > 14:\n",
    "            print ('n_attackers actual %d adjusted 14' % n_attacker)\n",
    "            n_attacker = 14\n",
    "\n",
    "        at_idx = []\n",
    "        attacker_count = 0\n",
    "        for i in round_users:\n",
    "            if i < (34*at_fraction) and attacker_count < n_attacker:\n",
    "                at_idx.append(i)\n",
    "                attacker_count += 1\n",
    "                continue\n",
    "\n",
    "            inputs = user_tr_data_tensors[i]\n",
    "            targets = user_tr_label_tensors[i]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "            outputs = fed_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            optimizer_fed.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            param_grad=[]\n",
    "            for param in fed_model.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "            user_grads=param_grad[None,:] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]),0)    \n",
    "\n",
    "        if n_attacker > 0:\n",
    "            attacker_grads = []\n",
    "            n_attacker_ = max(1, n_attacker**2//60)\n",
    "            for i in at_idx:\n",
    "\n",
    "                inputs = user_tr_data_tensors[i]\n",
    "                targets = user_tr_label_tensors[i]\n",
    "\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "                outputs = fed_model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                optimizer_fed.zero_grad()\n",
    "                loss.backward(retain_graph=True)\n",
    "\n",
    "                param_grad=[]\n",
    "                for param in fed_model.parameters():\n",
    "                    param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "                attacker_grads=param_grad[None,:] if len(attacker_grads)==0 else torch.cat((attacker_grads,param_grad[None,:]),0)\n",
    "\n",
    "            mal_updates = []\n",
    "            if at_type == 'fang':\n",
    "                agg_grads = torch.mean(attacker_grads, 0)\n",
    "                deviation = torch.sign(agg_grads)\n",
    "                mal_update = get_malicious_updates_fang(attacker_grads, agg_grads, deviation, n_attacker_)\n",
    "            elif at_type == 'our-agr':\n",
    "                agg_grads = torch.mean(attacker_grads, 0)\n",
    "                mal_update = our_attack_mkrum(attacker_grads, agg_grads, n_attacker_, dev_type='sign')\n",
    "            elif at_type == 'min-max':\n",
    "                agg_grads = torch.mean(malicious_grads, 0)\n",
    "                mal_update = our_attack_dist(attacker_grads, agg_grads, n_attacker_, dev_type='sign')\n",
    "            elif at_type == 'min-sum':\n",
    "                agg_grads = torch.mean(malicious_grads, 0)\n",
    "                mal_update = our_attack_score(attacker_grads, agg_grads, n_attacker_, dev_type='sign')\n",
    "\n",
    "        if not len(mal_updates):\n",
    "            mal_updates = torch.stack([mal_update] * n_attacker)\n",
    "        malicious_grads = torch.cat((mal_updates, user_grads), 0)    \n",
    "        \n",
    "        if malicious_grads.shape[0] != 60: \n",
    "            print('malicious grads shape ', malicious_grads.shape)\n",
    "            sys.exit()\n",
    "\n",
    "        agg_grads, krum_candidate = bulyan(malicious_grads, n_attacker)\n",
    "\n",
    "        start_idx=0\n",
    "\n",
    "        if epoch_num in schedule:\n",
    "            for param_group in optimizer_fed.param_groups:\n",
    "                param_group['lr'] *= gamma\n",
    "                print('New learnin rate ', param_group['lr'])\n",
    "\n",
    "        optimizer_fed.zero_grad()\n",
    "\n",
    "        model_grads=[]\n",
    "\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "\n",
    "        optimizer_fed.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if is_best:\n",
    "            best_global_te_acc = te_acc\n",
    "\n",
    "        if epoch_num % 20 == 0:\n",
    "            print('%s: at %s at_frac %.1f n_at %d n_mal_sel %d e %d fed_model val loss %.4f val acc %.4f best val_acc %f te_acc %f'%(aggregation, at_type, at_fraction, n_attacker, np.sum(krum_candidate < n_attacker), epoch_num, val_loss, val_acc, best_global_acc,best_global_te_acc))\n",
    "\n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our second AGR-agnostic attack - Min-Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_attack_score(all_updates, model_re, n_attackers, dev_type='unit_vec'):\n",
    "\n",
    "    if dev_type == 'unit_vec':\n",
    "        deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "    elif dev_type == 'sign':\n",
    "        deviation = torch.sign(model_re)\n",
    "    elif dev_type == 'std':\n",
    "        deviation = torch.std(all_updates, 0)\n",
    "    \n",
    "    lamda = torch.Tensor([50.0]).float().cuda()\n",
    "    # print(lamda)\n",
    "    threshold_diff = 1e-5\n",
    "    lamda_fail = lamda\n",
    "    lamda_succ = 0\n",
    "    \n",
    "    distances = []\n",
    "    for update in all_updates:\n",
    "        distance = torch.norm((all_updates - update), dim=1) ** 2\n",
    "        distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)\n",
    "    \n",
    "    scores = torch.sum(distances, dim=1)\n",
    "    min_score = torch.min(scores)\n",
    "    del distances\n",
    "\n",
    "    while torch.abs(lamda_succ - lamda) > threshold_diff:\n",
    "        mal_update = (model_re - lamda * deviation)\n",
    "        distance = torch.norm((all_updates - mal_update), dim=1) ** 2\n",
    "        score = torch.sum(distance)\n",
    "        \n",
    "        if score <= min_score:\n",
    "            # print('successful lamda is ', lamda)\n",
    "            lamda_succ = lamda\n",
    "            lamda = lamda + lamda_fail / 2\n",
    "        else:\n",
    "            lamda = lamda - lamda_fail / 2\n",
    "\n",
    "        lamda_fail = lamda_fail / 2\n",
    "\n",
    "    mal_update = (model_re - lamda_succ * deviation)\n",
    "    return mal_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Initializing global model\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 2 e 0 fed_model val loss 3.9542 val acc 5.6631 best val_acc 5.663097 te_acc 6.515136\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 7 e 20 fed_model val loss 3.8183 val acc 11.0945 best val_acc 15.501442 te_acc 16.888900\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 5 e 40 fed_model val loss 3.5834 val acc 24.2509 best val_acc 24.315280 te_acc 26.727245\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 6 e 60 fed_model val loss 3.2242 val acc 24.4646 best val_acc 26.253604 te_acc 29.167525\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 80 fed_model val loss 3.1330 val acc 27.6462 best val_acc 31.638694 te_acc 35.790774\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 100 fed_model val loss 3.0015 val acc 31.4637 best val_acc 34.066104 te_acc 37.911862\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 120 fed_model val loss 2.5655 val acc 37.4691 best val_acc 39.090815 te_acc 43.096170\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 140 fed_model val loss 2.3599 val acc 40.5761 best val_acc 41.093493 te_acc 45.062809\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 160 fed_model val loss 2.3861 val acc 38.3211 best val_acc 42.743513 te_acc 46.772035\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 180 fed_model val loss 2.2235 val acc 43.2069 best val_acc 43.356157 te_acc 47.158155\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 200 fed_model val loss 2.2037 val acc 42.7744 best val_acc 46.586697 te_acc 49.760605\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 220 fed_model val loss 2.1523 val acc 46.6176 best val_acc 49.181425 te_acc 52.206034\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 240 fed_model val loss 2.0395 val acc 49.2818 best val_acc 50.064353 te_acc 53.660420\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 260 fed_model val loss 1.9245 val acc 51.6088 best val_acc 51.608834 te_acc 54.744131\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 280 fed_model val loss 1.8758 val acc 52.3193 best val_acc 52.399094 te_acc 55.663097\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 300 fed_model val loss 1.8920 val acc 51.4827 best val_acc 52.944811 te_acc 56.013180\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 320 fed_model val loss 1.9293 val acc 52.6359 best val_acc 52.944811 te_acc 56.013180\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 340 fed_model val loss 1.9454 val acc 50.5406 best val_acc 52.944811 te_acc 56.013180\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 360 fed_model val loss 1.9152 val acc 52.5844 best val_acc 52.944811 te_acc 56.013180\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 380 fed_model val loss 1.8121 val acc 53.9153 best val_acc 53.922982 te_acc 57.055704\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 400 fed_model val loss 1.6058 val acc 57.2797 best val_acc 57.279654 te_acc 59.812603\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 420 fed_model val loss 1.6626 val acc 57.3646 best val_acc 58.126544 te_acc 60.893740\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 440 fed_model val loss 1.6016 val acc 58.1652 best val_acc 58.540980 te_acc 61.019872\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 460 fed_model val loss 1.6567 val acc 57.3800 best val_acc 59.351833 te_acc 61.969728\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 480 fed_model val loss 1.6144 val acc 57.2771 best val_acc 59.351833 te_acc 61.969728\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 500 fed_model val loss 1.5010 val acc 60.3068 best val_acc 60.829386 te_acc 63.331446\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 520 fed_model val loss 1.5153 val acc 59.9465 best val_acc 61.251544 te_acc 63.305704\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 540 fed_model val loss 1.5173 val acc 60.2373 best val_acc 61.743204 te_acc 64.070222\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 560 fed_model val loss 1.4555 val acc 60.9813 best val_acc 61.743204 te_acc 64.070222\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 580 fed_model val loss 1.4453 val acc 61.0739 best val_acc 62.708505 te_acc 64.863056\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 600 fed_model val loss 1.5840 val acc 59.8641 best val_acc 63.138386 te_acc 65.241454\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 620 fed_model val loss 1.4259 val acc 61.3262 best val_acc 63.138386 te_acc 65.241454\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 640 fed_model val loss 1.4506 val acc 61.6891 best val_acc 63.138386 te_acc 65.241454\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 660 fed_model val loss 1.4110 val acc 62.1036 best val_acc 63.470449 te_acc 65.563221\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 680 fed_model val loss 1.4354 val acc 61.9852 best val_acc 63.470449 te_acc 65.563221\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 700 fed_model val loss 1.3364 val acc 63.3366 best val_acc 63.529654 te_acc 65.485997\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 720 fed_model val loss 1.5073 val acc 61.0276 best val_acc 64.546437 te_acc 66.361203\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 740 fed_model val loss 1.3455 val acc 62.8295 best val_acc 65.292937 te_acc 66.796231\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 760 fed_model val loss 1.2903 val acc 64.7936 best val_acc 65.292937 te_acc 66.796231\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 780 fed_model val loss 1.2079 val acc 65.9030 best val_acc 66.327739 te_acc 67.331652\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 800 fed_model val loss 1.2943 val acc 65.7177 best val_acc 66.327739 te_acc 67.331652\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 820 fed_model val loss 1.2294 val acc 65.9416 best val_acc 66.590301 te_acc 68.278933\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 840 fed_model val loss 1.2317 val acc 65.9390 best val_acc 66.953254 te_acc 68.587829\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 860 fed_model val loss 1.1767 val acc 66.7525 best val_acc 67.746087 te_acc 69.244234\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 880 fed_model val loss 1.3116 val acc 64.9815 best val_acc 68.466845 te_acc 69.532537\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 900 fed_model val loss 1.2100 val acc 65.6379 best val_acc 68.466845 te_acc 69.532537\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 920 fed_model val loss 1.2717 val acc 65.8438 best val_acc 68.466845 te_acc 69.532537\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 940 fed_model val loss 1.1409 val acc 67.9057 best val_acc 68.466845 te_acc 69.532537\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 960 fed_model val loss 1.2090 val acc 67.5942 best val_acc 68.822076 te_acc 70.320222\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 980 fed_model val loss 1.1447 val acc 67.6122 best val_acc 68.881281 te_acc 70.696046\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 1000 fed_model val loss 1.1508 val acc 67.8001 best val_acc 68.881281 te_acc 70.696046\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 1020 fed_model val loss 1.1523 val acc 68.6728 best val_acc 68.922467 te_acc 70.521005\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 1040 fed_model val loss 1.2177 val acc 66.1707 best val_acc 68.922467 te_acc 70.521005\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 1060 fed_model val loss 1.2161 val acc 66.6727 best val_acc 68.922467 te_acc 70.521005\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 1080 fed_model val loss 1.2312 val acc 66.8915 best val_acc 68.922467 te_acc 70.521005\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 1100 fed_model val loss 1.2338 val acc 67.4861 best val_acc 68.922467 te_acc 70.521005\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 1120 fed_model val loss 1.2089 val acc 67.2905 best val_acc 68.922467 te_acc 70.521005\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 1140 fed_model val loss 1.2053 val acc 66.5465 best val_acc 68.922467 te_acc 70.521005\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 1160 fed_model val loss 1.2216 val acc 67.2827 best val_acc 68.922467 te_acc 70.521005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 1180 fed_model val loss 1.2743 val acc 66.3612 best val_acc 69.167010 te_acc 70.855643\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 1200 fed_model val loss 1.1394 val acc 69.2288 best val_acc 69.864600 te_acc 71.270078\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 1220 fed_model val loss 1.1540 val acc 67.5582 best val_acc 70.379428 te_acc 71.558381\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 1240 fed_model val loss 1.1259 val acc 68.9688 best val_acc 70.415465 te_acc 71.591845\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 1260 fed_model val loss 1.1274 val acc 69.5866 best val_acc 70.876236 te_acc 71.754016\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 1280 fed_model val loss 1.1709 val acc 69.1902 best val_acc 70.876236 te_acc 71.754016\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 1300 fed_model val loss 1.1578 val acc 69.0409 best val_acc 70.876236 te_acc 71.754016\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 1320 fed_model val loss 1.1256 val acc 69.3318 best val_acc 70.876236 te_acc 71.754016\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 1340 fed_model val loss 1.1386 val acc 69.1799 best val_acc 70.876236 te_acc 71.754016\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 1360 fed_model val loss 1.1511 val acc 70.2533 best val_acc 70.876236 te_acc 71.754016\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 1380 fed_model val loss 1.1703 val acc 68.5518 best val_acc 70.876236 te_acc 71.754016\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 1400 fed_model val loss 1.2303 val acc 67.7126 best val_acc 70.876236 te_acc 71.754016\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 1420 fed_model val loss 1.2175 val acc 68.2352 best val_acc 70.876236 te_acc 71.754016\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 1440 fed_model val loss 1.1201 val acc 69.2365 best val_acc 70.876236 te_acc 71.754016\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 1460 fed_model val loss 1.2130 val acc 68.1579 best val_acc 70.876236 te_acc 71.754016\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 1480 fed_model val loss 1.1925 val acc 70.0731 best val_acc 70.876236 te_acc 71.754016\n",
      "bulyan: at min-sum at_frac 20.0 n_at 9 n_mal_sel 9 e 1500 fed_model val loss 1.1424 val acc 69.9933 best val_acc 70.883958 te_acc 71.929057\n"
     ]
    }
   ],
   "source": [
    "resume=0\n",
    "nepochs=1500\n",
    "gamma=.1\n",
    "fed_lr=0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "batch_size = 100\n",
    "schedule = [2000]\n",
    "\n",
    "aggregation = 'bulyan'\n",
    "at_type = 'min-sum'\n",
    "chkpt = './' + aggregation\n",
    "epoch_num = 0\n",
    "\n",
    "at_fractions = [20]\n",
    "\n",
    "for at_fraction in at_fractions:\n",
    "\n",
    "    fed_model = mnist_conv().cuda()\n",
    "    fed_model.apply(weights_init)\n",
    "    optimizer_fed = Adam(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "    print('==> Initializing global model')\n",
    "    epoch_num = 0\n",
    "    n_attacker = 0\n",
    "    best_global_acc=0\n",
    "    best_global_te_acc=0\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        user_grads = []\n",
    "\n",
    "        # The following condition is necessary for Krum/Multi-krum/Bulyan to work\n",
    "        while n_attacker < 4:\n",
    "            round_users = np.random.choice(3400, 60)\n",
    "            n_attacker = np.sum(round_users < (34*at_fraction))\n",
    "\n",
    "        if n_attacker > 14:\n",
    "            print ('n_attackers actual %d adjusted 14' % n_attacker)\n",
    "            n_attacker = 14\n",
    "\n",
    "        at_idx = []\n",
    "        attacker_count = 0\n",
    "        for i in round_users:\n",
    "            if i < (34*at_fraction) and attacker_count < n_attacker:\n",
    "                at_idx.append(i)\n",
    "                attacker_count += 1\n",
    "                continue\n",
    "\n",
    "            inputs = user_tr_data_tensors[i]\n",
    "            targets = user_tr_label_tensors[i]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "            outputs = fed_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            optimizer_fed.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            param_grad=[]\n",
    "            for param in fed_model.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "            user_grads=param_grad[None,:] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]),0)    \n",
    "\n",
    "        if n_attacker > 0:\n",
    "            attacker_grads = []\n",
    "            n_attacker_ = max(1, n_attacker**2//60)\n",
    "            for i in at_idx:\n",
    "\n",
    "                inputs = user_tr_data_tensors[i]\n",
    "                targets = user_tr_label_tensors[i]\n",
    "\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "                outputs = fed_model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                optimizer_fed.zero_grad()\n",
    "                loss.backward(retain_graph=True)\n",
    "\n",
    "                param_grad=[]\n",
    "                for param in fed_model.parameters():\n",
    "                    param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "                attacker_grads=param_grad[None,:] if len(attacker_grads)==0 else torch.cat((attacker_grads,param_grad[None,:]),0)\n",
    "\n",
    "            mal_updates = []\n",
    "            if at_type == 'fang':\n",
    "                agg_grads = torch.mean(attacker_grads, 0)\n",
    "                deviation = torch.sign(agg_grads)\n",
    "                mal_update = get_malicious_updates_fang(attacker_grads, agg_grads, deviation, n_attacker_)\n",
    "            elif at_type == 'our-agr':\n",
    "                agg_grads = torch.mean(attacker_grads, 0)\n",
    "                mal_update = our_attack_mkrum(attacker_grads, agg_grads, n_attacker_, dev_type='sign')\n",
    "            elif at_type == 'min-max':\n",
    "                agg_grads = torch.mean(malicious_grads, 0)\n",
    "                mal_update = our_attack_dist(attacker_grads, agg_grads, n_attacker_, dev_type='sign')\n",
    "            elif at_type == 'min-sum':\n",
    "                agg_grads = torch.mean(malicious_grads, 0)\n",
    "                mal_update = our_attack_score(attacker_grads, agg_grads, n_attacker_, dev_type='sign')\n",
    "\n",
    "        if not len(mal_updates):\n",
    "            mal_updates = torch.stack([mal_update] * n_attacker)\n",
    "        malicious_grads = torch.cat((mal_updates, user_grads), 0)    \n",
    "        \n",
    "        if malicious_grads.shape[0] != 60: \n",
    "            print('malicious grads shape ', malicious_grads.shape)\n",
    "            sys.exit()\n",
    "\n",
    "        agg_grads, krum_candidate = bulyan(malicious_grads, n_attacker)\n",
    "\n",
    "        start_idx=0\n",
    "\n",
    "        if epoch_num in schedule:\n",
    "            for param_group in optimizer_fed.param_groups:\n",
    "                param_group['lr'] *= gamma\n",
    "                print('New learnin rate ', param_group['lr'])\n",
    "\n",
    "        optimizer_fed.zero_grad()\n",
    "\n",
    "        model_grads=[]\n",
    "\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "\n",
    "        optimizer_fed.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if is_best:\n",
    "            best_global_te_acc = te_acc\n",
    "\n",
    "        if epoch_num % 20 == 0:\n",
    "            print('%s: at %s at_frac %.1f n_at %d n_mal_sel %d e %d fed_model val loss %.4f val acc %.4f best val_acc %f te_acc %f'%(aggregation, at_type, at_fraction, n_attacker, np.sum(krum_candidate < n_attacker), epoch_num, val_loss, val_acc, best_global_acc,best_global_te_acc))\n",
    "\n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
