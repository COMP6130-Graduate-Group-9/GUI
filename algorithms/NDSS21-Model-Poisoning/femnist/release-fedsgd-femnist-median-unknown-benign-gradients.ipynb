{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The notebook contains\n",
    "### Code for _Median_ aggregation algorithm, *when gradient updates of benign clients are unknown to adversary*\n",
    "### Evaluation of all of the attacks (Fang, LIE, and our SOTA AGR-tailored and AGR-agnstic) on Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import json\n",
    "import argparse, os, sys, csv, shutil, time, random, operator, pickle, ast, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.optim import Optimizer\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "sys.path.insert(0,'./../utils/')\n",
    "from logger import *\n",
    "from eval import *\n",
    "from misc import *\n",
    "\n",
    "from femnist_normal_train import *\n",
    "from femnist_util import *\n",
    "from adam import Adam\n",
    "from sgd import SGD\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the FEMNIST dataset; we use [LEAF framework](https://leaf.cmu.edu/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_tr_data = []\n",
    "user_tr_labels = []\n",
    "\n",
    "for i in range(34):\n",
    "    f = '/mnt/nfs/work1/amir/vshejwalkar/leaf/data/femnist/data/train/all_data_%d_niid_0_keep_0_train_9.json'%i\n",
    "    with open(f, 'r') as myfile:\n",
    "        data=myfile.read()\n",
    "    obj = json.loads(data)\n",
    "    \n",
    "    for user in obj['users']:\n",
    "        user_tr_data.append(obj['user_data'][user]['x'])\n",
    "        user_tr_labels.append(obj['user_data'][user]['y'])\n",
    "\n",
    "user_te_data = []\n",
    "user_te_labels = []\n",
    "\n",
    "for i in range(34):\n",
    "    f = '/mnt/nfs/work1/amir/vshejwalkar/leaf/data/femnist/data/test/all_data_%d_niid_0_keep_0_test_9.json'%i\n",
    "    with open(f, 'r') as myfile:\n",
    "        data=myfile.read()\n",
    "    obj = json.loads(data)\n",
    "    \n",
    "    for user in obj['users']:\n",
    "        user_te_data.append(obj['user_data'][user]['x'])\n",
    "        user_te_labels.append(obj['user_data'][user]['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of FL clients are  244\n"
     ]
    }
   ],
   "source": [
    "user_tr_data_tensors=[]\n",
    "user_tr_label_tensors=[]\n",
    "\n",
    "for i in range(len(user_tr_data)):\n",
    "    \n",
    "    user_tr_data_tensor=torch.from_numpy(np.array(user_tr_data[i])).type(torch.FloatTensor)\n",
    "    user_tr_label_tensor=torch.from_numpy(np.array(user_tr_labels[i])).type(torch.LongTensor)\n",
    "\n",
    "    user_tr_data_tensors.append(user_tr_data_tensor)\n",
    "    user_tr_label_tensors.append(user_tr_label_tensor)\n",
    "    \n",
    "#     print('user %d tr len %d'%(i,len(user_tr_data_tensor)))\n",
    "print(\"number of FL clients are \", len(user_tr_data_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_data = np.concatenate(user_te_data, 0)\n",
    "te_labels = np.concatenate(user_te_labels)\n",
    "te_len = len(te_labels)\n",
    "\n",
    "te_data_tensor = torch.from_numpy(te_data[:(te_len//2)]).type(torch.FloatTensor)\n",
    "te_label_tensor = torch.from_numpy(te_labels[:(te_len//2)]).type(torch.LongTensor)\n",
    "\n",
    "val_data_tensor = torch.from_numpy(te_data[(te_len//2):]).type(torch.FloatTensor)\n",
    "val_label_tensor = torch.from_numpy(te_labels[(te_len//2):]).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture for FEMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mnist_conv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mnist_conv, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 16, 5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5, padding=2)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 512)\n",
    "        self.fc2 = nn.Linear(512, 62)\n",
    "\n",
    "    def forward(self, x, noise=torch.Tensor()):\n",
    "        x = x.reshape(-1, 1, 28, 28)\n",
    "\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, 32 * 7 * 7)  # reshape Variable\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.fill_(0)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for (Full-knowledge) Fang attack on Median\n",
    "\n",
    "### Note that the Fang attacks on Trimmed-mean and Median are the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fang_attack_trmean_partial(all_updates, n_attackers):\n",
    "\n",
    "    model_re = torch.mean(all_updates, 0)\n",
    "    model_std = torch.std(all_updates, 0)\n",
    "    deviation = torch.sign(model_re)\n",
    "    \n",
    "    max_vector_low = model_re + 3 * model_std \n",
    "    max_vector_hig = model_re + 4 * model_std\n",
    "    min_vector_low = model_re - 4 * model_std\n",
    "    min_vector_hig = model_re - 3 * model_std\n",
    "\n",
    "    max_range = torch.cat((max_vector_low[:,None], max_vector_hig[:,None]), dim=1)\n",
    "    min_range = torch.cat((min_vector_low[:,None], min_vector_hig[:,None]), dim=1)\n",
    "\n",
    "    rand = torch.from_numpy(np.random.uniform(0, 1, [len(deviation), n_attackers])).type(torch.FloatTensor).cuda()\n",
    "\n",
    "    max_rand = torch.stack([max_range[:, 0]] * rand.shape[1]).T + rand * torch.stack([max_range[:, 1] - max_range[:, 0]] * rand.shape[1]).T\n",
    "    min_rand = torch.stack([min_range[:, 0]] * rand.shape[1]).T + rand * torch.stack([min_range[:, 1] - min_range[:, 0]] * rand.shape[1]).T\n",
    "\n",
    "    mal_vec = (torch.stack([(deviation > 0).type(torch.FloatTensor)] * max_rand.shape[1]).T.cuda() * max_rand + torch.stack(\n",
    "        [(deviation > 0).type(torch.FloatTensor)] * min_rand.shape[1]).T.cuda() * min_rand).T\n",
    "\n",
    "    return mal_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation for Full-knolwledge Fang attack on Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median: at fang n_at 11 e 0 | val loss 3.9576 val acc 4.8548 best val_acc 4.854819 te_acc 5.202327\n",
      "median: at fang n_at 12 e 20 | val loss 3.7768 val acc 5.5962 best val_acc 9.171643 te_acc 9.658155\n",
      "median: at fang n_at 11 e 40 | val loss 3.7763 val acc 21.1594 best val_acc 21.159390 te_acc 23.190383\n",
      "median: at fang n_at 14 e 60 | val loss 3.5524 val acc 28.9873 best val_acc 28.987335 te_acc 31.849773\n",
      "median: at fang n_at 8 e 80 | val loss 3.1900 val acc 29.7776 best val_acc 31.564044 te_acc 34.789436\n",
      "median: at fang n_at 12 e 100 | val loss 2.7007 val acc 35.0391 best val_acc 35.039127 te_acc 39.433175\n",
      "median: at fang n_at 18 e 120 | val loss 2.4746 val acc 37.1525 best val_acc 37.976215 te_acc 42.099979\n",
      "median: at fang n_at 6 e 140 | val loss 2.1404 val acc 43.2532 best val_acc 43.253192 te_acc 47.098950\n",
      "median: at fang n_at 13 e 160 | val loss 1.9909 val acc 46.3705 best val_acc 46.437397 te_acc 50.105540\n",
      "median: at fang n_at 13 e 180 | val loss 1.8721 val acc 49.2175 best val_acc 50.285729 te_acc 53.588344\n",
      "median: at fang n_at 10 e 200 | val loss 1.7184 val acc 52.7904 best val_acc 52.790362 te_acc 55.596170\n",
      "median: at fang n_at 11 e 220 | val loss 1.6530 val acc 54.5382 best val_acc 55.011841 te_acc 57.794481\n",
      "median: at fang n_at 12 e 240 | val loss 1.5441 val acc 56.9862 best val_acc 57.318266 te_acc 59.534596\n",
      "median: at fang n_at 11 e 260 | val loss 1.4641 val acc 58.6954 best val_acc 58.746911 te_acc 61.079077\n",
      "median: at fang n_at 10 e 280 | val loss 1.4319 val acc 59.3029 best val_acc 60.723847 te_acc 62.801174\n",
      "median: at fang n_at 10 e 300 | val loss 1.4067 val acc 60.4742 best val_acc 61.529551 te_acc 63.617175\n",
      "median: at fang n_at 12 e 320 | val loss 1.2973 val acc 62.6287 best val_acc 63.089477 te_acc 64.891371\n",
      "median: at fang n_at 11 e 340 | val loss 1.2362 val acc 64.0831 best val_acc 64.183484 te_acc 65.933896\n",
      "median: at fang n_at 12 e 360 | val loss 1.2072 val acc 65.2106 best val_acc 65.210564 te_acc 66.966124\n",
      "median: at fang n_at 9 e 380 | val loss 1.1918 val acc 65.1642 best val_acc 65.939044 te_acc 67.578769\n",
      "median: at fang n_at 14 e 400 | val loss 1.1627 val acc 65.6585 best val_acc 66.685544 te_acc 68.150227\n",
      "median: at fang n_at 9 e 420 | val loss 1.1815 val acc 65.5709 best val_acc 66.775638 te_acc 68.428233\n",
      "median: at fang n_at 14 e 440 | val loss 1.1205 val acc 67.0923 best val_acc 67.406301 te_acc 69.004839\n",
      "median: at fang n_at 14 e 460 | val loss 1.1084 val acc 67.2827 best val_acc 67.661141 te_acc 69.334329\n",
      "median: at fang n_at 14 e 480 | val loss 1.0769 val acc 68.0679 best val_acc 68.520902 te_acc 69.738468\n",
      "median: at fang n_at 13 e 500 | val loss 1.0657 val acc 68.4463 best val_acc 68.520902 te_acc 69.738468\n",
      "median: at fang n_at 13 e 520 | val loss 1.0538 val acc 68.5981 best val_acc 68.845243 te_acc 70.428336\n",
      "median: at fang n_at 11 e 540 | val loss 1.0422 val acc 68.6367 best val_acc 68.986820 te_acc 70.647138\n",
      "median: at fang n_at 15 e 560 | val loss 1.0237 val acc 68.8504 best val_acc 69.759061 te_acc 71.313839\n",
      "median: at fang n_at 9 e 580 | val loss 0.9998 val acc 69.9393 best val_acc 69.980437 te_acc 71.602142\n",
      "median: at fang n_at 15 e 600 | val loss 0.9982 val acc 70.0062 best val_acc 70.145181 te_acc 71.599568\n",
      "median: at fang n_at 8 e 620 | val loss 1.0068 val acc 69.5866 best val_acc 70.240424 te_acc 72.062912\n",
      "median: at fang n_at 10 e 640 | val loss 0.9689 val acc 70.4772 best val_acc 70.672879 te_acc 72.093801\n",
      "median: at fang n_at 12 e 660 | val loss 0.9580 val acc 70.5236 best val_acc 71.324135 te_acc 72.786244\n",
      "median: at fang n_at 10 e 680 | val loss 0.9360 val acc 71.0281 best val_acc 71.447694 te_acc 72.958711\n",
      "median: at fang n_at 12 e 700 | val loss 0.9581 val acc 70.7475 best val_acc 71.468287 te_acc 72.948414\n",
      "median: at fang n_at 15 e 720 | val loss 0.9352 val acc 71.2443 best val_acc 71.581549 te_acc 73.270181\n",
      "median: at fang n_at 23 e 740 | val loss 0.9264 val acc 71.7823 best val_acc 71.808072 te_acc 73.159493\n",
      "median: at fang n_at 12 e 760 | val loss 0.9271 val acc 72.0037 best val_acc 72.003707 te_acc 73.406610\n",
      "median: at fang n_at 8 e 780 | val loss 0.9295 val acc 71.4863 best val_acc 72.191619 te_acc 73.625412\n",
      "median: at fang n_at 16 e 800 | val loss 0.9150 val acc 71.8158 best val_acc 72.443884 te_acc 73.792731\n",
      "median: at fang n_at 7 e 820 | val loss 0.9255 val acc 71.4322 best val_acc 72.443884 te_acc 73.792731\n",
      "median: at fang n_at 8 e 840 | val loss 0.9040 val acc 71.9728 best val_acc 72.443884 te_acc 73.792731\n",
      "median: at fang n_at 13 e 860 | val loss 0.9072 val acc 71.8029 best val_acc 73.149197 te_acc 74.433690\n",
      "median: at fang n_at 13 e 880 | val loss 0.8845 val acc 72.7914 best val_acc 73.149197 te_acc 74.433690\n",
      "median: at fang n_at 9 e 900 | val loss 0.8774 val acc 72.5108 best val_acc 73.373147 te_acc 74.580416\n",
      "median: at fang n_at 13 e 920 | val loss 0.8533 val acc 73.9498 best val_acc 73.949753 te_acc 74.956240\n",
      "median: at fang n_at 7 e 940 | val loss 0.8559 val acc 72.9896 best val_acc 74.050144 te_acc 75.218801\n",
      "median: at fang n_at 11 e 960 | val loss 0.8787 val acc 72.9793 best val_acc 74.915054 te_acc 75.646108\n",
      "median: at fang n_at 13 e 980 | val loss 0.8486 val acc 73.7310 best val_acc 74.915054 te_acc 75.646108\n",
      "median: at fang n_at 12 e 1000 | val loss 0.8315 val acc 74.5701 best val_acc 75.221376 te_acc 75.792834\n",
      "median: at fang n_at 15 e 1020 | val loss 0.8256 val acc 74.5367 best val_acc 75.221376 te_acc 75.792834\n",
      "median: at fang n_at 6 e 1040 | val loss 0.8507 val acc 73.6074 best val_acc 75.221376 te_acc 75.792834\n",
      "median: at fang n_at 8 e 1060 | val loss 0.8387 val acc 73.9909 best val_acc 75.221376 te_acc 75.792834\n",
      "median: at fang n_at 7 e 1080 | val loss 0.8285 val acc 74.1402 best val_acc 75.221376 te_acc 75.792834\n",
      "median: at fang n_at 12 e 1100 | val loss 0.8545 val acc 73.6306 best val_acc 75.221376 te_acc 75.792834\n",
      "median: at fang n_at 18 e 1120 | val loss 0.8230 val acc 74.3745 best val_acc 75.221376 te_acc 75.792834\n",
      "median: at fang n_at 12 e 1140 | val loss 0.8334 val acc 73.8468 best val_acc 75.221376 te_acc 75.792834\n",
      "median: at fang n_at 9 e 1160 | val loss 0.8208 val acc 73.9420 best val_acc 75.805704 te_acc 76.323105\n",
      "median: at fang n_at 12 e 1180 | val loss 0.8124 val acc 74.4285 best val_acc 76.050247 te_acc 76.539333\n",
      "median: at fang n_at 12 e 1200 | val loss 0.8164 val acc 74.2638 best val_acc 76.050247 te_acc 76.539333\n",
      "median: at fang n_at 19 e 1220 | val loss 0.8033 val acc 74.4002 best val_acc 76.050247 te_acc 76.539333\n",
      "median: at fang n_at 16 e 1240 | val loss 0.7881 val acc 75.9730 best val_acc 76.050247 te_acc 76.539333\n",
      "median: at fang n_at 15 e 1260 | val loss 0.8006 val acc 74.9073 best val_acc 76.050247 te_acc 76.539333\n",
      "median: at fang n_at 13 e 1280 | val loss 0.7962 val acc 75.6049 best val_acc 76.050247 te_acc 76.539333\n",
      "median: at fang n_at 9 e 1300 | val loss 0.7845 val acc 75.2677 best val_acc 76.145490 te_acc 76.647446\n",
      "median: at fang n_at 7 e 1320 | val loss 0.7638 val acc 76.0914 best val_acc 76.724671 te_acc 77.275535\n",
      "median: at fang n_at 12 e 1340 | val loss 0.7785 val acc 75.6744 best val_acc 76.724671 te_acc 77.275535\n",
      "median: at fang n_at 11 e 1360 | val loss 0.7851 val acc 75.9138 best val_acc 76.724671 te_acc 77.275535\n",
      "median: at fang n_at 11 e 1380 | val loss 0.7656 val acc 76.5574 best val_acc 76.724671 te_acc 77.275535\n",
      "median: at fang n_at 9 e 1400 | val loss 0.8014 val acc 74.8327 best val_acc 76.724671 te_acc 77.275535\n",
      "median: at fang n_at 11 e 1420 | val loss 0.8230 val acc 74.2509 best val_acc 76.724671 te_acc 77.275535\n",
      "median: at fang n_at 14 e 1440 | val loss 0.7783 val acc 76.0116 best val_acc 76.724671 te_acc 77.275535\n",
      "median: at fang n_at 8 e 1460 | val loss 0.7726 val acc 76.2896 best val_acc 76.724671 te_acc 77.275535\n",
      "median: at fang n_at 9 e 1480 | val loss 0.7857 val acc 75.6847 best val_acc 76.871396 te_acc 77.334741\n",
      "median: at fang n_at 8 e 1499 | val loss 0.7750 val acc 75.4865 best val_acc 76.933175 te_acc 77.417113\n",
      "median: at fang n_at 11 e 1500 | val loss 0.7733 val acc 75.5148 best val_acc 76.933175 te_acc 77.417113\n"
     ]
    }
   ],
   "source": [
    "resume=0\n",
    "nepochs=1500\n",
    "gamma=.1\n",
    "fed_lr=0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "batch_size = 100\n",
    "schedule = [5000]\n",
    "\n",
    "aggregation = 'median'\n",
    "chkpt = './' + aggregation\n",
    "\n",
    "at_type='fang'\n",
    "at_fractions = [20]\n",
    "\n",
    "for at_fraction in at_fractions:\n",
    "    epoch_num = 0\n",
    "\n",
    "    fed_model = mnist_conv().cuda()\n",
    "    fed_model.apply(weights_init)\n",
    "    optimizer_fed = Adam(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "    best_global_acc=0\n",
    "    best_global_te_acc=0\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        user_grads = []\n",
    "\n",
    "        round_users = np.random.choice(3400, 60)\n",
    "        n_attacker = np.sum(round_users < (34*at_fraction))\n",
    "\n",
    "        at_idx = []\n",
    "        for i in np.sort(round_users):\n",
    "            if i < (34*at_fraction):\n",
    "                at_idx.append(i)\n",
    "                continue\n",
    "\n",
    "            inputs = user_tr_data_tensors[i]\n",
    "            targets = user_tr_label_tensors[i]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "            outputs = fed_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            optimizer_fed.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            param_grad=[]\n",
    "            for param in fed_model.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "            user_grads=param_grad[None,:] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]),0)    \n",
    "\n",
    "        if n_attacker > 0:\n",
    "            attacker_grads = []\n",
    "            proxy_n_attackers = max(1, n_attacker**2//60)\n",
    "            for i in at_idx:\n",
    "\n",
    "                inputs = user_tr_data_tensors[i]\n",
    "                targets = user_tr_label_tensors[i]\n",
    "\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "                outputs = fed_model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                optimizer_fed.zero_grad()\n",
    "                loss.backward(retain_graph=True)\n",
    "\n",
    "                param_grad=[]\n",
    "                for param in fed_model.parameters():\n",
    "                    param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "                attacker_grads=param_grad[None,:] if len(attacker_grads)==0 else torch.cat((attacker_grads,param_grad[None,:]),0)\n",
    "\n",
    "            mal_updates = []\n",
    "            if at_type == 'fang':\n",
    "                mal_updates = fang_attack_trmean_partial(attacker_grads, n_attacker)\n",
    "        \n",
    "        if not len(mal_updates):\n",
    "            mal_updates = torch.stack([mal_update] * n_attacker)\n",
    "        malicious_grads = torch.cat((mal_updates, user_grads), 0)\n",
    "            \n",
    "        if not (malicious_grads.shape[0]==60):\n",
    "            print(malicious_grads.shape)\n",
    "            sys.exit()\n",
    "            \n",
    "        agg_grads=torch.median(malicious_grads,dim=0)[0]\n",
    "        \n",
    "        start_idx=0\n",
    "\n",
    "        if epoch_num in schedule:\n",
    "            for param_group in optimizer_fed.param_groups:\n",
    "                param_group['lr'] *= gamma\n",
    "                print('New learnin rate ', param_group['lr'])\n",
    "\n",
    "        optimizer_fed.zero_grad()\n",
    "\n",
    "        model_grads=[]\n",
    "\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "\n",
    "        optimizer_fed.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if is_best:\n",
    "            best_global_te_acc = te_acc\n",
    "\n",
    "        if epoch_num % 20 == 0 or epoch_num == nepochs-1:\n",
    "            print('%s: at %s n_at %d e %d | val loss %.4f val acc %.4f best val_acc %f te_acc %f'%(aggregation, at_type, n_attacker, epoch_num, val_loss, val_acc, best_global_acc,best_global_te_acc))\n",
    "\n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for our SOTA AGR-tailored attack on Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_attack_median(all_updates, n_attackers, dev_type='sign', threshold=5.0, threshold_diff=1e-5):\n",
    "    \n",
    "    model_re = torch.mean(all_updates, 0)\n",
    "    \n",
    "    if dev_type == 'sign':\n",
    "        deviation = torch.sign(model_re)\n",
    "    elif dev_type == 'unit_vec':\n",
    "        deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "    elif dev_type == 'std':\n",
    "        deviation = torch.std(all_updates, 0)\n",
    "\n",
    "    lamda = torch.Tensor([threshold]).cuda()  # compute_lambda_our(all_updates, model_re, n_attackers)\n",
    "\n",
    "    threshold_diff = threshold_diff\n",
    "    prev_loss = -1\n",
    "    lamda_fail = lamda\n",
    "    lamda_succ = 0\n",
    "\n",
    "    while torch.abs(lamda_succ - lamda) > threshold_diff:\n",
    "        mal_update = (model_re - lamda * deviation)\n",
    "        mal_updates = torch.stack([mal_update] * n_attackers)\n",
    "        mal_updates = torch.cat((mal_updates, all_updates), 0)\n",
    "\n",
    "        agg_grads = torch.median(mal_updates, 0)[0]\n",
    "\n",
    "        loss = torch.norm(agg_grads - model_re)\n",
    "\n",
    "        if prev_loss < loss:\n",
    "            # print('successful lamda is ', lamda)\n",
    "            lamda_succ = lamda\n",
    "            lamda = lamda + lamda_fail / 2\n",
    "        else:\n",
    "            lamda = lamda - lamda_fail / 2\n",
    "\n",
    "        lamda_fail = lamda_fail / 2\n",
    "        prev_loss = loss\n",
    "\n",
    "    mal_update = (model_re - lamda_succ * deviation)\n",
    "    \n",
    "    return mal_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of our SOTA AGR-tailored attack on Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malicious grads shape  torch.Size([60, 848382])\n",
      "median: at our-agr n_at 9 e 0 | val loss 3.9986 val acc 4.8986 best val_acc 4.898579 te_acc 5.403110\n",
      "median: at our-agr n_at 10 e 10 | val loss 3.7912 val acc 12.7471 best val_acc 12.747117 te_acc 13.789642\n",
      "median: at our-agr n_at 10 e 20 | val loss 3.8561 val acc 15.3161 best val_acc 16.441001 te_acc 18.139930\n",
      "median: at our-agr n_at 14 e 30 | val loss 3.8118 val acc 20.5416 best val_acc 26.266474 te_acc 29.458402\n",
      "median: at our-agr n_at 7 e 40 | val loss 3.6176 val acc 28.5832 best val_acc 31.000309 te_acc 34.416186\n",
      "median: at our-agr n_at 14 e 50 | val loss 3.3989 val acc 32.7327 best val_acc 32.732702 te_acc 36.655684\n",
      "median: at our-agr n_at 15 e 60 | val loss 3.3995 val acc 29.2962 best val_acc 32.732702 te_acc 36.655684\n",
      "median: at our-agr n_at 7 e 70 | val loss 3.0007 val acc 30.8150 best val_acc 33.100803 te_acc 36.985173\n",
      "median: at our-agr n_at 6 e 80 | val loss 2.7274 val acc 32.9644 best val_acc 34.557764 te_acc 38.560544\n",
      "median: at our-agr n_at 10 e 90 | val loss 2.8190 val acc 32.2925 best val_acc 35.185853 te_acc 39.268431\n",
      "median: at our-agr n_at 11 e 100 | val loss 2.7512 val acc 34.3647 best val_acc 35.185853 te_acc 39.268431\n",
      "median: at our-agr n_at 16 e 110 | val loss 2.8048 val acc 35.1833 best val_acc 35.914333 te_acc 39.502677\n",
      "median: at our-agr n_at 16 e 120 | val loss 2.9113 val acc 32.1484 best val_acc 36.712315 te_acc 40.987953\n",
      "median: at our-agr n_at 14 e 130 | val loss 3.0962 val acc 32.5242 best val_acc 36.712315 te_acc 40.987953\n",
      "median: at our-agr n_at 9 e 140 | val loss 2.7930 val acc 36.5604 best val_acc 36.712315 te_acc 40.987953\n",
      "median: at our-agr n_at 8 e 150 | val loss 2.6725 val acc 37.2761 best val_acc 37.996808 te_acc 42.009885\n",
      "median: at our-agr n_at 13 e 160 | val loss 2.6425 val acc 36.7175 best val_acc 38.063736 te_acc 42.210667\n",
      "median: at our-agr n_at 6 e 170 | val loss 2.8968 val acc 36.6454 best val_acc 38.063736 te_acc 42.210667\n",
      "median: at our-agr n_at 16 e 180 | val loss 2.7719 val acc 36.2464 best val_acc 38.063736 te_acc 42.210667\n",
      "median: at our-agr n_at 12 e 190 | val loss 2.7013 val acc 37.0675 best val_acc 38.303130 te_acc 42.576194\n",
      "median: at our-agr n_at 13 e 200 | val loss 2.8535 val acc 37.7497 best val_acc 38.436985 te_acc 42.707475\n",
      "median: at our-agr n_at 10 e 210 | val loss 2.5739 val acc 40.5452 best val_acc 40.545202 te_acc 44.432146\n",
      "median: at our-agr n_at 9 e 220 | val loss 2.7689 val acc 38.3881 best val_acc 40.851524 te_acc 44.102657\n",
      "median: at our-agr n_at 9 e 230 | val loss 2.6549 val acc 39.8476 best val_acc 40.851524 te_acc 44.102657\n",
      "median: at our-agr n_at 11 e 240 | val loss 2.6026 val acc 39.1243 best val_acc 40.851524 te_acc 44.102657\n",
      "median: at our-agr n_at 11 e 250 | val loss 2.6274 val acc 40.2131 best val_acc 40.851524 te_acc 44.102657\n",
      "median: at our-agr n_at 10 e 260 | val loss 2.9779 val acc 38.8257 best val_acc 40.851524 te_acc 44.102657\n",
      "median: at our-agr n_at 8 e 270 | val loss 2.6795 val acc 40.9777 best val_acc 40.977657 te_acc 44.499073\n",
      "median: at our-agr n_at 13 e 280 | val loss 4.0260 val acc 35.1807 best val_acc 40.977657 te_acc 44.499073\n",
      "median: at our-agr n_at 11 e 290 | val loss 4.7720 val acc 35.6749 best val_acc 40.977657 te_acc 44.499073\n",
      "median: at our-agr n_at 13 e 300 | val loss 4.4558 val acc 40.7434 best val_acc 41.309720 te_acc 44.280272\n",
      "median: at our-agr n_at 17 e 310 | val loss 3.8035 val acc 39.5825 best val_acc 41.309720 te_acc 44.280272\n",
      "median: at our-agr n_at 16 e 320 | val loss 3.1979 val acc 37.3713 best val_acc 41.309720 te_acc 44.280272\n",
      "median: at our-agr n_at 9 e 330 | val loss 3.0507 val acc 39.5027 best val_acc 41.309720 te_acc 44.280272\n",
      "median: at our-agr n_at 18 e 340 | val loss 3.7110 val acc 32.9644 best val_acc 41.309720 te_acc 44.280272\n",
      "median: at our-agr n_at 15 e 350 | val loss 3.2688 val acc 36.5141 best val_acc 41.309720 te_acc 44.280272\n",
      "median: at our-agr n_at 12 e 360 | val loss 2.9412 val acc 38.4524 best val_acc 41.309720 te_acc 44.280272\n",
      "median: at our-agr n_at 17 e 370 | val loss 2.9450 val acc 40.7383 best val_acc 41.448723 te_acc 44.846582\n",
      "median: at our-agr n_at 12 e 380 | val loss 2.9576 val acc 40.1179 best val_acc 41.801380 te_acc 45.322797\n",
      "median: at our-agr n_at 17 e 390 | val loss 3.2028 val acc 37.9170 best val_acc 41.801380 te_acc 45.322797\n",
      "median: at our-agr n_at 7 e 400 | val loss 2.8232 val acc 38.3752 best val_acc 41.801380 te_acc 45.322797\n",
      "median: at our-agr n_at 11 e 410 | val loss 2.9159 val acc 37.5952 best val_acc 41.801380 te_acc 45.322797\n",
      "median: at our-agr n_at 12 e 420 | val loss 2.8279 val acc 40.0458 best val_acc 42.756384 te_acc 45.770696\n",
      "median: at our-agr n_at 11 e 430 | val loss 2.7549 val acc 41.6032 best val_acc 43.188839 te_acc 46.143946\n",
      "median: at our-agr n_at 16 e 440 | val loss 2.9324 val acc 38.8025 best val_acc 43.188839 te_acc 46.143946\n",
      "median: at our-agr n_at 11 e 450 | val loss 3.0977 val acc 41.5208 best val_acc 43.613571 te_acc 46.983114\n",
      "median: at our-agr n_at 9 e 460 | val loss 2.7732 val acc 41.5543 best val_acc 43.613571 te_acc 46.983114\n",
      "median: at our-agr n_at 12 e 470 | val loss 3.0284 val acc 41.8168 best val_acc 43.613571 te_acc 46.983114\n",
      "median: at our-agr n_at 17 e 480 | val loss 2.8785 val acc 40.8000 best val_acc 43.613571 te_acc 46.983114\n",
      "median: at our-agr n_at 11 e 490 | val loss 2.8285 val acc 41.2737 best val_acc 43.613571 te_acc 46.983114\n",
      "median: at our-agr n_at 9 e 500 | val loss 2.6803 val acc 43.7526 best val_acc 43.922467 te_acc 47.086079\n",
      "median: at our-agr n_at 11 e 510 | val loss 2.6544 val acc 43.2558 best val_acc 43.922467 te_acc 47.086079\n",
      "median: at our-agr n_at 13 e 520 | val loss 3.0977 val acc 39.9351 best val_acc 45.248147 te_acc 48.115733\n",
      "median: at our-agr n_at 13 e 530 | val loss 3.0141 val acc 40.1333 best val_acc 45.248147 te_acc 48.115733\n",
      "median: at our-agr n_at 12 e 540 | val loss 2.6426 val acc 41.0626 best val_acc 45.248147 te_acc 48.115733\n",
      "median: at our-agr n_at 6 e 550 | val loss 2.5969 val acc 44.5300 best val_acc 45.248147 te_acc 48.115733\n",
      "median: at our-agr n_at 11 e 560 | val loss 2.8702 val acc 41.5440 best val_acc 45.248147 te_acc 48.115733\n",
      "median: at our-agr n_at 15 e 570 | val loss 3.0107 val acc 41.2505 best val_acc 45.248147 te_acc 48.115733\n",
      "median: at our-agr n_at 12 e 580 | val loss 2.8704 val acc 42.8516 best val_acc 45.248147 te_acc 48.115733\n",
      "median: at our-agr n_at 13 e 590 | val loss 2.9189 val acc 40.1977 best val_acc 45.248147 te_acc 48.115733\n",
      "median: at our-agr n_at 8 e 600 | val loss 3.1298 val acc 40.7048 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 16 e 610 | val loss 3.4642 val acc 39.6031 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 15 e 620 | val loss 3.5765 val acc 33.4432 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 18 e 630 | val loss 3.9203 val acc 36.5630 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 15 e 640 | val loss 5.0301 val acc 30.7609 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 14 e 650 | val loss 4.5770 val acc 29.7004 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 10 e 660 | val loss 4.0667 val acc 33.1703 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 15 e 670 | val loss 3.5811 val acc 35.1884 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 14 e 680 | val loss 3.9939 val acc 34.8641 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 10 e 690 | val loss 3.4798 val acc 38.0766 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 10 e 700 | val loss 3.3158 val acc 39.5027 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 9 e 710 | val loss 3.3591 val acc 39.7138 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 13 e 720 | val loss 3.2482 val acc 39.9120 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 10 e 730 | val loss 3.1360 val acc 42.2905 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 14 e 740 | val loss 3.8139 val acc 40.5014 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 15 e 750 | val loss 4.4827 val acc 38.2362 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 11 e 760 | val loss 4.3026 val acc 35.1678 best val_acc 45.279036 te_acc 47.842875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median: at our-agr n_at 13 e 770 | val loss 3.8688 val acc 34.9645 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 15 e 780 | val loss 3.8979 val acc 36.5167 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 17 e 790 | val loss 3.8346 val acc 36.9672 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 6 e 800 | val loss 4.1446 val acc 37.7986 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 10 e 810 | val loss 4.2521 val acc 38.2722 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 12 e 820 | val loss 4.0767 val acc 38.0689 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 12 e 830 | val loss 4.5735 val acc 35.0546 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 22 e 840 | val loss 5.0179 val acc 32.7224 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 19 e 850 | val loss 4.2900 val acc 37.0959 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 14 e 860 | val loss 3.8865 val acc 36.6222 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 11 e 870 | val loss 4.1865 val acc 38.6532 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 11 e 880 | val loss 4.5796 val acc 37.2323 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 14 e 890 | val loss 4.4711 val acc 38.2594 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 9 e 900 | val loss 5.0527 val acc 35.5668 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 15 e 910 | val loss 4.3834 val acc 37.1654 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 15 e 920 | val loss 4.2489 val acc 39.1294 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 16 e 930 | val loss 4.7654 val acc 38.2645 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 10 e 940 | val loss 4.5216 val acc 38.5863 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 9 e 950 | val loss 4.8146 val acc 39.7575 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 13 e 960 | val loss 5.0663 val acc 35.8294 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 18 e 970 | val loss 3.6905 val acc 39.2942 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 11 e 980 | val loss 3.9300 val acc 39.8039 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 9 e 990 | val loss 4.1710 val acc 40.2749 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 8 e 1000 | val loss 4.1628 val acc 40.6894 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 9 e 1010 | val loss 4.1936 val acc 40.5787 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 11 e 1020 | val loss 4.1950 val acc 38.6481 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 10 e 1030 | val loss 5.0588 val acc 35.8242 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 12 e 1040 | val loss 5.1090 val acc 36.9929 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 17 e 1050 | val loss 6.1636 val acc 31.0956 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 9 e 1060 | val loss 5.2053 val acc 36.7432 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 11 e 1070 | val loss 5.8171 val acc 37.4923 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 15 e 1080 | val loss 6.5716 val acc 36.5038 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 9 e 1090 | val loss 6.6473 val acc 37.1165 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 15 e 1100 | val loss 7.7249 val acc 34.6324 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 12 e 1110 | val loss 7.8394 val acc 29.7441 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 10 e 1120 | val loss 7.7218 val acc 33.6182 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 12 e 1130 | val loss 7.3289 val acc 34.4162 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 13 e 1140 | val loss 6.8677 val acc 36.2953 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 15 e 1150 | val loss 5.2592 val acc 35.9118 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 13 e 1160 | val loss 5.6869 val acc 36.4884 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 13 e 1170 | val loss 6.0004 val acc 37.6956 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 13 e 1180 | val loss 5.7572 val acc 35.4922 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 7 e 1190 | val loss 6.1092 val acc 36.1434 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 12 e 1200 | val loss 5.9421 val acc 37.3481 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 11 e 1210 | val loss 6.0041 val acc 38.5580 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 9 e 1220 | val loss 6.0000 val acc 35.9246 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 8 e 1230 | val loss 5.9197 val acc 38.2516 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 17 e 1240 | val loss 6.4442 val acc 35.5617 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 10 e 1250 | val loss 6.1747 val acc 40.0227 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 16 e 1260 | val loss 7.5602 val acc 38.0586 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 11 e 1270 | val loss 7.4931 val acc 34.9825 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 14 e 1280 | val loss 7.1991 val acc 36.9054 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 10 e 1290 | val loss 6.8401 val acc 35.8320 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 16 e 1300 | val loss 7.8486 val acc 35.1035 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 11 e 1310 | val loss 6.8094 val acc 36.4832 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 21 e 1320 | val loss 7.4584 val acc 35.7779 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 11 e 1330 | val loss 7.6830 val acc 36.1100 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 14 e 1340 | val loss 7.8773 val acc 35.9375 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 12 e 1350 | val loss 8.1673 val acc 36.3365 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 9 e 1360 | val loss 8.4407 val acc 35.3377 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 8 e 1370 | val loss 11.2911 val acc 33.2038 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 19 e 1380 | val loss 10.1726 val acc 29.3580 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 19 e 1390 | val loss 8.9396 val acc 32.8202 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 17 e 1400 | val loss 10.0145 val acc 30.9977 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 6 e 1410 | val loss 11.3787 val acc 30.8768 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 11 e 1420 | val loss 11.6797 val acc 31.5589 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 11 e 1430 | val loss 10.1645 val acc 32.7816 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 13 e 1440 | val loss 11.8715 val acc 32.8151 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 10 e 1450 | val loss 11.2924 val acc 32.8563 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 13 e 1460 | val loss 14.3347 val acc 29.7802 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 14 e 1470 | val loss 19.6391 val acc 26.6011 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 13 e 1480 | val loss 19.4697 val acc 28.0014 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 10 e 1490 | val loss 16.1751 val acc 30.0505 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 14 e 1499 | val loss 13.7351 val acc 31.5589 best val_acc 45.279036 te_acc 47.842875\n",
      "median: at our-agr n_at 2 e 1500 | val loss 13.5178 val acc 32.0300 best val_acc 45.279036 te_acc 47.842875\n"
     ]
    }
   ],
   "source": [
    "resume=0\n",
    "nepochs=1500\n",
    "gamma=.1\n",
    "fed_lr=0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "batch_size = 100\n",
    "schedule = [5000]\n",
    "\n",
    "aggregation = 'median'\n",
    "chkpt = './' + aggregation\n",
    "\n",
    "at_type='our-agr'\n",
    "at_fractions = [20]\n",
    "\n",
    "for at_fraction in at_fractions:\n",
    "    epoch_num = 0\n",
    "\n",
    "    fed_model = mnist_conv().cuda()\n",
    "    fed_model.apply(weights_init)\n",
    "    optimizer_fed = Adam(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "    best_global_acc=0\n",
    "    best_global_te_acc=0\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        user_grads = []\n",
    "\n",
    "        round_users = np.random.choice(3400, 60)\n",
    "        n_attacker = np.sum(round_users < (34*at_fraction))\n",
    "\n",
    "        at_idx = []\n",
    "        for i in np.sort(round_users):\n",
    "            if i < (34*at_fraction):\n",
    "                at_idx.append(i)\n",
    "                continue\n",
    "\n",
    "            inputs = user_tr_data_tensors[i]\n",
    "            targets = user_tr_label_tensors[i]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "            outputs = fed_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            optimizer_fed.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            param_grad=[]\n",
    "            for param in fed_model.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "            user_grads=param_grad[None,:] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]),0)    \n",
    "\n",
    "        if n_attacker > 0:\n",
    "            attacker_grads = []\n",
    "            n_attacker_ = max(1, n_attacker**2//60)\n",
    "            for i in at_idx:\n",
    "\n",
    "                inputs = user_tr_data_tensors[i]\n",
    "                targets = user_tr_label_tensors[i]\n",
    "\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "                outputs = fed_model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                optimizer_fed.zero_grad()\n",
    "                loss.backward(retain_graph=True)\n",
    "\n",
    "                param_grad=[]\n",
    "                for param in fed_model.parameters():\n",
    "                    param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "                attacker_grads=param_grad[None,:] if len(attacker_grads)==0 else torch.cat((attacker_grads,param_grad[None,:]),0)\n",
    "\n",
    "            mal_updates = []\n",
    "            if at_type == 'lie':\n",
    "                mal_update = lie_attack(malicious_grads, z)\n",
    "            elif at_type == 'fang':\n",
    "                mal_updates = fang_attack_trmean_partial(attacker_grads, n_attacker)\n",
    "            elif at_type == 'our-agr':\n",
    "                mal_update = our_attack_median(attacker_grads, n_attacker_, dev_type='sign', threshold=5.0, threshold_diff=1e-5)\n",
    "            elif at_type == 'min-max':\n",
    "                agg_grads = torch.mean(attacker_grads, 0)\n",
    "                mal_update = our_attack_dist(malicious_grads, agg_grads, n_attacker_, dev_type)\n",
    "            elif at_type == 'min-sum':\n",
    "                agg_grads = torch.mean(malicious_grads, 0)\n",
    "                mal_update = our_attack_score(attacker_grads, agg_grads, n_attacker_, dev_type)\n",
    "                \n",
    "        if not len(mal_updates):\n",
    "            mal_updates = torch.stack([mal_update] * n_attacker)\n",
    "        malicious_grads = torch.cat((mal_updates, user_grads), 0)\n",
    "                \n",
    "        if malicious_grads.shape[0] != 60 or not epoch_num: \n",
    "            print('malicious grads shape ', malicious_grads.shape)\n",
    "            sys.exit()\n",
    "\n",
    "        agg_grads=torch.median(malicious_grads,dim=0)[0]\n",
    "\n",
    "        start_idx=0\n",
    "\n",
    "        if epoch_num in schedule:\n",
    "            for param_group in optimizer_fed.param_groups:\n",
    "                param_group['lr'] *= gamma\n",
    "                print('New learnin rate ', param_group['lr'])\n",
    "\n",
    "        optimizer_fed.zero_grad()\n",
    "\n",
    "        model_grads=[]\n",
    "\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "\n",
    "        optimizer_fed.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if is_best:\n",
    "            best_global_te_acc = te_acc\n",
    "\n",
    "        if epoch_num % 20 == 0 or epoch_num == nepochs-1:\n",
    "            print('%s: at %s n_at %d e %d | val loss %.4f val acc %.4f best val_acc %f te_acc %f'%(aggregation, at_type, n_attacker, epoch_num, val_loss, val_acc, best_global_acc,best_global_te_acc))\n",
    "\n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for our first SOTA AGR-agnostic attack - Min-max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_attack_dist(all_updates, model_re, n_attackers, dev_type='unit_vec'):\n",
    "\n",
    "    if dev_type == 'unit_vec':\n",
    "        deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "    elif dev_type == 'sign':\n",
    "        deviation = torch.sign(model_re)\n",
    "    elif dev_type == 'std':\n",
    "        deviation = torch.std(all_updates, 0)\n",
    "\n",
    "    lamda = torch.Tensor([50.0]).float().cuda()\n",
    "    # print(lamda)\n",
    "    threshold_diff = 1e-5\n",
    "    lamda_fail = lamda\n",
    "    lamda_succ = 0\n",
    "    \n",
    "    distances = []\n",
    "    for update in all_updates:\n",
    "        distance = torch.norm((all_updates - update), dim=1) ** 2\n",
    "        distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)\n",
    "    \n",
    "    max_distance = torch.max(distances)\n",
    "    del distances\n",
    "\n",
    "    while torch.abs(lamda_succ - lamda) > threshold_diff:\n",
    "        mal_update = (model_re - lamda * deviation)\n",
    "        distance = torch.norm((all_updates - mal_update), dim=1) ** 2\n",
    "        max_d = torch.max(distance)\n",
    "        \n",
    "        if max_d <= max_distance:\n",
    "            # print('successful lamda is ', lamda)\n",
    "            lamda_succ = lamda\n",
    "            lamda = lamda + lamda_fail / 2\n",
    "        else:\n",
    "            lamda = lamda - lamda_fail / 2\n",
    "\n",
    "        lamda_fail = lamda_fail / 2\n",
    "\n",
    "    mal_update = (model_re - lamda_succ * deviation)\n",
    "    \n",
    "    return mal_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median: at min-max n_at 12 e 0 | val loss 3.9719 val acc 5.1457 best val_acc 5.145696 te_acc 5.397961\n",
      "median: at min-max n_at 12 e 20 | val loss 3.8173 val acc 24.8661 best val_acc 24.866145 te_acc 28.132722\n",
      "median: at min-max n_at 13 e 40 | val loss 3.3582 val acc 34.7791 best val_acc 34.797158 te_acc 39.255560\n",
      "median: at min-max n_at 14 e 60 | val loss 2.6429 val acc 38.2516 best val_acc 38.251647 te_acc 42.753810\n",
      "median: at min-max n_at 15 e 80 | val loss 2.4092 val acc 40.8335 best val_acc 41.008546 te_acc 45.636841\n",
      "median: at min-max n_at 10 e 100 | val loss 2.2684 val acc 42.4732 best val_acc 43.917319 te_acc 48.043657\n",
      "median: at min-max n_at 6 e 120 | val loss 2.2017 val acc 44.4450 best val_acc 44.445016 te_acc 48.857084\n",
      "median: at min-max n_at 16 e 140 | val loss 2.1328 val acc 47.1401 best val_acc 47.140136 te_acc 51.346273\n",
      "median: at min-max n_at 16 e 160 | val loss 2.0475 val acc 48.4169 best val_acc 49.886738 te_acc 53.132722\n",
      "median: at min-max n_at 12 e 180 | val loss 1.9577 val acc 50.8392 best val_acc 52.368204 te_acc 56.069811\n",
      "median: at min-max n_at 16 e 200 | val loss 2.0270 val acc 50.4530 best val_acc 53.330931 te_acc 56.564044\n",
      "median: at min-max n_at 17 e 220 | val loss 1.8985 val acc 53.1996 best val_acc 53.997632 te_acc 57.470140\n",
      "median: at min-max n_at 10 e 240 | val loss 1.7528 val acc 55.8973 best val_acc 55.897343 te_acc 59.117586\n",
      "median: at min-max n_at 11 e 260 | val loss 1.8333 val acc 55.0994 best val_acc 57.459843 te_acc 60.409802\n",
      "median: at min-max n_at 13 e 280 | val loss 1.6722 val acc 57.6889 best val_acc 57.779036 te_acc 60.718699\n",
      "median: at min-max n_at 15 e 300 | val loss 1.9618 val acc 51.7993 best val_acc 58.306734 te_acc 60.909185\n",
      "median: at min-max n_at 12 e 320 | val loss 1.7675 val acc 57.0377 best val_acc 58.847302 te_acc 61.575886\n",
      "median: at min-max n_at 10 e 340 | val loss 1.6010 val acc 59.0635 best val_acc 59.272035 te_acc 61.887356\n",
      "median: at min-max n_at 17 e 360 | val loss 1.7834 val acc 55.6090 best val_acc 59.802306 te_acc 62.466536\n",
      "median: at min-max n_at 7 e 380 | val loss 1.6048 val acc 59.6350 best val_acc 60.175556 te_acc 63.040568\n",
      "median: at min-max n_at 12 e 400 | val loss 1.5892 val acc 59.6839 best val_acc 60.175556 te_acc 63.040568\n",
      "median: at min-max n_at 15 e 420 | val loss 1.5289 val acc 60.0906 best val_acc 60.703254 te_acc 63.403521\n",
      "median: at min-max n_at 13 e 440 | val loss 1.5399 val acc 59.8461 best val_acc 60.703254 te_acc 63.403521\n",
      "median: at min-max n_at 14 e 460 | val loss 1.6602 val acc 57.6194 best val_acc 60.703254 te_acc 63.403521\n",
      "median: at min-max n_at 13 e 480 | val loss 1.5510 val acc 60.2142 best val_acc 60.893740 te_acc 63.699547\n",
      "median: at min-max n_at 10 e 500 | val loss 1.5905 val acc 59.7096 best val_acc 60.968390 te_acc 63.570840\n",
      "median: at min-max n_at 15 e 520 | val loss 1.5822 val acc 60.1035 best val_acc 61.956857 te_acc 64.808999\n",
      "median: at min-max n_at 16 e 540 | val loss 1.5185 val acc 61.0405 best val_acc 61.956857 te_acc 64.808999\n",
      "median: at min-max n_at 12 e 560 | val loss 1.5300 val acc 60.6440 best val_acc 61.956857 te_acc 64.808999\n",
      "median: at min-max n_at 9 e 580 | val loss 1.6424 val acc 59.6221 best val_acc 62.255457 te_acc 64.893946\n",
      "median: at min-max n_at 16 e 600 | val loss 1.6337 val acc 59.9027 best val_acc 62.255457 te_acc 64.893946\n",
      "median: at min-max n_at 10 e 620 | val loss 1.5232 val acc 60.4407 best val_acc 62.255457 te_acc 64.893946\n",
      "median: at min-max n_at 10 e 640 | val loss 1.4865 val acc 62.2503 best val_acc 62.255457 te_acc 64.893946\n",
      "median: at min-max n_at 10 e 660 | val loss 1.5792 val acc 59.6350 best val_acc 62.453666 te_acc 65.107599\n",
      "median: at min-max n_at 17 e 680 | val loss 1.5683 val acc 60.3454 best val_acc 62.453666 te_acc 65.107599\n",
      "median: at min-max n_at 16 e 700 | val loss 1.5406 val acc 61.6119 best val_acc 62.453666 te_acc 65.107599\n",
      "median: at min-max n_at 10 e 720 | val loss 1.5770 val acc 59.5706 best val_acc 62.458814 te_acc 64.945428\n",
      "median: at min-max n_at 14 e 740 | val loss 1.5550 val acc 60.6415 best val_acc 62.458814 te_acc 64.945428\n",
      "median: at min-max n_at 9 e 760 | val loss 1.5846 val acc 60.5848 best val_acc 62.458814 te_acc 64.945428\n",
      "median: at min-max n_at 12 e 780 | val loss 1.5147 val acc 62.1885 best val_acc 62.458814 te_acc 64.945428\n",
      "median: at min-max n_at 12 e 800 | val loss 1.5357 val acc 60.8937 best val_acc 62.458814 te_acc 64.945428\n",
      "median: at min-max n_at 13 e 820 | val loss 1.4926 val acc 61.7149 best val_acc 62.458814 te_acc 64.945428\n",
      "median: at min-max n_at 9 e 840 | val loss 1.5915 val acc 59.1124 best val_acc 62.458814 te_acc 64.945428\n",
      "median: at min-max n_at 17 e 860 | val loss 1.5337 val acc 60.8011 best val_acc 62.667319 te_acc 65.352142\n",
      "median: at min-max n_at 11 e 880 | val loss 1.5452 val acc 60.9478 best val_acc 62.667319 te_acc 65.352142\n",
      "median: at min-max n_at 13 e 900 | val loss 1.5366 val acc 59.9156 best val_acc 62.767710 te_acc 65.334123\n",
      "median: at min-max n_at 8 e 920 | val loss 1.5204 val acc 61.0096 best val_acc 62.767710 te_acc 65.334123\n",
      "median: at min-max n_at 13 e 940 | val loss 1.5300 val acc 60.4021 best val_acc 62.767710 te_acc 65.334123\n",
      "median: at min-max n_at 12 e 960 | val loss 1.6189 val acc 59.3930 best val_acc 63.035420 te_acc 65.717669\n",
      "median: at min-max n_at 11 e 980 | val loss 1.5466 val acc 60.5282 best val_acc 63.187294 te_acc 65.838653\n",
      "median: at min-max n_at 12 e 1000 | val loss 1.5203 val acc 61.0919 best val_acc 63.187294 te_acc 65.838653\n",
      "median: at min-max n_at 14 e 1020 | val loss 1.6938 val acc 60.9787 best val_acc 63.187294 te_acc 65.838653\n",
      "median: at min-max n_at 9 e 1040 | val loss 1.9839 val acc 54.4790 best val_acc 63.187294 te_acc 65.838653\n",
      "median: at min-max n_at 10 e 1060 | val loss 1.6310 val acc 59.5140 best val_acc 63.187294 te_acc 65.838653\n",
      "median: at min-max n_at 8 e 1080 | val loss 1.5104 val acc 61.6325 best val_acc 63.187294 te_acc 65.838653\n",
      "median: at min-max n_at 14 e 1100 | val loss 1.6096 val acc 59.4908 best val_acc 63.187294 te_acc 65.838653\n",
      "median: at min-max n_at 12 e 1120 | val loss 1.5705 val acc 61.3339 best val_acc 63.187294 te_acc 65.838653\n",
      "median: at min-max n_at 16 e 1140 | val loss 1.7154 val acc 59.2463 best val_acc 63.187294 te_acc 65.838653\n",
      "median: at min-max n_at 11 e 1160 | val loss 1.5631 val acc 62.2992 best val_acc 63.187294 te_acc 65.838653\n",
      "median: at min-max n_at 11 e 1180 | val loss 1.5264 val acc 62.3121 best val_acc 63.243925 te_acc 65.604407\n",
      "median: at min-max n_at 13 e 1200 | val loss 1.4950 val acc 61.9208 best val_acc 63.243925 te_acc 65.604407\n",
      "median: at min-max n_at 12 e 1220 | val loss 1.4681 val acc 62.3353 best val_acc 63.243925 te_acc 65.604407\n",
      "median: at min-max n_at 10 e 1240 | val loss 1.6072 val acc 60.1189 best val_acc 63.243925 te_acc 65.604407\n",
      "median: at min-max n_at 13 e 1260 | val loss 1.5334 val acc 62.2966 best val_acc 63.243925 te_acc 65.604407\n",
      "median: at min-max n_at 11 e 1280 | val loss 1.5483 val acc 61.9594 best val_acc 63.243925 te_acc 65.604407\n",
      "median: at min-max n_at 11 e 1300 | val loss 1.4731 val acc 62.8784 best val_acc 63.243925 te_acc 65.604407\n",
      "median: at min-max n_at 10 e 1320 | val loss 1.5762 val acc 61.3674 best val_acc 63.243925 te_acc 65.604407\n",
      "median: at min-max n_at 15 e 1340 | val loss 1.4560 val acc 63.5889 best val_acc 63.588859 te_acc 66.052306\n",
      "median: at min-max n_at 17 e 1360 | val loss 1.5842 val acc 60.6003 best val_acc 63.588859 te_acc 66.052306\n",
      "median: at min-max n_at 8 e 1380 | val loss 1.5506 val acc 62.4691 best val_acc 63.588859 te_acc 66.052306\n",
      "median: at min-max n_at 9 e 1400 | val loss 1.6007 val acc 60.6363 best val_acc 63.588859 te_acc 66.052306\n",
      "median: at min-max n_at 10 e 1420 | val loss 1.6644 val acc 60.0057 best val_acc 63.588859 te_acc 66.052306\n",
      "median: at min-max n_at 13 e 1440 | val loss 1.6092 val acc 61.6608 best val_acc 63.588859 te_acc 66.052306\n",
      "median: at min-max n_at 13 e 1460 | val loss 1.6173 val acc 60.4098 best val_acc 63.588859 te_acc 66.052306\n",
      "median: at min-max n_at 13 e 1480 | val loss 1.5998 val acc 61.6377 best val_acc 63.588859 te_acc 66.052306\n",
      "median: at min-max n_at 8 e 1499 | val loss 1.6264 val acc 61.3700 best val_acc 63.588859 te_acc 66.052306\n",
      "median: at min-max n_at 15 e 1500 | val loss 1.5706 val acc 62.0135 best val_acc 63.588859 te_acc 66.052306\n"
     ]
    }
   ],
   "source": [
    "resume=0\n",
    "nepochs=1500\n",
    "gamma=.1\n",
    "fed_lr=0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "batch_size = 100\n",
    "schedule = [5000]\n",
    "\n",
    "aggregation = 'median'\n",
    "chkpt = './' + aggregation\n",
    "\n",
    "at_type='min-max'\n",
    "at_fractions = [20]\n",
    "\n",
    "for at_fraction in at_fractions:\n",
    "    epoch_num = 0\n",
    "\n",
    "    fed_model = mnist_conv().cuda()\n",
    "    fed_model.apply(weights_init)\n",
    "    optimizer_fed = Adam(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "    best_global_acc=0\n",
    "    best_global_te_acc=0\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        user_grads = []\n",
    "\n",
    "        round_users = np.random.choice(3400, 60)\n",
    "        n_attacker = np.sum(round_users < (34*at_fraction))\n",
    "\n",
    "        at_idx = []\n",
    "        for i in np.sort(round_users):\n",
    "            if i < (34*at_fraction):\n",
    "                at_idx.append(i)\n",
    "                continue\n",
    "\n",
    "            inputs = user_tr_data_tensors[i]\n",
    "            targets = user_tr_label_tensors[i]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "            outputs = fed_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            optimizer_fed.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            param_grad=[]\n",
    "            for param in fed_model.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "            user_grads=param_grad[None,:] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]),0)    \n",
    "\n",
    "        if n_attacker > 0:\n",
    "            attacker_grads = []\n",
    "            n_attacker_ = max(1, n_attacker**2//60)\n",
    "            for i in at_idx:\n",
    "\n",
    "                inputs = user_tr_data_tensors[i]\n",
    "                targets = user_tr_label_tensors[i]\n",
    "\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "                outputs = fed_model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                optimizer_fed.zero_grad()\n",
    "                loss.backward(retain_graph=True)\n",
    "\n",
    "                param_grad=[]\n",
    "                for param in fed_model.parameters():\n",
    "                    param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "                attacker_grads=param_grad[None,:] if len(attacker_grads)==0 else torch.cat((attacker_grads,param_grad[None,:]),0)\n",
    "\n",
    "            mal_updates = []\n",
    "            if at_type == 'lie':\n",
    "                mal_update = lie_attack(malicious_grads, z)\n",
    "            elif at_type == 'fang':\n",
    "                mal_updates = fang_attack_trmean_partial(attacker_grads, n_attacker)\n",
    "            elif at_type == 'our-agr':\n",
    "                mal_update = our_attack_median(attacker_grads, n_attacker_, dev_type='sign', threshold=5.0, threshold_diff=1e-5)\n",
    "            elif at_type == 'min-max':\n",
    "                agg_grads = torch.mean(attacker_grads, 0)\n",
    "                mal_update = our_attack_dist(malicious_grads, agg_grads, n_attacker_, dev_type='sign')\n",
    "            elif at_type == 'min-sum':\n",
    "                agg_grads = torch.mean(malicious_grads, 0)\n",
    "                mal_update = our_attack_score(attacker_grads, agg_grads, n_attacker_, dev_type='sign')\n",
    "                \n",
    "        if not len(mal_updates):\n",
    "            mal_updates = torch.stack([mal_update] * n_attacker)\n",
    "        malicious_grads = torch.cat((mal_updates, user_grads), 0)\n",
    "                \n",
    "        if malicious_grads.shape[0] != 60: \n",
    "            print('malicious grads shape ', malicious_grads.shape)\n",
    "            sys.exit()\n",
    "\n",
    "        agg_grads=torch.median(malicious_grads,dim=0)[0]\n",
    "\n",
    "        start_idx=0\n",
    "\n",
    "        if epoch_num in schedule:\n",
    "            for param_group in optimizer_fed.param_groups:\n",
    "                param_group['lr'] *= gamma\n",
    "                print('New learnin rate ', param_group['lr'])\n",
    "\n",
    "        optimizer_fed.zero_grad()\n",
    "\n",
    "        model_grads=[]\n",
    "\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "\n",
    "        optimizer_fed.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if is_best:\n",
    "            best_global_te_acc = te_acc\n",
    "\n",
    "        if epoch_num % 20 == 0 or epoch_num == nepochs-1:\n",
    "            print('%s: at %s n_at %d e %d | val loss %.4f val acc %.4f best val_acc %f te_acc %f'%(aggregation, at_type, n_attacker, epoch_num, val_loss, val_acc, best_global_acc,best_global_te_acc))\n",
    "\n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for our first SOTA AGR-agnostic attack - Min-sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_attack_score(all_updates, model_re, n_attackers, dev_type='unit_vec'):\n",
    "\n",
    "    if dev_type == 'unit_vec':\n",
    "        deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "    elif dev_type == 'sign':\n",
    "        deviation = torch.sign(model_re)\n",
    "    elif dev_type == 'std':\n",
    "        deviation = torch.std(all_updates, 0)\n",
    "    \n",
    "    lamda = torch.Tensor([50.0]).float().cuda()\n",
    "    # print(lamda)\n",
    "    threshold_diff = 1e-5\n",
    "    lamda_fail = lamda\n",
    "    lamda_succ = 0\n",
    "    \n",
    "    distances = []\n",
    "    for update in all_updates:\n",
    "        distance = torch.norm((all_updates - update), dim=1) ** 2\n",
    "        distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)\n",
    "    \n",
    "    scores = torch.sum(distances, dim=1)\n",
    "    min_score = torch.min(scores)\n",
    "    del distances\n",
    "\n",
    "    while torch.abs(lamda_succ - lamda) > threshold_diff:\n",
    "        mal_update = (model_re - lamda * deviation)\n",
    "        distance = torch.norm((all_updates - mal_update), dim=1) ** 2\n",
    "        score = torch.sum(distance)\n",
    "        \n",
    "        if score <= min_score:\n",
    "            # print('successful lamda is ', lamda)\n",
    "            lamda_succ = lamda\n",
    "            lamda = lamda + lamda_fail / 2\n",
    "        else:\n",
    "            lamda = lamda - lamda_fail / 2\n",
    "\n",
    "        lamda_fail = lamda_fail / 2\n",
    "\n",
    "    # print(lamda_succ)\n",
    "    mal_update = (model_re - lamda_succ * deviation)\n",
    "    \n",
    "    return mal_update\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median: at min-sum n_at 14 e 0 | val loss 3.9414 val acc 5.3336 best val_acc 5.333608 te_acc 5.781507\n",
      "median: at min-sum n_at 12 e 20 | val loss 3.7699 val acc 5.1457 best val_acc 11.259267 te_acc 12.963344\n",
      "median: at min-sum n_at 13 e 40 | val loss 3.7748 val acc 10.8783 best val_acc 16.394666 te_acc 18.677924\n",
      "median: at min-sum n_at 10 e 60 | val loss 3.6096 val acc 19.6278 best val_acc 25.738777 te_acc 29.002780\n",
      "median: at min-sum n_at 10 e 80 | val loss 3.1643 val acc 27.2704 best val_acc 29.339992 te_acc 32.951503\n",
      "median: at min-sum n_at 11 e 100 | val loss 2.8909 val acc 28.5420 best val_acc 31.981054 te_acc 35.968390\n",
      "median: at min-sum n_at 13 e 120 | val loss 2.6897 val acc 31.3427 best val_acc 33.489498 te_acc 36.992895\n",
      "median: at min-sum n_at 11 e 140 | val loss 2.4129 val acc 35.8783 best val_acc 36.735482 te_acc 40.485997\n",
      "median: at min-sum n_at 13 e 160 | val loss 2.2661 val acc 39.2916 best val_acc 39.567030 te_acc 42.962315\n",
      "median: at min-sum n_at 16 e 180 | val loss 2.2568 val acc 40.7202 best val_acc 44.890342 te_acc 47.809411\n",
      "median: at min-sum n_at 12 e 200 | val loss 2.1696 val acc 41.2248 best val_acc 44.890342 te_acc 47.809411\n",
      "median: at min-sum n_at 15 e 220 | val loss 1.9268 val acc 47.8480 best val_acc 48.445222 te_acc 51.632002\n",
      "median: at min-sum n_at 14 e 240 | val loss 1.9260 val acc 47.0938 best val_acc 50.453048 te_acc 52.893328\n",
      "median: at min-sum n_at 12 e 260 | val loss 1.9272 val acc 46.5095 best val_acc 52.061882 te_acc 54.491866\n",
      "median: at min-sum n_at 18 e 280 | val loss 1.6687 val acc 53.7505 best val_acc 53.750515 te_acc 56.172776\n",
      "median: at min-sum n_at 14 e 300 | val loss 1.5560 val acc 56.4302 best val_acc 56.430189 te_acc 58.834432\n",
      "median: at min-sum n_at 11 e 320 | val loss 1.6233 val acc 54.5691 best val_acc 59.637562 te_acc 61.323620\n",
      "median: at min-sum n_at 9 e 340 | val loss 1.5740 val acc 55.6374 best val_acc 59.719934 te_acc 61.637665\n",
      "median: at min-sum n_at 9 e 360 | val loss 1.4300 val acc 60.3300 best val_acc 61.091948 te_acc 62.515445\n",
      "median: at min-sum n_at 10 e 380 | val loss 1.3843 val acc 60.4716 best val_acc 62.101009 te_acc 63.897755\n",
      "median: at min-sum n_at 8 e 400 | val loss 1.2727 val acc 63.7304 best val_acc 63.730437 te_acc 65.370161\n",
      "median: at min-sum n_at 12 e 420 | val loss 1.2581 val acc 63.4396 best val_acc 64.757516 te_acc 66.402389\n",
      "median: at min-sum n_at 9 e 440 | val loss 1.2338 val acc 64.7163 best val_acc 65.504016 te_acc 67.136017\n",
      "median: at min-sum n_at 11 e 460 | val loss 1.2375 val acc 63.5914 best val_acc 67.262150 te_acc 68.739703\n",
      "median: at min-sum n_at 13 e 480 | val loss 1.2141 val acc 64.5413 best val_acc 67.995778 te_acc 69.393534\n",
      "median: at min-sum n_at 12 e 500 | val loss 1.0972 val acc 68.2249 best val_acc 68.224876 te_acc 69.470758\n",
      "median: at min-sum n_at 9 e 520 | val loss 1.0999 val acc 67.2029 best val_acc 69.192751 te_acc 70.168348\n",
      "median: at min-sum n_at 8 e 540 | val loss 1.0607 val acc 68.9199 best val_acc 69.192751 te_acc 70.168348\n",
      "median: at min-sum n_at 11 e 560 | val loss 1.0401 val acc 68.7191 best val_acc 70.333093 te_acc 71.496602\n",
      "median: at min-sum n_at 11 e 580 | val loss 1.0191 val acc 69.1284 best val_acc 70.333093 te_acc 71.496602\n",
      "median: at min-sum n_at 12 e 600 | val loss 1.0240 val acc 68.9173 best val_acc 70.333093 te_acc 71.496602\n",
      "median: at min-sum n_at 11 e 620 | val loss 1.0264 val acc 69.5840 best val_acc 71.228892 te_acc 72.410420\n",
      "median: at min-sum n_at 13 e 640 | val loss 1.0988 val acc 66.4461 best val_acc 71.622735 te_acc 72.701297\n",
      "median: at min-sum n_at 14 e 660 | val loss 0.9947 val acc 70.4361 best val_acc 71.622735 te_acc 72.701297\n",
      "median: at min-sum n_at 16 e 680 | val loss 0.9667 val acc 70.5828 best val_acc 72.160729 te_acc 72.984452\n",
      "median: at min-sum n_at 15 e 700 | val loss 0.9515 val acc 71.2701 best val_acc 72.407846 te_acc 73.391166\n",
      "median: at min-sum n_at 16 e 720 | val loss 0.9254 val acc 71.9754 best val_acc 72.925247 te_acc 73.869955\n",
      "median: at min-sum n_at 12 e 740 | val loss 0.9619 val acc 70.9097 best val_acc 72.925247 te_acc 73.869955\n",
      "median: at min-sum n_at 15 e 760 | val loss 1.0975 val acc 66.7010 best val_acc 73.218699 te_acc 73.828769\n",
      "median: at min-sum n_at 16 e 780 | val loss 0.9508 val acc 70.2044 best val_acc 73.218699 te_acc 73.828769\n",
      "median: at min-sum n_at 18 e 800 | val loss 0.8615 val acc 73.0102 best val_acc 73.916289 te_acc 75.082372\n",
      "median: at min-sum n_at 14 e 820 | val loss 0.8682 val acc 73.6666 best val_acc 73.921437 te_acc 74.884164\n",
      "median: at min-sum n_at 10 e 840 | val loss 0.9018 val acc 71.3808 best val_acc 74.510914 te_acc 74.981981\n",
      "median: at min-sum n_at 17 e 860 | val loss 0.8264 val acc 75.1004 best val_acc 75.102965 te_acc 75.630663\n",
      "median: at min-sum n_at 11 e 880 | val loss 0.8850 val acc 72.0861 best val_acc 75.187912 te_acc 75.779963\n",
      "median: at min-sum n_at 14 e 900 | val loss 0.8595 val acc 73.8931 best val_acc 75.187912 te_acc 75.779963\n",
      "median: at min-sum n_at 10 e 920 | val loss 0.8331 val acc 74.0733 best val_acc 75.442751 te_acc 76.009061\n",
      "median: at min-sum n_at 11 e 940 | val loss 0.8554 val acc 73.4941 best val_acc 75.442751 te_acc 76.009061\n",
      "median: at min-sum n_at 10 e 960 | val loss 0.8572 val acc 73.1621 best val_acc 75.442751 te_acc 76.009061\n",
      "median: at min-sum n_at 6 e 980 | val loss 0.7896 val acc 76.0065 best val_acc 76.050247 te_acc 76.361717\n",
      "median: at min-sum n_at 10 e 1000 | val loss 0.8025 val acc 74.8327 best val_acc 76.683484 te_acc 76.678336\n",
      "median: at min-sum n_at 14 e 1020 | val loss 0.7908 val acc 75.3398 best val_acc 76.683484 te_acc 76.678336\n",
      "median: at min-sum n_at 13 e 1040 | val loss 0.8002 val acc 74.7863 best val_acc 76.801895 te_acc 76.935750\n",
      "median: at min-sum n_at 20 e 1060 | val loss 0.7673 val acc 76.1635 best val_acc 76.801895 te_acc 76.935750\n",
      "median: at min-sum n_at 15 e 1080 | val loss 0.7477 val acc 77.3090 best val_acc 77.308999 te_acc 77.141680\n",
      "median: at min-sum n_at 17 e 1100 | val loss 0.7979 val acc 75.4144 best val_acc 77.308999 te_acc 77.141680\n",
      "median: at min-sum n_at 16 e 1120 | val loss 0.7705 val acc 76.2999 best val_acc 77.308999 te_acc 77.141680\n",
      "median: at min-sum n_at 8 e 1140 | val loss 0.7955 val acc 75.6718 best val_acc 77.308999 te_acc 77.141680\n",
      "median: at min-sum n_at 11 e 1160 | val loss 0.7652 val acc 75.8340 best val_acc 77.308999 te_acc 77.141680\n",
      "median: at min-sum n_at 13 e 1180 | val loss 0.7528 val acc 75.9344 best val_acc 77.913921 te_acc 77.733731\n",
      "median: at min-sum n_at 6 e 1200 | val loss 0.7364 val acc 76.9847 best val_acc 77.913921 te_acc 77.733731\n",
      "median: at min-sum n_at 11 e 1220 | val loss 0.7161 val acc 77.6076 best val_acc 77.913921 te_acc 77.733731\n",
      "median: at min-sum n_at 8 e 1240 | val loss 0.7390 val acc 76.5084 best val_acc 77.957681 te_acc 77.965404\n",
      "median: at min-sum n_at 12 e 1260 | val loss 0.7576 val acc 75.8649 best val_acc 78.590918 te_acc 78.521417\n",
      "median: at min-sum n_at 12 e 1280 | val loss 0.7318 val acc 76.2330 best val_acc 78.590918 te_acc 78.521417\n",
      "median: at min-sum n_at 5 e 1300 | val loss 0.7050 val acc 77.0979 best val_acc 79.139209 te_acc 79.092875\n",
      "median: at min-sum n_at 13 e 1320 | val loss 0.7015 val acc 77.3064 best val_acc 79.139209 te_acc 79.092875\n",
      "median: at min-sum n_at 14 e 1340 | val loss 0.6772 val acc 78.6038 best val_acc 79.139209 te_acc 79.092875\n",
      "median: at min-sum n_at 12 e 1360 | val loss 0.6770 val acc 79.0903 best val_acc 79.139209 te_acc 79.092875\n",
      "median: at min-sum n_at 15 e 1380 | val loss 0.6710 val acc 78.9127 best val_acc 79.962932 te_acc 79.504736\n",
      "median: at min-sum n_at 9 e 1400 | val loss 0.7163 val acc 76.4055 best val_acc 79.962932 te_acc 79.504736\n",
      "median: at min-sum n_at 11 e 1420 | val loss 0.7147 val acc 76.8740 best val_acc 79.962932 te_acc 79.504736\n",
      "median: at min-sum n_at 10 e 1440 | val loss 0.7756 val acc 75.3527 best val_acc 79.962932 te_acc 79.504736\n",
      "median: at min-sum n_at 12 e 1460 | val loss 0.6847 val acc 78.0606 best val_acc 79.962932 te_acc 79.504736\n",
      "median: at min-sum n_at 14 e 1480 | val loss 0.6916 val acc 77.7672 best val_acc 79.962932 te_acc 79.504736\n",
      "median: at min-sum n_at 15 e 1499 | val loss 0.6869 val acc 77.7440 best val_acc 79.962932 te_acc 79.504736\n",
      "median: at min-sum n_at 5 e 1500 | val loss 0.7012 val acc 76.9744 best val_acc 79.962932 te_acc 79.504736\n"
     ]
    }
   ],
   "source": [
    "resume=0\n",
    "nepochs=1500\n",
    "gamma=.1\n",
    "fed_lr=0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "batch_size = 100\n",
    "schedule = [5000]\n",
    "\n",
    "aggregation = 'median'\n",
    "chkpt = './' + aggregation\n",
    "\n",
    "at_type='min-sum'\n",
    "at_fractions = [20]\n",
    "\n",
    "for at_fraction in at_fractions:\n",
    "    epoch_num = 0\n",
    "\n",
    "    fed_model = mnist_conv().cuda()\n",
    "    fed_model.apply(weights_init)\n",
    "    optimizer_fed = Adam(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "    best_global_acc=0\n",
    "    best_global_te_acc=0\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        user_grads = []\n",
    "\n",
    "        round_users = np.random.choice(3400, 60)\n",
    "        n_attacker = np.sum(round_users < (34*at_fraction))\n",
    "\n",
    "        at_idx = []\n",
    "        for i in np.sort(round_users):\n",
    "            if i < (34*at_fraction):\n",
    "                at_idx.append(i)\n",
    "                continue\n",
    "\n",
    "            inputs = user_tr_data_tensors[i]\n",
    "            targets = user_tr_label_tensors[i]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "            outputs = fed_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            optimizer_fed.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            param_grad=[]\n",
    "            for param in fed_model.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "            user_grads=param_grad[None,:] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]),0)    \n",
    "\n",
    "        if n_attacker > 0:\n",
    "            attacker_grads = []\n",
    "            n_attacker_ = max(1, n_attacker**2//60)\n",
    "            for i in at_idx:\n",
    "\n",
    "                inputs = user_tr_data_tensors[i]\n",
    "                targets = user_tr_label_tensors[i]\n",
    "\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "                outputs = fed_model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                optimizer_fed.zero_grad()\n",
    "                loss.backward(retain_graph=True)\n",
    "\n",
    "                param_grad=[]\n",
    "                for param in fed_model.parameters():\n",
    "                    param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "                attacker_grads=param_grad[None,:] if len(attacker_grads)==0 else torch.cat((attacker_grads,param_grad[None,:]),0)\n",
    "\n",
    "            mal_updates = []\n",
    "            if at_type == 'lie':\n",
    "                mal_update = lie_attack(malicious_grads, z)\n",
    "            elif at_type == 'fang':\n",
    "                mal_updates = fang_attack_trmean_partial(attacker_grads, n_attacker)\n",
    "            elif at_type == 'our-agr':\n",
    "                mal_update = our_attack_median(attacker_grads, n_attacker_, dev_type='sign', threshold=5.0, threshold_diff=1e-5)\n",
    "            elif at_type == 'min-max':\n",
    "                agg_grads = torch.mean(attacker_grads, 0)\n",
    "                mal_update = our_attack_dist(malicious_grads, agg_grads, n_attacker_, dev_type='sign')\n",
    "            elif at_type == 'min-sum':\n",
    "                agg_grads = torch.mean(malicious_grads, 0)\n",
    "                mal_update = our_attack_score(attacker_grads, agg_grads, n_attacker_, dev_type='sign')\n",
    "                \n",
    "        if not len(mal_updates):\n",
    "            mal_updates = torch.stack([mal_update] * n_attacker)\n",
    "        malicious_grads = torch.cat((mal_updates, user_grads), 0)\n",
    "                \n",
    "        if malicious_grads.shape[0] != 60: \n",
    "            print('malicious grads shape ', malicious_grads.shape)\n",
    "            sys.exit()\n",
    "\n",
    "        agg_grads=torch.median(malicious_grads,dim=0)[0]\n",
    "\n",
    "        start_idx=0\n",
    "\n",
    "        if epoch_num in schedule:\n",
    "            for param_group in optimizer_fed.param_groups:\n",
    "                param_group['lr'] *= gamma\n",
    "                print('New learnin rate ', param_group['lr'])\n",
    "\n",
    "        optimizer_fed.zero_grad()\n",
    "\n",
    "        model_grads=[]\n",
    "\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "\n",
    "        optimizer_fed.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if is_best:\n",
    "            best_global_te_acc = te_acc\n",
    "\n",
    "        if epoch_num % 20 == 0 or epoch_num == nepochs-1:\n",
    "            print('%s: at %s n_at %d e %d | val loss %.4f val acc %.4f best val_acc %f te_acc %f'%(aggregation, at_type, n_attacker, epoch_num, val_loss, val_acc, best_global_acc,best_global_te_acc))\n",
    "\n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
