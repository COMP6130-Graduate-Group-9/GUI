{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The notebook contains\n",
    "\n",
    "### Code for _Trimmed-mean_ aggregation algorithm, *when gradient updates of benign clients are unknown to adversary*\n",
    "### Evaluation of all of the attacks (Fang, LIE, and our SOTA AGR-tailored and AGR-agnstic) on Trimmed-mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse, os, sys, csv, shutil, time, random, operator, pickle, ast, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.optim import Optimizer\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "sys.path.insert(0,'./../utils/')\n",
    "from logger import *\n",
    "from eval import *\n",
    "from misc import *\n",
    "\n",
    "from cifar10_normal_train import *\n",
    "from cifar10_util import *\n",
    "from adam import Adam\n",
    "from sgd import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get CIFAR10 data and split it in IID fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "total data len:  60000\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "data_loc='/mnt/nfs/work1/amir/vshejwalkar/cifar10_data/'\n",
    "# load the train dataset\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=train_transform)\n",
    "\n",
    "cifar10_test = datasets.CIFAR10(root=data_loc, train=False, download=True, transform=train_transform)\n",
    "\n",
    "X=[]\n",
    "Y=[]\n",
    "for i in range(len(cifar10_train)):\n",
    "    X.append(cifar10_train[i][0].numpy())\n",
    "    Y.append(cifar10_train[i][1])\n",
    "\n",
    "for i in range(len(cifar10_test)):\n",
    "    X.append(cifar10_test[i][0].numpy())\n",
    "    Y.append(cifar10_test[i][1])\n",
    "\n",
    "X=np.array(X)\n",
    "Y=np.array(Y)\n",
    "\n",
    "print('total data len: ',len(X))\n",
    "\n",
    "if not os.path.isfile('./cifar10_shuffle.pkl'):\n",
    "    all_indices = np.arange(len(X))\n",
    "    np.random.shuffle(all_indices)\n",
    "    pickle.dump(all_indices,open('./cifar10_shuffle.pkl','wb'))\n",
    "else:\n",
    "    all_indices=pickle.load(open('./cifar10_shuffle.pkl','rb'))\n",
    "\n",
    "X=X[all_indices]\n",
    "Y=Y[all_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data len:  60000\n",
      "total tr len 50000 | val len 5000 | test len 5000\n",
      "user 0 tr len 1000\n",
      "user 1 tr len 1000\n",
      "user 2 tr len 1000\n",
      "user 3 tr len 1000\n",
      "user 4 tr len 1000\n",
      "user 5 tr len 1000\n",
      "user 6 tr len 1000\n",
      "user 7 tr len 1000\n",
      "user 8 tr len 1000\n",
      "user 9 tr len 1000\n",
      "user 10 tr len 1000\n",
      "user 11 tr len 1000\n",
      "user 12 tr len 1000\n",
      "user 13 tr len 1000\n",
      "user 14 tr len 1000\n",
      "user 15 tr len 1000\n",
      "user 16 tr len 1000\n",
      "user 17 tr len 1000\n",
      "user 18 tr len 1000\n",
      "user 19 tr len 1000\n",
      "user 20 tr len 1000\n",
      "user 21 tr len 1000\n",
      "user 22 tr len 1000\n",
      "user 23 tr len 1000\n",
      "user 24 tr len 1000\n",
      "user 25 tr len 1000\n",
      "user 26 tr len 1000\n",
      "user 27 tr len 1000\n",
      "user 28 tr len 1000\n",
      "user 29 tr len 1000\n",
      "user 30 tr len 1000\n",
      "user 31 tr len 1000\n",
      "user 32 tr len 1000\n",
      "user 33 tr len 1000\n",
      "user 34 tr len 1000\n",
      "user 35 tr len 1000\n",
      "user 36 tr len 1000\n",
      "user 37 tr len 1000\n",
      "user 38 tr len 1000\n",
      "user 39 tr len 1000\n",
      "user 40 tr len 1000\n",
      "user 41 tr len 1000\n",
      "user 42 tr len 1000\n",
      "user 43 tr len 1000\n",
      "user 44 tr len 1000\n",
      "user 45 tr len 1000\n",
      "user 46 tr len 1000\n",
      "user 47 tr len 1000\n",
      "user 48 tr len 1000\n",
      "user 49 tr len 1000\n"
     ]
    }
   ],
   "source": [
    "# data loading\n",
    "\n",
    "nusers=50\n",
    "user_tr_len=1000\n",
    "\n",
    "total_tr_len=user_tr_len*nusers\n",
    "val_len=5000\n",
    "te_len=5000\n",
    "\n",
    "print('total data len: ',len(X))\n",
    "\n",
    "if not os.path.isfile('./cifar10_shuffle.pkl'):\n",
    "    all_indices = np.arange(len(X))\n",
    "    np.random.shuffle(all_indices)\n",
    "    pickle.dump(all_indices,open('./cifar10_shuffle.pkl','wb'))\n",
    "else:\n",
    "    all_indices=pickle.load(open('./cifar10_shuffle.pkl','rb'))\n",
    "\n",
    "total_tr_data=X[:total_tr_len]\n",
    "total_tr_label=Y[:total_tr_len]\n",
    "\n",
    "val_data=X[total_tr_len:(total_tr_len+val_len)]\n",
    "val_label=Y[total_tr_len:(total_tr_len+val_len)]\n",
    "\n",
    "te_data=X[(total_tr_len+val_len):(total_tr_len+val_len+te_len)]\n",
    "te_label=Y[(total_tr_len+val_len):(total_tr_len+val_len+te_len)]\n",
    "\n",
    "total_tr_data_tensor=torch.from_numpy(total_tr_data).type(torch.FloatTensor)\n",
    "total_tr_label_tensor=torch.from_numpy(total_tr_label).type(torch.LongTensor)\n",
    "\n",
    "val_data_tensor=torch.from_numpy(val_data).type(torch.FloatTensor)\n",
    "val_label_tensor=torch.from_numpy(val_label).type(torch.LongTensor)\n",
    "\n",
    "te_data_tensor=torch.from_numpy(te_data).type(torch.FloatTensor)\n",
    "te_label_tensor=torch.from_numpy(te_label).type(torch.LongTensor)\n",
    "\n",
    "print('total tr len %d | val len %d | test len %d'%(len(total_tr_data_tensor),len(val_data_tensor),len(te_data_tensor)))\n",
    "\n",
    "#==============================================================================================================\n",
    "\n",
    "user_tr_data_tensors=[]\n",
    "user_tr_label_tensors=[]\n",
    "\n",
    "for i in range(nusers):\n",
    "    \n",
    "    user_tr_data_tensor=torch.from_numpy(total_tr_data[user_tr_len*i:user_tr_len*(i+1)]).type(torch.FloatTensor)\n",
    "    user_tr_label_tensor=torch.from_numpy(total_tr_label[user_tr_len*i:user_tr_len*(i+1)]).type(torch.LongTensor)\n",
    "\n",
    "    user_tr_data_tensors.append(user_tr_data_tensor)\n",
    "    user_tr_label_tensors.append(user_tr_label_tensor)\n",
    "    print('user %d tr len %d'%(i,len(user_tr_data_tensor)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Trimmed-mean aggregation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_mean(all_updates, n_attackers):\n",
    "    sorted_updates = torch.sort(all_updates, 0)[0]\n",
    "    out = torch.mean(sorted_updates[n_attackers:-n_attackers], 0) if n_attackers else torch.mean(sorted_updates,0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial knowledge Fang attack on trimmed-mean aggregation\n",
    "### Note that Fang attacks on Trimmed-mean and median are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fang_attack_trmean_partial(all_updates, n_attackers):\n",
    "\n",
    "    model_re = torch.mean(all_updates, 0)\n",
    "    model_std = torch.std(all_updates, 0)\n",
    "    deviation = torch.sign(model_re)\n",
    "    \n",
    "    max_vector_low = model_re + 3 * model_std \n",
    "    max_vector_hig = model_re + 4 * model_std\n",
    "    min_vector_low = model_re - 4 * model_std\n",
    "    min_vector_hig = model_re - 3 * model_std\n",
    "\n",
    "    max_range = torch.cat((max_vector_low[:,None], max_vector_hig[:,None]), dim=1)\n",
    "    min_range = torch.cat((min_vector_low[:,None], min_vector_hig[:,None]), dim=1)\n",
    "\n",
    "    rand = torch.from_numpy(np.random.uniform(0, 1, [len(deviation), n_attackers])).type(torch.FloatTensor).cuda()\n",
    "\n",
    "    max_rand = torch.stack([max_range[:, 0]] * rand.shape[1]).T + rand * torch.stack([max_range[:, 1] - max_range[:, 0]] * rand.shape[1]).T\n",
    "    min_rand = torch.stack([min_range[:, 0]] * rand.shape[1]).T + rand * torch.stack([min_range[:, 1] - min_range[:, 0]] * rand.shape[1]).T\n",
    "\n",
    "    mal_vec = (torch.stack([(deviation > 0).type(torch.FloatTensor)] * max_rand.shape[1]).T.cuda() * max_rand + torch.stack(\n",
    "        [(deviation > 0).type(torch.FloatTensor)] * min_rand.shape[1]).T.cuda() * min_rand).T\n",
    "\n",
    "    return mal_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 2472266])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vshejwalkar/NDSS21-Model-Poisoning/cifar10/sgd.py:109: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729138878/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  p.data.add_(-group['lr'], d_p)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trmean: at fang n_at 10 e 0 fed_model val loss 2.3025 val acc 9.8620 best val_acc 9.862013 te_acc 9.618506\n",
      "trmean: at fang n_at 10 e 25 fed_model val loss 2.2168 val acc 18.9732 best val_acc 18.973214 te_acc 18.810877\n",
      "trmean: at fang n_at 10 e 50 fed_model val loss 2.2950 val acc 12.2768 best val_acc 21.611201 te_acc 22.362013\n",
      "trmean: at fang n_at 10 e 75 fed_model val loss 2.1410 val acc 18.4050 best val_acc 21.611201 te_acc 22.362013\n",
      "trmean: at fang n_at 10 e 100 fed_model val loss 2.0626 val acc 23.2955 best val_acc 23.295455 te_acc 23.153409\n",
      "trmean: at fang n_at 10 e 125 fed_model val loss 2.0220 val acc 23.3360 best val_acc 25.446429 te_acc 25.121753\n",
      "trmean: at fang n_at 10 e 150 fed_model val loss 1.9653 val acc 23.4375 best val_acc 27.820617 te_acc 28.145292\n",
      "trmean: at fang n_at 10 e 175 fed_model val loss 1.9375 val acc 26.9278 best val_acc 29.931006 te_acc 30.458604\n",
      "trmean: at fang n_at 10 e 200 fed_model val loss 1.9631 val acc 26.5016 best val_acc 32.974838 te_acc 33.360390\n",
      "trmean: at fang n_at 10 e 225 fed_model val loss 1.7419 val acc 34.4359 best val_acc 34.435877 te_acc 34.963474\n",
      "trmean: at fang n_at 10 e 250 fed_model val loss 1.8737 val acc 31.0471 best val_acc 37.337662 te_acc 37.987013\n",
      "trmean: at fang n_at 10 e 275 fed_model val loss 1.7963 val acc 34.5576 best val_acc 38.108766 te_acc 38.514610\n",
      "trmean: at fang n_at 10 e 300 fed_model val loss 1.7211 val acc 36.6477 best val_acc 39.711851 te_acc 39.914773\n",
      "trmean: at fang n_at 10 e 325 fed_model val loss 1.7127 val acc 37.9667 best val_acc 42.086039 te_acc 42.633929\n",
      "trmean: at fang n_at 10 e 350 fed_model val loss 1.5441 val acc 43.2224 best val_acc 43.222403 te_acc 44.460227\n",
      "trmean: at fang n_at 10 e 375 fed_model val loss 1.5978 val acc 42.2890 best val_acc 43.222403 te_acc 44.460227\n",
      "trmean: at fang n_at 10 e 400 fed_model val loss 1.6472 val acc 39.0422 best val_acc 44.500812 te_acc 46.347403\n",
      "trmean: at fang n_at 10 e 425 fed_model val loss 1.5592 val acc 42.7760 best val_acc 45.474838 te_acc 47.605519\n",
      "trmean: at fang n_at 10 e 450 fed_model val loss 1.5245 val acc 44.7037 best val_acc 46.469156 te_acc 47.970779\n",
      "trmean: at fang n_at 10 e 475 fed_model val loss 1.5335 val acc 44.9067 best val_acc 46.469156 te_acc 47.970779\n",
      "trmean: at fang n_at 10 e 500 fed_model val loss 1.5175 val acc 45.0893 best val_acc 47.179383 te_acc 48.904221\n",
      "trmean: at fang n_at 10 e 525 fed_model val loss 1.4453 val acc 48.4375 best val_acc 48.437500 te_acc 50.020292\n",
      "trmean: at fang n_at 10 e 550 fed_model val loss 1.5012 val acc 45.9213 best val_acc 48.437500 te_acc 50.020292\n",
      "trmean: at fang n_at 10 e 575 fed_model val loss 1.4423 val acc 48.6404 best val_acc 49.411526 te_acc 50.466721\n",
      "trmean: at fang n_at 10 e 600 fed_model val loss 1.4619 val acc 48.2955 best val_acc 49.411526 te_acc 50.466721\n",
      "trmean: at fang n_at 10 e 625 fed_model val loss 1.4794 val acc 47.7881 best val_acc 49.411526 te_acc 50.466721\n",
      "trmean: at fang n_at 10 e 650 fed_model val loss 1.4438 val acc 48.9651 best val_acc 49.411526 te_acc 50.466721\n",
      "trmean: at fang n_at 10 e 675 fed_model val loss 1.4053 val acc 49.9594 best val_acc 50.202922 te_acc 51.948052\n",
      "trmean: at fang n_at 10 e 700 fed_model val loss 1.4679 val acc 47.5244 best val_acc 50.730519 te_acc 51.278409\n",
      "trmean: at fang n_at 10 e 725 fed_model val loss 1.4061 val acc 50.9131 best val_acc 50.913149 te_acc 51.420455\n",
      "trmean: at fang n_at 10 e 750 fed_model val loss 1.4341 val acc 48.7216 best val_acc 50.913149 te_acc 51.420455\n",
      "trmean: at fang n_at 10 e 775 fed_model val loss 1.4645 val acc 48.2143 best val_acc 50.994318 te_acc 52.394481\n",
      "trmean: at fang n_at 10 e 800 fed_model val loss 1.4452 val acc 48.5998 best val_acc 50.994318 te_acc 52.394481\n",
      "trmean: at fang n_at 10 e 825 fed_model val loss 1.4314 val acc 50.0203 best val_acc 51.968344 te_acc 52.130682\n",
      "trmean: at fang n_at 10 e 850 fed_model val loss 1.4379 val acc 49.6347 best val_acc 51.968344 te_acc 52.130682\n",
      "trmean: at fang n_at 10 e 875 fed_model val loss 1.4261 val acc 49.1883 best val_acc 51.968344 te_acc 52.130682\n",
      "trmean: at fang n_at 10 e 900 fed_model val loss 1.4055 val acc 50.8117 best val_acc 51.968344 te_acc 52.130682\n",
      "trmean: at fang n_at 10 e 925 fed_model val loss 1.4234 val acc 50.3856 best val_acc 51.968344 te_acc 52.130682\n",
      "trmean: at fang n_at 10 e 950 fed_model val loss 1.4221 val acc 50.3044 best val_acc 52.232143 te_acc 52.719156\n",
      "trmean: at fang n_at 10 e 975 fed_model val loss 1.3939 val acc 51.2175 best val_acc 52.232143 te_acc 52.719156\n",
      "New learnin rate  0.25\n",
      "trmean: at fang n_at 10 e 1000 fed_model val loss 1.3792 val acc 51.2987 best val_acc 52.232143 te_acc 52.719156\n",
      "trmean: at fang n_at 10 e 1025 fed_model val loss 1.3702 val acc 52.8612 best val_acc 53.043831 te_acc 54.220779\n",
      "trmean: at fang n_at 10 e 1050 fed_model val loss 1.3778 val acc 52.6177 best val_acc 53.409091 te_acc 54.362825\n",
      "trmean: at fang n_at 10 e 1075 fed_model val loss 1.3866 val acc 52.6583 best val_acc 53.409091 te_acc 54.362825\n",
      "trmean: at fang n_at 10 e 1100 fed_model val loss 1.3887 val acc 52.7800 best val_acc 53.409091 te_acc 54.362825\n",
      "trmean: at fang n_at 10 e 1125 fed_model val loss 1.3982 val acc 52.4554 best val_acc 53.409091 te_acc 54.362825\n",
      "trmean: at fang n_at 10 e 1150 fed_model val loss 1.4035 val acc 52.5568 best val_acc 53.409091 te_acc 54.362825\n",
      "trmean: at fang n_at 10 e 1175 fed_model val loss 1.4064 val acc 52.8612 best val_acc 53.409091 te_acc 54.362825\n",
      "trmean: at fang n_at 10 e 1199 fed_model val loss 1.4068 val acc 52.9221 best val_acc 53.409091 te_acc 54.362825\n",
      "trmean: at fang n_at 10 e 1200 fed_model val loss 1.4088 val acc 52.7394 best val_acc 53.409091 te_acc 54.362825\n"
     ]
    }
   ],
   "source": [
    "batch_size=250\n",
    "resume=0\n",
    "nepochs=1200\n",
    "schedule=[1000]\n",
    "nbatches = user_tr_len//batch_size\n",
    "\n",
    "gamma=.5\n",
    "opt = 'sgd'\n",
    "fed_lr=0.5\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "aggregation='trmean'\n",
    "multi_k = False\n",
    "candidates = []\n",
    "\n",
    "at_type='fang'\n",
    "z_values=[0.0]\n",
    "n_attackers=[10]\n",
    "\n",
    "arch='alexnet'\n",
    "chkpt='./'+aggregation\n",
    "\n",
    "for n_attacker in n_attackers:\n",
    "    epoch_num = 0\n",
    "    best_global_acc = 0\n",
    "    best_global_te_acc = 0\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    r=np.arange(user_tr_len)\n",
    "\n",
    "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
    "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        user_grads=[]\n",
    "        if not epoch_num and epoch_num%nbatches == 0:\n",
    "            np.random.shuffle(r)\n",
    "            for i in range(nusers):\n",
    "                user_tr_data_tensors[i]=user_tr_data_tensors[i][r]\n",
    "                user_tr_label_tensors[i]=user_tr_label_tensors[i][r]\n",
    "\n",
    "        for i in range(nusers):\n",
    "\n",
    "            inputs = user_tr_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "            targets = user_tr_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "            outputs = fed_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            fed_model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            param_grad=[]\n",
    "            for param in fed_model.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "\n",
    "        if epoch_num in schedule:\n",
    "            for param_group in optimizer_fed.param_groups:\n",
    "                param_group['lr'] *= gamma\n",
    "                print('New learnin rate ', param_group['lr'])\n",
    "\n",
    "        if n_attacker > 0:\n",
    "            if at_type == 'lie':\n",
    "                malicious_grad = get_malicious_updates_lie(user_grads[:n_attacker], n_attacker, z, epoch_num)\n",
    "            elif at_type == 'fang':\n",
    "                mal_updates = fang_attack_trmean_partial(user_grads[:n_attacker], n_attacker)\n",
    "\n",
    "        malicious_grads = torch.cat((mal_updates, user_grads[n_attacker:]))\n",
    "        \n",
    "        if not epoch_num : \n",
    "            print(malicious_grads.shape)\n",
    "\n",
    "        agg_grads=tr_mean(malicious_grads, n_attacker)\n",
    "\n",
    "        del user_grads\n",
    "\n",
    "        start_idx=0\n",
    "\n",
    "        optimizer_fed.zero_grad()\n",
    "\n",
    "        model_grads=[]\n",
    "\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "\n",
    "        optimizer_fed.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if is_best:\n",
    "            best_global_te_acc = te_acc\n",
    "\n",
    "        if epoch_num%25==0 or epoch_num==nepochs-1:\n",
    "            print('%s: at %s n_at %d e %d fed_model val loss %.4f val acc %.4f best val_acc %f te_acc %f'%(aggregation, at_type, n_attacker, epoch_num, val_loss, val_acc, best_global_acc,best_global_te_acc))\n",
    "\n",
    "        if val_loss > 10:\n",
    "            print('val loss %f too high'%val_loss)\n",
    "            break\n",
    "\n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for LIE attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lie_attack(all_updates, z):\n",
    "    avg = torch.mean(all_updates, dim=0)\n",
    "    std = torch.std(all_updates, dim=0)\n",
    "    return avg + z * std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 2472266])\n",
      "trmean: at LIE n_at 10 e 0 fed_model val loss 2.3029 val acc 10.2881 best val_acc 10.288149 te_acc 9.659091\n",
      "trmean: at LIE n_at 10 e 10 fed_model val loss 2.3001 val acc 13.1696 best val_acc 13.392857 te_acc 13.230519\n",
      "trmean: at LIE n_at 10 e 20 fed_model val loss 2.2937 val acc 16.4570 best val_acc 16.456981 te_acc 17.065747\n",
      "trmean: at LIE n_at 10 e 30 fed_model val loss 2.2309 val acc 16.3555 best val_acc 17.633929 te_acc 18.080357\n",
      "trmean: at LIE n_at 10 e 40 fed_model val loss 2.2709 val acc 11.2825 best val_acc 19.237013 te_acc 18.486201\n",
      "trmean: at LIE n_at 10 e 50 fed_model val loss 2.2089 val acc 16.7005 best val_acc 19.237013 te_acc 18.486201\n",
      "trmean: at LIE n_at 10 e 60 fed_model val loss 2.1692 val acc 20.9821 best val_acc 20.982143 te_acc 22.443182\n",
      "trmean: at LIE n_at 10 e 70 fed_model val loss 2.1550 val acc 21.3474 best val_acc 21.347403 te_acc 20.332792\n",
      "trmean: at LIE n_at 10 e 80 fed_model val loss 2.1300 val acc 19.8458 best val_acc 21.753247 te_acc 22.504058\n",
      "trmean: at LIE n_at 10 e 90 fed_model val loss 2.3019 val acc 11.5260 best val_acc 21.753247 te_acc 22.504058\n",
      "trmean: at LIE n_at 10 e 100 fed_model val loss 2.0729 val acc 22.0982 best val_acc 22.849026 te_acc 22.564935\n",
      "trmean: at LIE n_at 10 e 110 fed_model val loss 2.0899 val acc 21.7938 best val_acc 22.991071 te_acc 22.930195\n",
      "trmean: at LIE n_at 10 e 120 fed_model val loss 2.0833 val acc 22.8490 best val_acc 23.072240 te_acc 23.498377\n",
      "trmean: at LIE n_at 10 e 130 fed_model val loss 2.0032 val acc 23.9448 best val_acc 24.472403 te_acc 25.284091\n",
      "trmean: at LIE n_at 10 e 140 fed_model val loss 2.0549 val acc 22.8287 best val_acc 26.400162 te_acc 27.394481\n",
      "trmean: at LIE n_at 10 e 150 fed_model val loss 2.0571 val acc 21.9359 best val_acc 27.455357 te_acc 26.765422\n",
      "trmean: at LIE n_at 10 e 160 fed_model val loss 1.9267 val acc 28.5106 best val_acc 28.510552 te_acc 29.464286\n",
      "trmean: at LIE n_at 10 e 170 fed_model val loss 2.1365 val acc 19.3588 best val_acc 29.241071 te_acc 30.133929\n",
      "trmean: at LIE n_at 10 e 180 fed_model val loss 2.0735 val acc 24.3709 best val_acc 29.241071 te_acc 30.133929\n",
      "trmean: at LIE n_at 10 e 190 fed_model val loss 1.9485 val acc 28.7744 best val_acc 29.241071 te_acc 30.133929\n",
      "trmean: at LIE n_at 10 e 200 fed_model val loss 1.8745 val acc 29.8295 best val_acc 29.829545 te_acc 30.012175\n",
      "trmean: at LIE n_at 10 e 210 fed_model val loss 1.9133 val acc 26.8060 best val_acc 30.093344 te_acc 31.128247\n",
      "trmean: at LIE n_at 10 e 220 fed_model val loss 1.8990 val acc 31.2297 best val_acc 31.229708 te_acc 31.534091\n",
      "trmean: at LIE n_at 10 e 230 fed_model val loss 2.0124 val acc 26.8060 best val_acc 33.969156 te_acc 34.719968\n",
      "trmean: at LIE n_at 10 e 240 fed_model val loss 1.8718 val acc 29.8701 best val_acc 33.969156 te_acc 34.719968\n",
      "trmean: at LIE n_at 10 e 250 fed_model val loss 1.8608 val acc 29.5049 best val_acc 33.969156 te_acc 34.719968\n",
      "trmean: at LIE n_at 10 e 260 fed_model val loss 2.0166 val acc 25.1015 best val_acc 33.969156 te_acc 34.719968\n",
      "trmean: at LIE n_at 10 e 270 fed_model val loss 1.7533 val acc 35.3490 best val_acc 35.815747 te_acc 35.551948\n",
      "trmean: at LIE n_at 10 e 280 fed_model val loss 1.7563 val acc 35.0852 best val_acc 36.850649 te_acc 36.911526\n",
      "trmean: at LIE n_at 10 e 290 fed_model val loss 1.8080 val acc 31.1891 best val_acc 38.149351 te_acc 38.900162\n",
      "trmean: at LIE n_at 10 e 300 fed_model val loss 1.6854 val acc 38.4740 best val_acc 38.474026 te_acc 39.123377\n",
      "trmean: at LIE n_at 10 e 310 fed_model val loss 1.7355 val acc 35.5317 best val_acc 39.427760 te_acc 40.787338\n",
      "trmean: at LIE n_at 10 e 320 fed_model val loss 1.7073 val acc 37.5000 best val_acc 39.590097 te_acc 40.726461\n",
      "trmean: at LIE n_at 10 e 330 fed_model val loss 1.7333 val acc 36.5666 best val_acc 40.706169 te_acc 41.497565\n",
      "trmean: at LIE n_at 10 e 340 fed_model val loss 1.6042 val acc 40.0568 best val_acc 41.396104 te_acc 41.964286\n",
      "trmean: at LIE n_at 10 e 350 fed_model val loss 1.6730 val acc 38.7581 best val_acc 41.396104 te_acc 41.964286\n",
      "trmean: at LIE n_at 10 e 360 fed_model val loss 1.6357 val acc 38.9205 best val_acc 43.120942 te_acc 43.810877\n",
      "trmean: at LIE n_at 10 e 370 fed_model val loss 1.5999 val acc 41.5584 best val_acc 43.120942 te_acc 43.810877\n",
      "trmean: at LIE n_at 10 e 380 fed_model val loss 1.5787 val acc 42.1469 best val_acc 43.952922 te_acc 45.231331\n",
      "trmean: at LIE n_at 10 e 390 fed_model val loss 1.5492 val acc 43.7906 best val_acc 44.257305 te_acc 44.805195\n",
      "trmean: at LIE n_at 10 e 400 fed_model val loss 1.7156 val acc 36.5463 best val_acc 44.257305 te_acc 44.805195\n",
      "trmean: at LIE n_at 10 e 410 fed_model val loss 1.5247 val acc 44.4805 best val_acc 44.480519 te_acc 46.164773\n",
      "trmean: at LIE n_at 10 e 420 fed_model val loss 1.5013 val acc 45.7792 best val_acc 46.245942 te_acc 47.037338\n",
      "trmean: at LIE n_at 10 e 430 fed_model val loss 1.5023 val acc 45.2922 best val_acc 46.245942 te_acc 47.037338\n",
      "trmean: at LIE n_at 10 e 440 fed_model val loss 1.5905 val acc 44.7849 best val_acc 46.773539 te_acc 47.788149\n",
      "trmean: at LIE n_at 10 e 450 fed_model val loss 1.5526 val acc 43.4862 best val_acc 46.773539 te_acc 47.788149\n",
      "trmean: at LIE n_at 10 e 460 fed_model val loss 1.6460 val acc 41.0917 best val_acc 46.773539 te_acc 47.788149\n",
      "trmean: at LIE n_at 10 e 470 fed_model val loss 1.4947 val acc 45.7386 best val_acc 46.773539 te_acc 47.788149\n",
      "trmean: at LIE n_at 10 e 480 fed_model val loss 1.4718 val acc 46.0633 best val_acc 46.773539 te_acc 47.788149\n",
      "trmean: at LIE n_at 10 e 490 fed_model val loss 1.5672 val acc 42.6745 best val_acc 48.599838 te_acc 50.182630\n",
      "trmean: at LIE n_at 10 e 500 fed_model val loss 1.5186 val acc 43.3644 best val_acc 48.599838 te_acc 50.182630\n",
      "trmean: at LIE n_at 10 e 510 fed_model val loss 1.4985 val acc 46.5909 best val_acc 48.985390 te_acc 49.857955\n",
      "trmean: at LIE n_at 10 e 520 fed_model val loss 1.4936 val acc 45.9213 best val_acc 48.985390 te_acc 49.857955\n",
      "trmean: at LIE n_at 10 e 530 fed_model val loss 1.7438 val acc 38.6161 best val_acc 50.263799 te_acc 50.892857\n",
      "trmean: at LIE n_at 10 e 540 fed_model val loss 1.5393 val acc 43.1615 best val_acc 51.116071 te_acc 52.658279\n",
      "trmean: at LIE n_at 10 e 550 fed_model val loss 1.4193 val acc 49.0666 best val_acc 51.116071 te_acc 52.658279\n",
      "trmean: at LIE n_at 10 e 560 fed_model val loss 1.6834 val acc 39.7727 best val_acc 51.116071 te_acc 52.658279\n",
      "trmean: at LIE n_at 10 e 570 fed_model val loss 1.3948 val acc 50.5073 best val_acc 51.116071 te_acc 52.658279\n",
      "trmean: at LIE n_at 10 e 580 fed_model val loss 1.3920 val acc 50.5682 best val_acc 51.116071 te_acc 52.658279\n",
      "trmean: at LIE n_at 10 e 590 fed_model val loss 1.4413 val acc 46.8750 best val_acc 51.521916 te_acc 52.232143\n",
      "trmean: at LIE n_at 10 e 600 fed_model val loss 1.5212 val acc 46.3068 best val_acc 51.907468 te_acc 52.800325\n",
      "trmean: at LIE n_at 10 e 610 fed_model val loss 1.4881 val acc 46.7330 best val_acc 51.907468 te_acc 52.800325\n",
      "trmean: at LIE n_at 10 e 620 fed_model val loss 1.3668 val acc 51.4610 best val_acc 52.008929 te_acc 53.125000\n",
      "trmean: at LIE n_at 10 e 630 fed_model val loss 1.3260 val acc 52.6786 best val_acc 52.678571 te_acc 54.078734\n",
      "trmean: at LIE n_at 10 e 640 fed_model val loss 1.4475 val acc 48.5795 best val_acc 53.003247 te_acc 53.632305\n",
      "trmean: at LIE n_at 10 e 650 fed_model val loss 1.3124 val acc 53.9976 best val_acc 53.997565 te_acc 54.667208\n",
      "trmean: at LIE n_at 10 e 660 fed_model val loss 1.3612 val acc 50.8320 best val_acc 53.997565 te_acc 54.667208\n",
      "trmean: at LIE n_at 10 e 670 fed_model val loss 1.3490 val acc 51.3596 best val_acc 53.997565 te_acc 54.667208\n",
      "trmean: at LIE n_at 10 e 680 fed_model val loss 1.3626 val acc 51.8060 best val_acc 53.997565 te_acc 54.667208\n",
      "trmean: at LIE n_at 10 e 690 fed_model val loss 2.2420 val acc 28.1859 best val_acc 53.997565 te_acc 54.667208\n",
      "val loss 20.460872 too high\n"
     ]
    }
   ],
   "source": [
    "batch_size=250\n",
    "resume=0\n",
    "nepochs=1200\n",
    "schedule=[1000]\n",
    "nbatches = user_tr_len//batch_size\n",
    "\n",
    "gamma=.5\n",
    "opt = 'sgd'\n",
    "fed_lr=0.5\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "aggregation='trmean'\n",
    "multi_k = False\n",
    "candidates = []\n",
    "\n",
    "at_type='LIE'\n",
    "z_values={3:0.69847, 5:0.7054, 8:0.71904, 10:0.72575, 12:0.73891}\n",
    "n_attackers=[10]\n",
    "\n",
    "arch='alexnet'\n",
    "chkpt='./'+aggregation\n",
    "\n",
    "for n_attacker in n_attackers:\n",
    "    epoch_num = 0\n",
    "    best_global_acc = 0\n",
    "    best_global_te_acc = 0\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    r=np.arange(user_tr_len)\n",
    "\n",
    "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
    "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        user_grads=[]\n",
    "        if not epoch_num and epoch_num%nbatches == 0:\n",
    "            np.random.shuffle(r)\n",
    "            for i in range(nusers):\n",
    "                user_tr_data_tensors[i]=user_tr_data_tensors[i][r]\n",
    "                user_tr_label_tensors[i]=user_tr_label_tensors[i][r]\n",
    "\n",
    "        for i in range(nusers):\n",
    "\n",
    "            inputs = user_tr_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "            targets = user_tr_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "            outputs = fed_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            fed_model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            param_grad=[]\n",
    "            for param in fed_model.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "\n",
    "        if epoch_num in schedule:\n",
    "            for param_group in optimizer_fed.param_groups:\n",
    "                param_group['lr'] *= gamma\n",
    "                print('New learnin rate ', param_group['lr'])\n",
    "\n",
    "        if n_attacker > 0:\n",
    "            if at_type == 'lie':\n",
    "                mal_update = lie_attack(user_grads[:n_attacker], z_values[n_attacker])\n",
    "                mal_updates = torch.cat((torch.stack([mal_update]*n_attacker), malicious_grads))\n",
    "            elif at_type == 'fang':\n",
    "                mal_updates = fang_attack_trmean_partial(user_grads[:n_attacker], n_attacker)\n",
    "\n",
    "        malicious_grads = torch.cat((mal_updates, user_grads[n_attacker:]))\n",
    "        \n",
    "        if not epoch_num : \n",
    "            print(malicious_grads.shape)\n",
    "        \n",
    "        agg_grads=tr_mean(malicious_grads, n_attacker)\n",
    "\n",
    "        del user_grads\n",
    "\n",
    "        start_idx=0\n",
    "\n",
    "        optimizer_fed.zero_grad()\n",
    "\n",
    "        model_grads=[]\n",
    "\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "\n",
    "        optimizer_fed.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if is_best:\n",
    "            best_global_te_acc = te_acc\n",
    "\n",
    "        if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "            print('%s: at %s n_at %d e %d fed_model val loss %.4f val acc %.4f best val_acc %f te_acc %f'%(aggregation, at_type, n_attacker, epoch_num, val_loss, val_acc, best_global_acc,best_global_te_acc))\n",
    "\n",
    "        if val_loss > 10:\n",
    "            print('val loss %f too high'%val_loss)\n",
    "            break\n",
    "\n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for our AGR-tailored attack on Trimmed-mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_attack_trmean(all_updates, model_re, n_attackers, dev_type='unit_vec', threshold=30):\n",
    "\n",
    "    if dev_type == 'unit_vec':\n",
    "        deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "    elif dev_type == 'sign':\n",
    "        deviation = torch.sign(model_re)\n",
    "    elif dev_type == 'std':\n",
    "        deviation = torch.std(all_updates, 0)\n",
    "\n",
    "    lamda = torch.Tensor([threshold]).cuda() #compute_lambda_our(all_updates, model_re, n_attackers)\n",
    "    # print(lamda)\n",
    "    threshold_diff = 1e-5\n",
    "    prev_loss = -1\n",
    "    lamda_fail = lamda\n",
    "    lamda_succ = 0\n",
    "    iters = 0 \n",
    "    while torch.abs(lamda_succ - lamda) > threshold_diff:\n",
    "        mal_update = (model_re - lamda * deviation)\n",
    "        mal_updates = torch.stack([mal_update] * n_attackers)\n",
    "        mal_updates = torch.cat((mal_updates, all_updates), 0)\n",
    "\n",
    "        agg_grads = tr_mean(mal_updates, n_attackers)\n",
    "        \n",
    "        loss = torch.norm(agg_grads - model_re)\n",
    "        \n",
    "        if prev_loss < loss:\n",
    "            # print('successful lamda is ', lamda)\n",
    "            lamda_succ = lamda\n",
    "            lamda = lamda + lamda_fail / 2\n",
    "        else:\n",
    "            lamda = lamda - lamda_fail / 2\n",
    "\n",
    "        lamda_fail = lamda_fail / 2\n",
    "        prev_loss = loss\n",
    "        \n",
    "    mal_update = (model_re - lamda_succ * deviation)\n",
    "    return mal_update\n",
    "    mal_updates = torch.stack([mal_update] * n_attackers)\n",
    "    mal_updates = torch.cat((mal_updates, all_updates), 0)\n",
    "\n",
    "    return mal_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trmean: at our-agr n_at 10 e 0 fed_model val loss 2.3032 val acc 9.7403 best val_acc 9.740260 te_acc 10.125812\n",
      "trmean: at our-agr n_at 10 e 25 fed_model val loss 2.3458 val acc 10.0852 best val_acc 20.596591 te_acc 21.022727\n",
      "trmean: at our-agr n_at 10 e 50 fed_model val loss 2.2755 val acc 14.1843 best val_acc 20.921266 te_acc 21.489448\n",
      "trmean: at our-agr n_at 10 e 75 fed_model val loss 2.2991 val acc 13.8190 best val_acc 20.921266 te_acc 21.489448\n",
      "trmean: at our-agr n_at 10 e 100 fed_model val loss 2.3228 val acc 9.9432 best val_acc 20.921266 te_acc 21.489448\n",
      "trmean: at our-agr n_at 10 e 125 fed_model val loss 2.1959 val acc 16.9846 best val_acc 20.921266 te_acc 21.489448\n",
      "trmean: at our-agr n_at 10 e 150 fed_model val loss 2.3341 val acc 16.5990 best val_acc 20.921266 te_acc 21.489448\n",
      "trmean: at our-agr n_at 10 e 175 fed_model val loss 2.3174 val acc 9.7403 best val_acc 20.921266 te_acc 21.489448\n",
      "trmean: at our-agr n_at 10 e 200 fed_model val loss 2.2994 val acc 11.7289 best val_acc 20.921266 te_acc 21.489448\n",
      "trmean: at our-agr n_at 10 e 225 fed_model val loss 2.1656 val acc 17.5731 best val_acc 20.921266 te_acc 21.489448\n",
      "trmean: at our-agr n_at 10 e 250 fed_model val loss 2.1641 val acc 17.9789 best val_acc 20.921266 te_acc 21.489448\n",
      "trmean: at our-agr n_at 10 e 275 fed_model val loss 2.1867 val acc 20.7183 best val_acc 20.921266 te_acc 21.489448\n",
      "trmean: at our-agr n_at 10 e 300 fed_model val loss 2.1786 val acc 17.5122 best val_acc 22.930195 te_acc 23.863636\n",
      "trmean: at our-agr n_at 10 e 325 fed_model val loss 2.1188 val acc 18.9529 best val_acc 22.930195 te_acc 23.863636\n",
      "trmean: at our-agr n_at 10 e 350 fed_model val loss 2.3303 val acc 9.8417 best val_acc 22.930195 te_acc 23.863636\n",
      "trmean: at our-agr n_at 10 e 375 fed_model val loss 2.3181 val acc 11.6883 best val_acc 22.930195 te_acc 23.863636\n",
      "trmean: at our-agr n_at 10 e 400 fed_model val loss 2.2397 val acc 14.9351 best val_acc 22.930195 te_acc 23.863636\n",
      "trmean: at our-agr n_at 10 e 425 fed_model val loss 2.2851 val acc 11.4651 best val_acc 22.930195 te_acc 23.863636\n",
      "trmean: at our-agr n_at 10 e 450 fed_model val loss 2.3182 val acc 10.3084 best val_acc 22.930195 te_acc 23.863636\n",
      "trmean: at our-agr n_at 10 e 475 fed_model val loss 2.1459 val acc 18.3239 best val_acc 22.930195 te_acc 23.863636\n",
      "trmean: at our-agr n_at 10 e 500 fed_model val loss 2.1833 val acc 15.2192 best val_acc 22.930195 te_acc 23.863636\n",
      "trmean: at our-agr n_at 10 e 525 fed_model val loss 2.8272 val acc 17.0049 best val_acc 25.608766 te_acc 26.278409\n",
      "trmean: at our-agr n_at 10 e 550 fed_model val loss 2.0324 val acc 24.6144 best val_acc 25.608766 te_acc 26.278409\n",
      "trmean: at our-agr n_at 10 e 575 fed_model val loss 1.9817 val acc 23.3563 best val_acc 25.608766 te_acc 26.278409\n",
      "trmean: at our-agr n_at 10 e 600 fed_model val loss 2.4630 val acc 8.7865 best val_acc 26.420455 te_acc 26.846591\n",
      "trmean: at our-agr n_at 10 e 625 fed_model val loss 2.1051 val acc 18.5268 best val_acc 26.420455 te_acc 26.846591\n",
      "trmean: at our-agr n_at 10 e 750 fed_model val loss 2.1126 val acc 18.5065 best val_acc 26.420455 te_acc 26.846591\n",
      "trmean: at our-agr n_at 10 e 775 fed_model val loss 2.1363 val acc 18.3036 best val_acc 26.420455 te_acc 26.846591\n",
      "trmean: at our-agr n_at 10 e 800 fed_model val loss 2.3563 val acc 20.2313 best val_acc 26.420455 te_acc 26.846591\n",
      "trmean: at our-agr n_at 10 e 825 fed_model val loss 1.9940 val acc 24.4521 best val_acc 26.420455 te_acc 26.846591\n",
      "trmean: at our-agr n_at 10 e 850 fed_model val loss 2.2734 val acc 14.6307 best val_acc 27.495942 te_acc 26.542208\n",
      "trmean: at our-agr n_at 10 e 875 fed_model val loss 2.1689 val acc 18.1412 best val_acc 27.495942 te_acc 26.542208\n",
      "trmean: at our-agr n_at 10 e 900 fed_model val loss 2.5565 val acc 11.2825 best val_acc 27.495942 te_acc 26.542208\n",
      "trmean: at our-agr n_at 10 e 925 fed_model val loss 1.9940 val acc 21.7938 best val_acc 27.495942 te_acc 26.542208\n",
      "trmean: at our-agr n_at 10 e 950 fed_model val loss 2.0165 val acc 23.6607 best val_acc 27.495942 te_acc 26.542208\n",
      "val loss 39.506796 too high\n"
     ]
    }
   ],
   "source": [
    "batch_size=250\n",
    "resume=0\n",
    "nepochs=1200\n",
    "schedule=[1000]\n",
    "nbatches = user_tr_len//batch_size\n",
    "\n",
    "gamma=.5\n",
    "opt = 'sgd'\n",
    "fed_lr=0.5\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "aggregation='trmean'\n",
    "multi_k = False\n",
    "candidates = []\n",
    "\n",
    "at_type='our-agr'\n",
    "dev_type = 'std'\n",
    "threshold=10\n",
    "n_attackers=[10]\n",
    "partial_attackers = {4:1, 5:1, 8:2, 10:3, 12:4}\n",
    "\n",
    "arch='alexnet'\n",
    "chkpt='./'+aggregation\n",
    "\n",
    "for n_attacker in n_attackers:\n",
    "    epoch_num = 0\n",
    "    best_global_acc = 0\n",
    "    best_global_te_acc = 0\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    r=np.arange(user_tr_len)\n",
    "\n",
    "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
    "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        user_grads=[]\n",
    "        if not epoch_num and epoch_num%nbatches == 0:\n",
    "            np.random.shuffle(r)\n",
    "            for i in range(nusers):\n",
    "                user_tr_data_tensors[i]=user_tr_data_tensors[i][r]\n",
    "                user_tr_label_tensors[i]=user_tr_label_tensors[i][r]\n",
    "\n",
    "        for i in range(nusers):\n",
    "\n",
    "            inputs = user_tr_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "            targets = user_tr_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "            outputs = fed_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            fed_model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            param_grad=[]\n",
    "            for param in fed_model.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "\n",
    "        if epoch_num in schedule:\n",
    "            for param_group in optimizer_fed.param_groups:\n",
    "                param_group['lr'] *= gamma\n",
    "                print('New learnin rate ', param_group['lr'])\n",
    "\n",
    "        if n_attacker > 0:\n",
    "            if at_type == 'lie':\n",
    "                mal_update = lie_attack(user_grads[:n_attacker], z_values[n_attacker])\n",
    "                mal_updates = torch.cat((torch.stack([mal_update]*n_attacker), malicious_grads))\n",
    "            elif at_type == 'our-agr':\n",
    "                n_attacker_ = partial_attackers[n_attacker]\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                mal_update = our_attack_trmean(user_grads[:n_attacker], agg_grads, n_attacker_, threshold=threshold, dev_type=dev_type)\n",
    "                mal_updates = torch.stack([mal_update]*n_attacker)\n",
    "                \n",
    "        malicious_grads = torch.cat((mal_updates, user_grads[n_attacker:]))\n",
    "        \n",
    "        if not (malicious_grads.shape[0]==50):\n",
    "            print(malicious_grads.shape)\n",
    "            sys.exit()\n",
    "\n",
    "        agg_grads=tr_mean(malicious_grads, n_attacker)\n",
    "\n",
    "        del user_grads\n",
    "\n",
    "        start_idx=0\n",
    "\n",
    "        optimizer_fed.zero_grad()\n",
    "\n",
    "        model_grads=[]\n",
    "\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "\n",
    "        optimizer_fed.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if is_best:\n",
    "            best_global_te_acc = te_acc\n",
    "\n",
    "        if epoch_num%25==0 or epoch_num==nepochs-1:\n",
    "            print('%s: at %s n_at %d e %d fed_model val loss %.4f val acc %.4f best val_acc %f te_acc %f'%(aggregation, at_type, n_attacker, epoch_num, val_loss, val_acc, best_global_acc,best_global_te_acc))\n",
    "\n",
    "        if val_loss > 10:\n",
    "            print('val loss %f too high'%val_loss)\n",
    "            break\n",
    "\n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for our first AGR-agnostic attack - Min-max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MIN-MAX attack\n",
    "'''\n",
    "def our_attack_dist(all_updates, model_re, n_attackers, dev_type='unit_vec', threshold=30):\n",
    "\n",
    "    if dev_type == 'unit_vec':\n",
    "        deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "    elif dev_type == 'sign':\n",
    "        deviation = torch.sign(model_re)\n",
    "    elif dev_type == 'std':\n",
    "        deviation = torch.std(all_updates, 0)\n",
    "\n",
    "    lamda = torch.Tensor([threshold]).float().cuda()\n",
    "    # print(lamda)\n",
    "    threshold_diff = 1e-5\n",
    "    lamda_fail = lamda\n",
    "    lamda_succ = 0\n",
    "    \n",
    "    distances = []\n",
    "    for update in all_updates:\n",
    "        distance = torch.norm((all_updates - update), dim=1) ** 2\n",
    "        distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)\n",
    "    \n",
    "    max_distance = torch.max(distances)\n",
    "    del distances\n",
    "\n",
    "    while torch.abs(lamda_succ - lamda) > threshold_diff:\n",
    "        mal_update = (model_re - lamda * deviation)\n",
    "        distance = torch.norm((all_updates - mal_update), dim=1) ** 2\n",
    "        max_d = torch.max(distance)\n",
    "        \n",
    "        if max_d <= max_distance:\n",
    "            # print('successful lamda is ', lamda)\n",
    "            lamda_succ = lamda\n",
    "            lamda = lamda + lamda_fail / 2\n",
    "        else:\n",
    "            lamda = lamda - lamda_fail / 2\n",
    "\n",
    "        lamda_fail = lamda_fail / 2\n",
    "\n",
    "    mal_update = (model_re - lamda_succ * deviation)\n",
    "    \n",
    "    return mal_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vshejwalkar/NDSS21-Model-Poisoning/cifar10/sgd.py:109: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729138878/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  p.data.add_(-group['lr'], d_p)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trmean: at min-max n_at 10 | e 0 fed_model val loss 2.3034 val acc 10.2881 best val_acc 10.288149 te_acc 9.659091\n",
      "trmean: at min-max n_at 10 | e 25 fed_model val loss 2.2930 val acc 10.7143 best val_acc 19.947240 te_acc 20.535714\n",
      "trmean: at min-max n_at 10 | e 50 fed_model val loss 2.1543 val acc 20.5154 best val_acc 20.515422 te_acc 21.002435\n",
      "trmean: at min-max n_at 10 | e 75 fed_model val loss 2.1215 val acc 21.6112 best val_acc 21.611201 te_acc 21.814123\n",
      "trmean: at min-max n_at 10 | e 100 fed_model val loss 2.0867 val acc 24.8377 best val_acc 24.837662 te_acc 25.507305\n",
      "trmean: at min-max n_at 10 | e 125 fed_model val loss 2.1190 val acc 22.7273 best val_acc 24.837662 te_acc 25.507305\n",
      "trmean: at min-max n_at 10 | e 150 fed_model val loss 2.1171 val acc 20.9821 best val_acc 24.837662 te_acc 25.507305\n",
      "trmean: at min-max n_at 10 | e 175 fed_model val loss 2.0823 val acc 24.4724 best val_acc 25.892857 te_acc 26.663961\n",
      "trmean: at min-max n_at 10 | e 200 fed_model val loss 2.0366 val acc 25.2435 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 225 fed_model val loss 2.2868 val acc 14.1031 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 250 fed_model val loss 2.2573 val acc 12.0130 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 275 fed_model val loss 2.2627 val acc 14.7321 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 300 fed_model val loss 2.2014 val acc 15.9903 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 325 fed_model val loss 2.3113 val acc 10.2476 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 350 fed_model val loss 2.3028 val acc 9.7606 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 375 fed_model val loss 2.3027 val acc 9.7606 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 400 fed_model val loss 2.3027 val acc 9.5982 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 425 fed_model val loss 2.3027 val acc 9.9635 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 450 fed_model val loss 2.3027 val acc 9.7606 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 475 fed_model val loss 2.3027 val acc 9.7606 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 500 fed_model val loss 2.3027 val acc 9.5982 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 525 fed_model val loss 2.3027 val acc 9.9635 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 550 fed_model val loss 2.3027 val acc 9.7606 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 575 fed_model val loss 2.3027 val acc 9.7606 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 600 fed_model val loss 2.3028 val acc 9.5982 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 625 fed_model val loss 2.3027 val acc 9.9635 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 650 fed_model val loss 2.3028 val acc 9.7606 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 675 fed_model val loss 2.3027 val acc 9.7606 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 700 fed_model val loss 2.3028 val acc 9.5982 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 725 fed_model val loss 2.3027 val acc 9.9635 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 750 fed_model val loss 2.3028 val acc 9.7606 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 775 fed_model val loss 2.3027 val acc 9.7606 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 800 fed_model val loss 2.3029 val acc 9.5982 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 825 fed_model val loss 2.3027 val acc 9.9635 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 850 fed_model val loss 2.3029 val acc 9.7606 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 875 fed_model val loss 2.3027 val acc 10.2881 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 900 fed_model val loss 2.3029 val acc 9.5982 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 925 fed_model val loss 2.3027 val acc 9.9635 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 950 fed_model val loss 2.3030 val acc 9.7606 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 975 fed_model val loss 2.3028 val acc 10.2881 best val_acc 26.501623 te_acc 27.739448\n",
      "New learnin rate  0.25\n",
      "trmean: at min-max n_at 10 | e 1000 fed_model val loss 2.3028 val acc 9.5982 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 1025 fed_model val loss 2.3027 val acc 9.9635 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 1050 fed_model val loss 2.3028 val acc 9.7606 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 1075 fed_model val loss 2.3027 val acc 9.7606 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 1100 fed_model val loss 2.3028 val acc 9.5982 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 1125 fed_model val loss 2.3027 val acc 9.9635 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 1150 fed_model val loss 2.3028 val acc 9.7606 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 1175 fed_model val loss 2.3027 val acc 9.7606 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 1199 fed_model val loss 2.3027 val acc 9.7606 best val_acc 26.501623 te_acc 27.739448\n",
      "trmean: at min-max n_at 10 | e 1200 fed_model val loss 2.3029 val acc 9.5982 best val_acc 26.501623 te_acc 27.739448\n"
     ]
    }
   ],
   "source": [
    "batch_size=250\n",
    "resume=0\n",
    "nepochs=1200\n",
    "schedule=[1000]\n",
    "nbatches = user_tr_len//batch_size\n",
    "\n",
    "gamma=.5\n",
    "opt = 'sgd'\n",
    "fed_lr=0.5\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "aggregation='trmean'\n",
    "multi_k = False\n",
    "candidates = []\n",
    "\n",
    "at_type='min-max'\n",
    "dev_type ='std'\n",
    "threshold=10\n",
    "n_attackers=[10]\n",
    "partial_attackers = {4:1, 5:1, 8:2, 10:3, 12:4}\n",
    "\n",
    "arch='alexnet'\n",
    "chkpt='./'+aggregation\n",
    "\n",
    "for n_attacker in n_attackers:\n",
    "    candidates = []\n",
    "\n",
    "    epoch_num = 0\n",
    "    best_global_acc = 0\n",
    "    best_global_te_acc = 0\n",
    "\n",
    "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
    "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    r=np.arange(user_tr_len)\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        user_grads=[]\n",
    "        if not epoch_num and epoch_num%nbatches == 0:\n",
    "            np.random.shuffle(r)\n",
    "            for i in range(nusers):\n",
    "                user_tr_data_tensors[i]=user_tr_data_tensors[i][r]\n",
    "                user_tr_label_tensors[i]=user_tr_label_tensors[i][r]\n",
    "\n",
    "        for i in range(nusers):\n",
    "\n",
    "            inputs = user_tr_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "            targets = user_tr_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "            outputs = fed_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            fed_model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            param_grad=[]\n",
    "            for param in fed_model.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "\n",
    "        if epoch_num in schedule:\n",
    "            for param_group in optimizer_fed.param_groups:\n",
    "                param_group['lr'] *= gamma\n",
    "                print('New learnin rate ', param_group['lr'])\n",
    "\n",
    "        if n_attacker > 0:\n",
    "            if at_type == 'lie':\n",
    "                mal_update = lie_attack(user_grads[:n_attacker], z_values[n_attacker])\n",
    "            elif at_type == 'fang':\n",
    "                mal_updates = fang_attack_trmean_partial(user_grads[:n_attacker], n_attacker)\n",
    "            elif at_type == 'our-agr':\n",
    "                n_attacker_ = partial_attackers[n_attacker]\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                mal_update = our_attack_median(user_grads[:n_attacker], agg_grads, n_attacker_, threshold=threshold, dev_type=dev_type)\n",
    "            elif at_type == 'min-max':\n",
    "                n_attacker_ = partial_attackers[n_attacker]\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                mal_update = our_attack_dist(user_grads[:n_attacker], agg_grads, n_attacker_, threshold=threshold, dev_type=dev_type)\n",
    "            elif at_type == 'min-sum':\n",
    "                n_attacker_ = partial_attackers[n_attacker]\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                mal_update = our_attack_score(user_grads[:n_attacker], agg_grads, n_attacker_, threshold=threshold, dev_type=dev_type)\n",
    "\n",
    "            mal_updates = torch.stack([mal_update] * n_attacker)\n",
    "            malicious_grads = torch.cat((mal_updates, user_grads[n_attacker:]), 0)\n",
    "\n",
    "        if not (malicious_grads.shape[0]==50):\n",
    "            print(malicious_grads.shape)\n",
    "            sys.exit()\n",
    "\n",
    "        agg_grads=tr_mean(malicious_grads, n_attacker)\n",
    "\n",
    "        del user_grads\n",
    "\n",
    "        start_idx=0\n",
    "\n",
    "        optimizer_fed.zero_grad()\n",
    "\n",
    "        model_grads=[]\n",
    "\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "\n",
    "        optimizer_fed.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if is_best:\n",
    "            best_global_te_acc = te_acc\n",
    "\n",
    "        if epoch_num%25==0 or epoch_num==nepochs-1:\n",
    "            print('%s: at %s n_at %d | e %d fed_model val loss %.4f val acc %.4f best val_acc %f te_acc %f'%(aggregation, at_type, n_attacker, epoch_num, val_loss, val_acc, best_global_acc,best_global_te_acc))\n",
    "\n",
    "        if val_loss > 1000:\n",
    "            print('val loss %f too high'%val_loss)\n",
    "            break\n",
    "            \n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for our second AGR-agnostic attack - Min-sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MIN-SUM attack\n",
    "'''\n",
    "\n",
    "def our_attack_score(all_updates, model_re, n_attackers, dev_type='unit_vec', threshold=30):\n",
    "\n",
    "    if dev_type == 'unit_vec':\n",
    "        deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "    elif dev_type == 'sign':\n",
    "        deviation = torch.sign(model_re)\n",
    "    elif dev_type == 'std':\n",
    "        deviation = torch.std(all_updates, 0)\n",
    "    \n",
    "    lamda = torch.Tensor([threshold]).float().cuda()\n",
    "    # print(lamda)\n",
    "    threshold_diff = 1e-5\n",
    "    lamda_fail = lamda\n",
    "    lamda_succ = 0\n",
    "    \n",
    "    distances = []\n",
    "    for update in all_updates:\n",
    "        distance = torch.norm((all_updates - update), dim=1) ** 2\n",
    "        distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)\n",
    "    \n",
    "    scores = torch.sum(distances, dim=1)\n",
    "    min_score = torch.min(scores)\n",
    "    del distances\n",
    "\n",
    "    while torch.abs(lamda_succ - lamda) > threshold_diff:\n",
    "        mal_update = (model_re - lamda * deviation)\n",
    "        distance = torch.norm((all_updates - mal_update), dim=1) ** 2\n",
    "        score = torch.sum(distance)\n",
    "        \n",
    "        if score <= min_score:\n",
    "            # print('successful lamda is ', lamda)\n",
    "            lamda_succ = lamda\n",
    "            lamda = lamda + lamda_fail / 2\n",
    "        else:\n",
    "            lamda = lamda - lamda_fail / 2\n",
    "\n",
    "        lamda_fail = lamda_fail / 2\n",
    "\n",
    "    # print(lamda_succ)\n",
    "    mal_update = (model_re - lamda_succ * deviation)\n",
    "    \n",
    "    return mal_update\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trmean: at min-sum n_at 10 | e 0 fed_model val loss 2.3026 val acc 10.2476 best val_acc 10.247565 te_acc 9.922890\n",
      "trmean: at min-sum n_at 10 | e 10 fed_model val loss 2.2941 val acc 15.7062 best val_acc 15.706169 te_acc 15.746753\n",
      "trmean: at min-sum n_at 10 | e 20 fed_model val loss 2.2267 val acc 16.6802 best val_acc 21.875000 te_acc 22.301136\n",
      "trmean: at min-sum n_at 10 | e 30 fed_model val loss 2.3322 val acc 16.9440 best val_acc 22.706981 te_acc 23.092532\n",
      "trmean: at min-sum n_at 10 | e 40 fed_model val loss 2.3030 val acc 10.3896 best val_acc 22.706981 te_acc 23.092532\n",
      "trmean: at min-sum n_at 10 | e 50 fed_model val loss 2.3024 val acc 10.3896 best val_acc 22.706981 te_acc 23.092532\n",
      "trmean: at min-sum n_at 10 | e 60 fed_model val loss 2.3019 val acc 12.4188 best val_acc 22.706981 te_acc 23.092532\n",
      "trmean: at min-sum n_at 10 | e 70 fed_model val loss 2.3009 val acc 12.8247 best val_acc 22.706981 te_acc 23.092532\n",
      "trmean: at min-sum n_at 10 | e 80 fed_model val loss 2.2976 val acc 14.8133 best val_acc 22.706981 te_acc 23.092532\n",
      "trmean: at min-sum n_at 10 | e 90 fed_model val loss 2.2791 val acc 13.2305 best val_acc 22.706981 te_acc 23.092532\n",
      "trmean: at min-sum n_at 10 | e 100 fed_model val loss 2.4411 val acc 11.2419 best val_acc 22.706981 te_acc 23.092532\n",
      "trmean: at min-sum n_at 10 | e 110 fed_model val loss 2.3063 val acc 9.7403 best val_acc 22.706981 te_acc 23.092532\n",
      "trmean: at min-sum n_at 10 | e 120 fed_model val loss 2.2926 val acc 16.2541 best val_acc 22.706981 te_acc 23.092532\n",
      "trmean: at min-sum n_at 10 | e 130 fed_model val loss 2.3059 val acc 9.8620 best val_acc 22.706981 te_acc 23.092532\n",
      "trmean: at min-sum n_at 10 | e 140 fed_model val loss 2.2891 val acc 14.4075 best val_acc 22.706981 te_acc 23.092532\n",
      "trmean: at min-sum n_at 10 | e 150 fed_model val loss 2.1844 val acc 18.5877 best val_acc 22.706981 te_acc 23.092532\n",
      "trmean: at min-sum n_at 10 | e 160 fed_model val loss 2.1267 val acc 19.9067 best val_acc 22.706981 te_acc 23.092532\n",
      "trmean: at min-sum n_at 10 | e 170 fed_model val loss 2.1787 val acc 16.3961 best val_acc 22.706981 te_acc 23.092532\n",
      "trmean: at min-sum n_at 10 | e 180 fed_model val loss 2.1096 val acc 18.4862 best val_acc 22.706981 te_acc 23.092532\n",
      "trmean: at min-sum n_at 10 | e 190 fed_model val loss 2.0861 val acc 18.8920 best val_acc 22.706981 te_acc 23.092532\n",
      "trmean: at min-sum n_at 10 | e 200 fed_model val loss 2.1003 val acc 20.2313 best val_acc 22.706981 te_acc 23.092532\n",
      "trmean: at min-sum n_at 10 | e 210 fed_model val loss 2.1391 val acc 20.3531 best val_acc 22.706981 te_acc 23.092532\n",
      "trmean: at min-sum n_at 10 | e 220 fed_model val loss 2.1472 val acc 16.1932 best val_acc 22.706981 te_acc 23.092532\n",
      "trmean: at min-sum n_at 10 | e 230 fed_model val loss 2.0744 val acc 17.6948 best val_acc 22.706981 te_acc 23.092532\n",
      "trmean: at min-sum n_at 10 | e 240 fed_model val loss 2.2584 val acc 13.5958 best val_acc 24.005682 te_acc 24.918831\n",
      "trmean: at min-sum n_at 10 | e 250 fed_model val loss 2.0394 val acc 21.4692 best val_acc 24.005682 te_acc 24.918831\n",
      "trmean: at min-sum n_at 10 | e 260 fed_model val loss 2.1144 val acc 20.3937 best val_acc 25.040584 te_acc 26.116071\n",
      "trmean: at min-sum n_at 10 | e 270 fed_model val loss 2.0221 val acc 22.9099 best val_acc 25.040584 te_acc 26.116071\n",
      "trmean: at min-sum n_at 10 | e 280 fed_model val loss 2.1104 val acc 18.9935 best val_acc 27.577110 te_acc 28.003247\n",
      "trmean: at min-sum n_at 10 | e 290 fed_model val loss 2.4727 val acc 15.0568 best val_acc 27.577110 te_acc 28.003247\n",
      "trmean: at min-sum n_at 10 | e 300 fed_model val loss 2.1721 val acc 21.7735 best val_acc 30.073052 te_acc 31.777597\n",
      "trmean: at min-sum n_at 10 | e 310 fed_model val loss 2.0700 val acc 22.1591 best val_acc 30.275974 te_acc 29.646916\n",
      "trmean: at min-sum n_at 10 | e 320 fed_model val loss 2.3787 val acc 16.2946 best val_acc 31.412338 te_acc 31.818182\n",
      "trmean: at min-sum n_at 10 | e 330 fed_model val loss 1.9506 val acc 23.8636 best val_acc 31.412338 te_acc 31.818182\n",
      "trmean: at min-sum n_at 10 | e 340 fed_model val loss 2.1944 val acc 13.9610 best val_acc 32.406656 te_acc 32.711039\n",
      "trmean: at min-sum n_at 10 | e 350 fed_model val loss 1.8648 val acc 29.4237 best val_acc 32.406656 te_acc 32.711039\n",
      "trmean: at min-sum n_at 10 | e 360 fed_model val loss 1.8881 val acc 27.3539 best val_acc 32.406656 te_acc 32.711039\n",
      "trmean: at min-sum n_at 10 | e 370 fed_model val loss 1.8649 val acc 29.6875 best val_acc 32.406656 te_acc 32.711039\n",
      "trmean: at min-sum n_at 10 | e 380 fed_model val loss 1.9218 val acc 27.6177 best val_acc 33.725649 te_acc 33.177760\n",
      "trmean: at min-sum n_at 10 | e 390 fed_model val loss 2.1982 val acc 20.8401 best val_acc 33.725649 te_acc 33.177760\n",
      "trmean: at min-sum n_at 10 | e 400 fed_model val loss 2.1021 val acc 20.1502 best val_acc 33.725649 te_acc 33.177760\n",
      "trmean: at min-sum n_at 10 | e 410 fed_model val loss 1.9326 val acc 27.6380 best val_acc 33.725649 te_acc 33.177760\n",
      "trmean: at min-sum n_at 10 | e 420 fed_model val loss 1.9318 val acc 33.3401 best val_acc 35.572240 te_acc 36.789773\n",
      "trmean: at min-sum n_at 10 | e 430 fed_model val loss 1.9451 val acc 23.9245 best val_acc 37.662338 te_acc 37.824675\n",
      "trmean: at min-sum n_at 10 | e 440 fed_model val loss 1.7586 val acc 34.2532 best val_acc 37.662338 te_acc 37.824675\n",
      "trmean: at min-sum n_at 10 | e 450 fed_model val loss 2.0415 val acc 25.6494 best val_acc 37.662338 te_acc 37.824675\n",
      "trmean: at min-sum n_at 10 | e 460 fed_model val loss 2.2076 val acc 25.9131 best val_acc 38.271104 te_acc 37.987013\n",
      "trmean: at min-sum n_at 10 | e 470 fed_model val loss 2.1765 val acc 21.7938 best val_acc 38.271104 te_acc 37.987013\n",
      "trmean: at min-sum n_at 10 | e 480 fed_model val loss 2.0220 val acc 24.4318 best val_acc 38.271104 te_acc 37.987013\n",
      "trmean: at min-sum n_at 10 | e 490 fed_model val loss 2.1137 val acc 19.5820 best val_acc 38.271104 te_acc 37.987013\n",
      "trmean: at min-sum n_at 10 | e 500 fed_model val loss 1.8600 val acc 31.0268 best val_acc 38.271104 te_acc 37.987013\n",
      "trmean: at min-sum n_at 10 | e 510 fed_model val loss 1.8522 val acc 32.8937 best val_acc 38.271104 te_acc 37.987013\n",
      "trmean: at min-sum n_at 10 | e 520 fed_model val loss 2.0153 val acc 22.4026 best val_acc 38.271104 te_acc 37.987013\n",
      "trmean: at min-sum n_at 10 | e 530 fed_model val loss 1.9315 val acc 26.7857 best val_acc 38.271104 te_acc 37.987013\n",
      "trmean: at min-sum n_at 10 | e 540 fed_model val loss 2.0166 val acc 27.7800 best val_acc 40.016234 te_acc 40.827922\n",
      "trmean: at min-sum n_at 10 | e 550 fed_model val loss 1.7816 val acc 31.3515 best val_acc 40.016234 te_acc 40.827922\n",
      "trmean: at min-sum n_at 10 | e 560 fed_model val loss 2.1947 val acc 26.1769 best val_acc 40.016234 te_acc 40.827922\n",
      "trmean: at min-sum n_at 10 | e 570 fed_model val loss 2.5533 val acc 15.4424 best val_acc 40.016234 te_acc 40.827922\n",
      "trmean: at min-sum n_at 10 | e 580 fed_model val loss 2.0350 val acc 23.3157 best val_acc 40.016234 te_acc 40.827922\n",
      "trmean: at min-sum n_at 10 | e 590 fed_model val loss 1.7555 val acc 32.8734 best val_acc 40.016234 te_acc 40.827922\n",
      "trmean: at min-sum n_at 10 | e 600 fed_model val loss 1.7846 val acc 35.5925 best val_acc 40.016234 te_acc 40.827922\n",
      "trmean: at min-sum n_at 10 | e 610 fed_model val loss 1.6930 val acc 37.0536 best val_acc 40.016234 te_acc 40.827922\n",
      "trmean: at min-sum n_at 10 | e 620 fed_model val loss 1.7840 val acc 32.8531 best val_acc 40.016234 te_acc 40.827922\n",
      "trmean: at min-sum n_at 10 | e 630 fed_model val loss 1.6374 val acc 40.4627 best val_acc 41.112013 te_acc 42.491883\n",
      "trmean: at min-sum n_at 10 | e 640 fed_model val loss 1.6090 val acc 39.8336 best val_acc 41.112013 te_acc 42.491883\n",
      "trmean: at min-sum n_at 10 | e 650 fed_model val loss 1.7033 val acc 38.6567 best val_acc 41.112013 te_acc 42.491883\n",
      "trmean: at min-sum n_at 10 | e 660 fed_model val loss 1.7961 val acc 38.5349 best val_acc 41.112013 te_acc 42.491883\n",
      "trmean: at min-sum n_at 10 | e 670 fed_model val loss 1.6981 val acc 38.5349 best val_acc 42.694805 te_acc 43.810877\n",
      "trmean: at min-sum n_at 10 | e 680 fed_model val loss 50.4675 val acc 10.2476 best val_acc 42.694805 te_acc 43.810877\n",
      "val loss 842668.422890 too high\n"
     ]
    }
   ],
   "source": [
    "batch_size=250\n",
    "resume=0\n",
    "nepochs=1200\n",
    "schedule=[1000]\n",
    "nbatches = user_tr_len//batch_size\n",
    "\n",
    "gamma=.5\n",
    "opt = 'sgd'\n",
    "fed_lr=0.5\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "aggregation='trmean'\n",
    "\n",
    "at_type='min-sum'\n",
    "dev_type ='std'\n",
    "threshold=10\n",
    "n_attackers=[10]\n",
    "partial_attackers = {4:1, 5:1, 8:2, 10:3, 12:4}\n",
    "\n",
    "arch='alexnet'\n",
    "chkpt='./'+aggregation\n",
    "\n",
    "for n_attacker in n_attackers:\n",
    "    candidates = []\n",
    "\n",
    "    epoch_num = 0\n",
    "    best_global_acc = 0\n",
    "    best_global_te_acc = 0\n",
    "\n",
    "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
    "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    r=np.arange(user_tr_len)\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        user_grads=[]\n",
    "        if not epoch_num and epoch_num%nbatches == 0:\n",
    "            np.random.shuffle(r)\n",
    "            for i in range(nusers):\n",
    "                user_tr_data_tensors[i]=user_tr_data_tensors[i][r]\n",
    "                user_tr_label_tensors[i]=user_tr_label_tensors[i][r]\n",
    "\n",
    "        for i in range(nusers):\n",
    "\n",
    "            inputs = user_tr_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "            targets = user_tr_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "            outputs = fed_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            fed_model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            param_grad=[]\n",
    "            for param in fed_model.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "\n",
    "        if epoch_num in schedule:\n",
    "            for param_group in optimizer_fed.param_groups:\n",
    "                param_group['lr'] *= gamma\n",
    "                print('New learnin rate ', param_group['lr'])\n",
    "\n",
    "        if n_attacker > 0:\n",
    "            if at_type == 'lie':\n",
    "                mal_update = lie_attack(user_grads[:n_attacker], z_values[n_attacker])\n",
    "                attacker_grads = torch.cat((torch.stack([mal_update]*n_attacker), malicious_grads))\n",
    "            elif at_type == 'fang':\n",
    "                attacker_grads = fang_attack_trmean_partial(user_grads[:n_attacker], n_attacker)\n",
    "            elif at_type == 'our-agr':\n",
    "                n_attacker_ = partial_attackers[n_attacker]\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                mal_update = our_attack_median(user_grads[:n_attacker], agg_grads, n_attacker_, threshold=threshold, dev_type=dev_type)\n",
    "            elif at_type == 'min-max':\n",
    "                n_attacker_ = partial_attackers[n_attacker]\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                mal_update = our_attack_dist(user_grads[:n_attacker], agg_grads, n_attacker_, threshold=threshold, dev_type=dev_type)\n",
    "            elif at_type == 'min-sum':\n",
    "                n_attacker_ = partial_attackers[n_attacker]\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                mal_update = our_attack_score(user_grads[:n_attacker], agg_grads, n_attacker_, threshold=threshold, dev_type=dev_type)\n",
    "\n",
    "            mal_updates = torch.stack([mal_update] * n_attacker)\n",
    "            malicious_grads = torch.cat((mal_updates, user_grads[n_attacker:]), 0)\n",
    "\n",
    "        if not (malicious_grads.shape[0]==50):\n",
    "            print(malicious_grads.shape)\n",
    "            sys.exit()\n",
    "\n",
    "        agg_grads=tr_mean(malicious_grads, n_attacker)\n",
    "\n",
    "        del user_grads\n",
    "\n",
    "        start_idx=0\n",
    "\n",
    "        optimizer_fed.zero_grad()\n",
    "\n",
    "        model_grads=[]\n",
    "\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "\n",
    "        optimizer_fed.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if is_best:\n",
    "            best_global_te_acc = te_acc\n",
    "\n",
    "        if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "            print('%s: at %s n_at %d | e %d fed_model val loss %.4f val acc %.4f best val_acc %f te_acc %f'%(aggregation, at_type, n_attacker, epoch_num, val_loss, val_acc, best_global_acc,best_global_te_acc))\n",
    "\n",
    "        if val_loss > 1000:\n",
    "            print('val loss %f too high'%val_loss)\n",
    "            break\n",
    "            \n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
