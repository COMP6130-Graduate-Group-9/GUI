{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The notebook contains\n",
    "### Code for _Bulyan_ aggregation algorithm, *when gradient updates of benign clients are unknown to adversary*\n",
    "### Evaluation of all of the attacks (Fang, LIE, and our SOTA AGR-tailored and AGR-agnstic) on Bulyan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse, os, sys, csv, shutil, time, random, operator, pickle, ast, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.optim import Optimizer\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "sys.path.insert(0,'./../utils/')\n",
    "from logger import *\n",
    "from eval import *\n",
    "from misc import *\n",
    "\n",
    "from cifar10_normal_train import *\n",
    "from cifar10_util import *\n",
    "from adam import Adam\n",
    "from sgd import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get cifar10 data and split it in IID fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "total data len:  60000\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "data_loc='/mnt/nfs/work1/amir/vshejwalkar/cifar10_data/'\n",
    "# load the train dataset\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=train_transform)\n",
    "\n",
    "cifar10_test = datasets.CIFAR10(root=data_loc, train=False, download=True, transform=train_transform)\n",
    "\n",
    "X=[]\n",
    "Y=[]\n",
    "for i in range(len(cifar10_train)):\n",
    "    X.append(cifar10_train[i][0].numpy())\n",
    "    Y.append(cifar10_train[i][1])\n",
    "\n",
    "for i in range(len(cifar10_test)):\n",
    "    X.append(cifar10_test[i][0].numpy())\n",
    "    Y.append(cifar10_test[i][1])\n",
    "\n",
    "X=np.array(X)\n",
    "Y=np.array(Y)\n",
    "\n",
    "print('total data len: ',len(X))\n",
    "\n",
    "if not os.path.isfile('./cifar10_shuffle.pkl'):\n",
    "    all_indices = np.arange(len(X))\n",
    "    np.random.shuffle(all_indices)\n",
    "    pickle.dump(all_indices,open('./cifar10_shuffle.pkl','wb'))\n",
    "else:\n",
    "    all_indices=pickle.load(open('./cifar10_shuffle.pkl','rb'))\n",
    "\n",
    "X=X[all_indices]\n",
    "Y=Y[all_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide cifar10 data among 50 clients in IID fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data len:  60000\n",
      "total tr len 50000 | val len 5000 | test len 5000\n",
      "user 0 tr len 1000\n",
      "user 1 tr len 1000\n",
      "user 2 tr len 1000\n",
      "user 3 tr len 1000\n",
      "user 4 tr len 1000\n",
      "user 5 tr len 1000\n",
      "user 6 tr len 1000\n",
      "user 7 tr len 1000\n",
      "user 8 tr len 1000\n",
      "user 9 tr len 1000\n",
      "user 10 tr len 1000\n",
      "user 11 tr len 1000\n",
      "user 12 tr len 1000\n",
      "user 13 tr len 1000\n",
      "user 14 tr len 1000\n",
      "user 15 tr len 1000\n",
      "user 16 tr len 1000\n",
      "user 17 tr len 1000\n",
      "user 18 tr len 1000\n",
      "user 19 tr len 1000\n",
      "user 20 tr len 1000\n",
      "user 21 tr len 1000\n",
      "user 22 tr len 1000\n",
      "user 23 tr len 1000\n",
      "user 24 tr len 1000\n",
      "user 25 tr len 1000\n",
      "user 26 tr len 1000\n",
      "user 27 tr len 1000\n",
      "user 28 tr len 1000\n",
      "user 29 tr len 1000\n",
      "user 30 tr len 1000\n",
      "user 31 tr len 1000\n",
      "user 32 tr len 1000\n",
      "user 33 tr len 1000\n",
      "user 34 tr len 1000\n",
      "user 35 tr len 1000\n",
      "user 36 tr len 1000\n",
      "user 37 tr len 1000\n",
      "user 38 tr len 1000\n",
      "user 39 tr len 1000\n",
      "user 40 tr len 1000\n",
      "user 41 tr len 1000\n",
      "user 42 tr len 1000\n",
      "user 43 tr len 1000\n",
      "user 44 tr len 1000\n",
      "user 45 tr len 1000\n",
      "user 46 tr len 1000\n",
      "user 47 tr len 1000\n",
      "user 48 tr len 1000\n",
      "user 49 tr len 1000\n"
     ]
    }
   ],
   "source": [
    "# data loading\n",
    "\n",
    "nusers=50\n",
    "user_tr_len=1000\n",
    "\n",
    "total_tr_len=user_tr_len*nusers\n",
    "val_len=5000\n",
    "te_len=5000\n",
    "\n",
    "print('total data len: ',len(X))\n",
    "\n",
    "if not os.path.isfile('./cifar10_shuffle.pkl'):\n",
    "    all_indices = np.arange(len(X))\n",
    "    np.random.shuffle(all_indices)\n",
    "    pickle.dump(all_indices,open('./cifar10_shuffle.pkl','wb'))\n",
    "else:\n",
    "    all_indices=pickle.load(open('./cifar10_shuffle.pkl','rb'))\n",
    "\n",
    "total_tr_data=X[:total_tr_len]\n",
    "total_tr_label=Y[:total_tr_len]\n",
    "\n",
    "val_data=X[total_tr_len:(total_tr_len+val_len)]\n",
    "val_label=Y[total_tr_len:(total_tr_len+val_len)]\n",
    "\n",
    "te_data=X[(total_tr_len+val_len):(total_tr_len+val_len+te_len)]\n",
    "te_label=Y[(total_tr_len+val_len):(total_tr_len+val_len+te_len)]\n",
    "\n",
    "total_tr_data_tensor=torch.from_numpy(total_tr_data).type(torch.FloatTensor)\n",
    "total_tr_label_tensor=torch.from_numpy(total_tr_label).type(torch.LongTensor)\n",
    "\n",
    "val_data_tensor=torch.from_numpy(val_data).type(torch.FloatTensor)\n",
    "val_label_tensor=torch.from_numpy(val_label).type(torch.LongTensor)\n",
    "\n",
    "te_data_tensor=torch.from_numpy(te_data).type(torch.FloatTensor)\n",
    "te_label_tensor=torch.from_numpy(te_label).type(torch.LongTensor)\n",
    "\n",
    "print('total tr len %d | val len %d | test len %d'%(len(total_tr_data_tensor),len(val_data_tensor),len(te_data_tensor)))\n",
    "\n",
    "#==============================================================================================================\n",
    "\n",
    "user_tr_data_tensors=[]\n",
    "user_tr_label_tensors=[]\n",
    "\n",
    "for i in range(nusers):\n",
    "    \n",
    "    user_tr_data_tensor=torch.from_numpy(total_tr_data[user_tr_len*i:user_tr_len*(i+1)]).type(torch.FloatTensor)\n",
    "    user_tr_label_tensor=torch.from_numpy(total_tr_label[user_tr_len*i:user_tr_len*(i+1)]).type(torch.LongTensor)\n",
    "\n",
    "    user_tr_data_tensors.append(user_tr_data_tensor)\n",
    "    user_tr_label_tensors.append(user_tr_label_tensor)\n",
    "    print('user %d tr len %d'%(i,len(user_tr_data_tensor)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Multi-krum aggregation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_krum(all_updates, n_attackers, multi_k=False):\n",
    "\n",
    "    candidates = []\n",
    "    candidate_indices = []\n",
    "    remaining_updates = all_updates\n",
    "    all_indices = np.arange(len(all_updates))\n",
    "\n",
    "    while len(remaining_updates) > 2 * n_attackers + 2:\n",
    "        torch.cuda.empty_cache()\n",
    "        distances = []\n",
    "        for update in remaining_updates:\n",
    "            distance = []\n",
    "            for update_ in remaining_updates:\n",
    "                distance.append(torch.norm((update - update_)) ** 2)\n",
    "            distance = torch.Tensor(distance).float()\n",
    "            distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)\n",
    "\n",
    "        distances = torch.sort(distances, dim=1)[0]\n",
    "        scores = torch.sum(distances[:, :len(remaining_updates) - 2 - n_attackers], dim=1)\n",
    "        indices = torch.argsort(scores)[:len(remaining_updates) - 2 - n_attackers]\n",
    "\n",
    "        candidate_indices.append(all_indices[indices[0].cpu().numpy()])\n",
    "        all_indices = np.delete(all_indices, indices[0].cpu().numpy())\n",
    "        candidates = remaining_updates[indices[0]][None, :] if not len(candidates) else torch.cat((candidates, remaining_updates[indices[0]][None, :]), 0)\n",
    "        remaining_updates = torch.cat((remaining_updates[:indices[0]], remaining_updates[indices[0] + 1:]), 0)\n",
    "        if not multi_k:\n",
    "            break\n",
    "    # print(len(remaining_updates))\n",
    "\n",
    "    aggregate = torch.mean(candidates, dim=0)\n",
    "\n",
    "    return aggregate, np.array(candidate_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Bulyan aggregation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulyan(all_updates, n_attackers):\n",
    "    nusers = all_updates.shape[0]\n",
    "    bulyan_cluster = []\n",
    "    candidate_indices = []\n",
    "    remaining_updates = all_updates\n",
    "    all_indices = np.arange(len(all_updates))\n",
    "\n",
    "    while len(bulyan_cluster) < (nusers - 2 * n_attackers):\n",
    "        torch.cuda.empty_cache()\n",
    "        distances = []\n",
    "        for update in remaining_updates:\n",
    "            distance = []\n",
    "            for update_ in remaining_updates:\n",
    "                distance.append(torch.norm((update - update_)) ** 2)\n",
    "            distance = torch.Tensor(distance).float()\n",
    "            distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)\n",
    "        # print(distances)\n",
    "\n",
    "        distances = torch.sort(distances, dim=1)[0]\n",
    "\n",
    "        scores = torch.sum(distances[:, :len(remaining_updates) - 2 - n_attackers], dim=1)\n",
    "        indices = torch.argsort(scores)[:len(remaining_updates) - 2 - n_attackers]\n",
    "        if not len(indices):\n",
    "            break\n",
    "        candidate_indices.append(all_indices[indices[0].cpu().numpy()])\n",
    "        all_indices = np.delete(all_indices, indices[0].cpu().numpy())\n",
    "        bulyan_cluster = remaining_updates[indices[0]][None, :] if not len(bulyan_cluster) else torch.cat((bulyan_cluster, remaining_updates[indices[0]][None, :]), 0)\n",
    "        remaining_updates = torch.cat((remaining_updates[:indices[0]], remaining_updates[indices[0] + 1:]), 0)\n",
    "\n",
    "    # print('dim of bulyan cluster ', bulyan_cluster.shape)\n",
    "\n",
    "    n, d = bulyan_cluster.shape\n",
    "    param_med = torch.median(bulyan_cluster, dim=0)[0]\n",
    "    sort_idx = torch.argsort(torch.abs(bulyan_cluster - param_med), dim=0)\n",
    "    sorted_params = bulyan_cluster[sort_idx, torch.arange(d)[None, :]]\n",
    "\n",
    "    return torch.mean(sorted_params[:n - 2 * n_attackers], dim=0), np.array(candidate_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for fang attack on Bulyan\n",
    "### Note that Fang attacks on Multi-krum and Bulyan are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lambda_fang(all_updates, model_re, n_attackers):\n",
    "\n",
    "    distances = []\n",
    "    n_benign, d = all_updates.shape\n",
    "    for update in all_updates:\n",
    "        distance = torch.norm((all_updates - update), dim=1)\n",
    "        distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)\n",
    "\n",
    "    distances[distances == 0] = 10000\n",
    "    distances = torch.sort(distances, dim=1)[0]\n",
    "    scores = torch.sum(distances[:, :n_benign - 2 - n_attackers], dim=1)\n",
    "    min_score = torch.min(scores)\n",
    "    term_1 = min_score / ((n_benign - n_attackers - 1) * torch.sqrt(torch.Tensor([d]))[0])\n",
    "    max_wre_dist = torch.max(torch.norm((all_updates - model_re), dim=1)) / (torch.sqrt(torch.Tensor([d]))[0])\n",
    "\n",
    "    return (term_1 + max_wre_dist)\n",
    "\n",
    "\n",
    "def get_malicious_updates_fang(all_updates, model_re, deviation, n_attackers):\n",
    "\n",
    "    lamda = compute_lambda_fang(all_updates, model_re, n_attackers)\n",
    "    threshold = 1e-5\n",
    "\n",
    "    mal_updates = []\n",
    "    while lamda > threshold:\n",
    "        mal_update = (- lamda * deviation)\n",
    "\n",
    "        mal_updates = torch.stack([mal_update] * n_attackers)\n",
    "        mal_updates = torch.cat((mal_updates, all_updates), 0)\n",
    "\n",
    "        agg_grads, krum_candidate = multi_krum(mal_updates, n_attackers, multi_k=False)\n",
    "        \n",
    "        if krum_candidate < n_attackers:\n",
    "            return mal_update\n",
    "        \n",
    "        lamda *= 0.5\n",
    "\n",
    "    if not len(mal_updates):\n",
    "        print(lamda, threshold)\n",
    "        mal_update = (model_re - lamda * deviation)\n",
    "\n",
    "    return mal_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Fang attack on Bulyan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vshejwalkar/NDSS21-Model-Poisoning/cifar10/sgd.py:109: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729138878/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  p.data.add_(-group['lr'], d_p)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bulyan: at fang n_at 10 n_mal_sel 10 e 0 val loss 2.3033 val acc 10.1461 best val_acc 10.146104 te_acc 10.531656\n",
      "bulyan: at fang n_at 10 n_mal_sel 10 e 25 val loss 2.2946 val acc 11.4651 best val_acc 11.465097 te_acc 12.297078\n",
      "bulyan: at fang n_at 10 n_mal_sel 9 e 50 val loss 2.2838 val acc 10.9172 best val_acc 19.926948 te_acc 20.271916\n",
      "bulyan: at fang n_at 10 n_mal_sel 10 e 75 val loss 2.1847 val acc 20.5763 best val_acc 20.576299 te_acc 20.819805\n",
      "bulyan: at fang n_at 10 n_mal_sel 10 e 100 val loss 2.1233 val acc 20.2719 best val_acc 22.017045 te_acc 23.051948\n",
      "bulyan: at fang n_at 10 n_mal_sel 7 e 125 val loss 2.2810 val acc 10.5722 best val_acc 22.017045 te_acc 23.051948\n",
      "bulyan: at fang n_at 10 n_mal_sel 10 e 150 val loss 2.1366 val acc 19.2979 best val_acc 22.017045 te_acc 23.051948\n",
      "bulyan: at fang n_at 10 n_mal_sel 6 e 175 val loss 2.3311 val acc 10.5519 best val_acc 22.666396 te_acc 22.808442\n",
      "bulyan: at fang n_at 10 n_mal_sel 5 e 200 val loss 2.6503 val acc 14.8539 best val_acc 24.918831 te_acc 25.446429\n",
      "bulyan: at fang n_at 10 n_mal_sel 10 e 225 val loss 2.2201 val acc 11.2825 best val_acc 24.918831 te_acc 25.446429\n",
      "bulyan: at fang n_at 10 n_mal_sel 7 e 250 val loss 2.2565 val acc 15.3206 best val_acc 24.918831 te_acc 25.446429\n",
      "bulyan: at fang n_at 10 n_mal_sel 10 e 275 val loss 2.2345 val acc 9.3547 best val_acc 24.918831 te_acc 25.446429\n",
      "bulyan: at fang n_at 10 n_mal_sel 10 e 300 val loss 2.0986 val acc 18.6891 best val_acc 24.918831 te_acc 25.446429\n",
      "bulyan: at fang n_at 10 n_mal_sel 10 e 350 val loss 2.0712 val acc 22.3011 best val_acc 24.918831 te_acc 25.446429\n",
      "bulyan: at fang n_at 10 n_mal_sel 10 e 375 val loss 2.2318 val acc 15.8279 best val_acc 25.020292 te_acc 25.101461\n",
      "bulyan: at fang n_at 10 n_mal_sel 10 e 400 val loss 2.0489 val acc 20.9821 best val_acc 25.020292 te_acc 25.101461\n",
      "bulyan: at fang n_at 10 n_mal_sel 10 e 425 val loss 2.0584 val acc 22.3620 best val_acc 25.020292 te_acc 25.101461\n",
      "bulyan: at fang n_at 10 n_mal_sel 5 e 450 val loss 2.3630 val acc 10.5722 best val_acc 25.020292 te_acc 25.101461\n",
      "bulyan: at fang n_at 10 n_mal_sel 10 e 475 val loss 2.0198 val acc 24.5942 best val_acc 25.020292 te_acc 25.101461\n",
      "bulyan: at fang n_at 10 n_mal_sel 5 e 500 val loss 2.3386 val acc 10.2476 best val_acc 25.020292 te_acc 25.101461\n",
      "bulyan: at fang n_at 10 n_mal_sel 10 e 525 val loss 1.9992 val acc 20.6981 best val_acc 25.020292 te_acc 25.101461\n",
      "bulyan: at fang n_at 10 n_mal_sel 5 e 550 val loss 2.2220 val acc 14.1234 best val_acc 25.020292 te_acc 25.101461\n",
      "bulyan: at fang n_at 10 n_mal_sel 8 e 575 val loss 2.3133 val acc 10.8969 best val_acc 25.020292 te_acc 25.101461\n",
      "bulyan: at fang n_at 10 n_mal_sel 5 e 600 val loss 2.3845 val acc 9.2735 best val_acc 25.020292 te_acc 25.101461\n",
      "bulyan: at fang n_at 10 n_mal_sel 10 e 625 val loss 2.1484 val acc 17.1875 best val_acc 25.020292 te_acc 25.101461\n",
      "bulyan: at fang n_at 10 n_mal_sel 10 e 650 val loss 2.1040 val acc 17.7151 best val_acc 25.020292 te_acc 25.101461\n",
      "bulyan: at fang n_at 10 n_mal_sel 10 e 675 val loss 2.0500 val acc 19.4602 best val_acc 25.020292 te_acc 25.101461\n",
      "bulyan: at fang n_at 10 n_mal_sel 10 e 700 val loss 2.0271 val acc 22.7273 best val_acc 25.020292 te_acc 25.101461\n",
      "bulyan: at fang n_at 10 n_mal_sel 7 e 725 val loss 2.3222 val acc 10.4302 best val_acc 25.020292 te_acc 25.101461\n",
      "bulyan: at fang n_at 10 n_mal_sel 10 e 750 val loss 2.0875 val acc 19.8458 best val_acc 25.020292 te_acc 25.101461\n",
      "bulyan: at fang n_at 10 n_mal_sel 2 e 775 val loss 5.0882 val acc 11.3231 best val_acc 25.020292 te_acc 25.101461\n",
      "bulyan: at fang n_at 10 n_mal_sel 10 e 800 val loss 2.3110 val acc 11.5057 best val_acc 25.020292 te_acc 25.101461\n",
      "bulyan: at fang n_at 10 n_mal_sel 10 e 825 val loss 2.1042 val acc 18.1006 best val_acc 25.020292 te_acc 25.101461\n",
      "bulyan: at fang n_at 10 n_mal_sel 10 e 850 val loss 2.0657 val acc 18.7703 best val_acc 25.020292 te_acc 25.101461\n",
      "bulyan: at fang n_at 10 n_mal_sel 10 e 875 val loss 2.0541 val acc 21.0836 best val_acc 25.020292 te_acc 25.101461\n",
      "bulyan: at fang n_at 10 n_mal_sel 8 e 900 val loss 2.0463 val acc 18.9529 best val_acc 25.020292 te_acc 25.101461\n",
      "bulyan: at fang n_at 10 n_mal_sel 10 e 925 val loss 2.0257 val acc 19.8864 best val_acc 25.020292 te_acc 25.101461\n",
      "bulyan: at fang n_at 10 n_mal_sel 9 e 950 val loss 2.6565 val acc 13.4537 best val_acc 25.020292 te_acc 25.101461\n",
      "bulyan: at fang n_at 10 n_mal_sel 9 e 975 val loss 2.1140 val acc 18.8515 best val_acc 25.020292 te_acc 25.101461\n",
      "New learnin rate  0.25\n",
      "bulyan: at fang n_at 10 n_mal_sel 10 e 1000 val loss 2.0911 val acc 18.1412 best val_acc 25.020292 te_acc 25.101461\n",
      "bulyan: at fang n_at 10 n_mal_sel 10 e 1025 val loss 2.0404 val acc 20.7589 best val_acc 25.020292 te_acc 25.101461\n",
      "bulyan: at fang n_at 10 n_mal_sel 10 e 1050 val loss 2.0042 val acc 22.5446 best val_acc 25.020292 te_acc 25.101461\n",
      "bulyan: at fang n_at 10 n_mal_sel 10 e 1075 val loss 1.9878 val acc 23.5390 best val_acc 25.020292 te_acc 25.101461\n",
      "bulyan: at fang n_at 10 n_mal_sel 10 e 1100 val loss 1.9771 val acc 23.4781 best val_acc 25.020292 te_acc 25.101461\n",
      "bulyan: at fang n_at 10 n_mal_sel 10 e 1125 val loss 2.0295 val acc 22.6258 best val_acc 25.121753 te_acc 25.649351\n",
      "bulyan: at fang n_at 10 n_mal_sel 10 e 1150 val loss 1.9933 val acc 21.7127 best val_acc 25.121753 te_acc 25.649351\n",
      "bulyan: at fang n_at 10 n_mal_sel 10 e 1175 val loss 1.9732 val acc 23.5593 best val_acc 25.121753 te_acc 25.649351\n",
      "bulyan: at fang n_at 10 n_mal_sel 3 e 1199 val loss 2.4265 val acc 11.5869 best val_acc 25.994318 te_acc 26.055195\n",
      "bulyan: at fang n_at 10 n_mal_sel 5 e 1200 val loss 2.3696 val acc 11.5057 best val_acc 25.994318 te_acc 26.055195\n"
     ]
    }
   ],
   "source": [
    "batch_size=250\n",
    "resume=0\n",
    "nepochs=1200\n",
    "schedule=[1000]\n",
    "nbatches = user_tr_len//batch_size\n",
    "\n",
    "gamma=.5\n",
    "opt = 'sgd'\n",
    "fed_lr=0.5\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "aggregation='bulyan'\n",
    "multi_k = False\n",
    "candidates = []\n",
    "\n",
    "at_type='fang'\n",
    "n_attackers=[10]\n",
    "\n",
    "arch='alexnet'\n",
    "chkpt='./'+aggregation\n",
    "\n",
    "for n_attacker in n_attackers:\n",
    "    epoch_num = 0\n",
    "    best_global_acc = 0\n",
    "    best_global_te_acc = 0\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    r=np.arange(user_tr_len)\n",
    "\n",
    "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
    "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        user_grads=[]\n",
    "        if not epoch_num and epoch_num%nbatches == 0:\n",
    "            np.random.shuffle(r)\n",
    "            for i in range(nusers):\n",
    "                user_tr_data_tensors[i]=user_tr_data_tensors[i][r]\n",
    "                user_tr_label_tensors[i]=user_tr_label_tensors[i][r]\n",
    "\n",
    "        for i in range(n_attacker, nusers):\n",
    "\n",
    "            inputs = user_tr_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "            targets = user_tr_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "            outputs = fed_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            fed_model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            param_grad=[]\n",
    "            for param in fed_model.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "\n",
    "        if epoch_num in schedule:\n",
    "            for param_group in optimizer_fed.param_groups:\n",
    "                param_group['lr'] *= gamma\n",
    "                print('New learnin rate ', param_group['lr'])\n",
    "\n",
    "        if n_attacker > 0:\n",
    "            n_attacker_ = max(1, n_attacker**2//nusers)\n",
    "            if at_type == 'lie':\n",
    "                mal_update = lie_attack(user_grads[:n_attacker], z_values[n_attacker])\n",
    "            elif at_type == 'fang':\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                deviation = torch.sign(agg_grads)\n",
    "                mal_update = get_malicious_updates_fang(user_grads[:n_attacker], agg_grads, deviation, n_attacker_)\n",
    "            elif at_type == 'our-agr':\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                mal_update = our_attack_median(user_grads[:n_attacker], agg_grads, n_attacker_, dev_type)\n",
    "            elif at_type == 'min-max':\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                mal_update = our_attack_dist(user_grads[:n_attacker], agg_grads, n_attacker_, dev_type)\n",
    "            elif at_type == 'min-sum':\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                mal_update = our_attack_score(user_grads[:n_attacker], agg_grads, n_attacker_, dev_type)\n",
    "\n",
    "            mal_updates = torch.stack([mal_update] * n_attacker)\n",
    "            malicious_grads = torch.cat((mal_updates, user_grads[n_attacker:]), 0)\n",
    "        \n",
    "        mal_updates = torch.stack([mal_update] * n_attacker)\n",
    "        malicious_grads = torch.cat((mal_updates, user_grads), 0)\n",
    "        \n",
    "        if not (malicious_grads.shape[0]==50):\n",
    "            print(malicious_grads.shape)\n",
    "            sys.exit()\n",
    "            \n",
    "        agg_grads, krum_candidate=bulyan(malicious_grads, n_attacker)\n",
    "\n",
    "        del user_grads\n",
    "\n",
    "        start_idx=0\n",
    "\n",
    "        optimizer_fed.zero_grad()\n",
    "\n",
    "        model_grads=[]\n",
    "\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "\n",
    "        optimizer_fed.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if is_best:\n",
    "            best_global_te_acc = te_acc\n",
    "\n",
    "        if epoch_num%25==0 or epoch_num==nepochs-1:\n",
    "            print('%s: at %s n_at %d n_mal_sel %d e %d val loss %.4f val acc %.4f best val_acc %f te_acc %f'%(aggregation, at_type, n_attacker, np.sum(krum_candidate < n_attacker), epoch_num, val_loss, val_acc, best_global_acc,best_global_te_acc))\n",
    "\n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for LIE attack, followed by its execution on Bulyan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lie_attack(all_updates, z):\n",
    "    avg = torch.mean(all_updates, dim=0)\n",
    "    std = torch.std(all_updates, dim=0)\n",
    "    return avg + z * std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vshejwalkar/NDSS21-Model-Poisoning/cifar10/sgd.py:109: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729138878/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  p.data.add_(-group['lr'], d_p)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bulyan: at lie n_at 10 n_mal_sel 10 e 0 fed_model val loss 2.3026 val acc 9.5982 best val_acc 9.598214 te_acc 10.024351\n",
      "bulyan: at lie n_at 10 n_mal_sel 9 e 10 fed_model val loss 2.2974 val acc 14.1843 best val_acc 14.995942 te_acc 14.813312\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 20 fed_model val loss 2.2733 val acc 16.5990 best val_acc 16.599026 te_acc 16.213474\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 30 fed_model val loss 2.2522 val acc 16.9846 best val_acc 19.622565 te_acc 19.094968\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 40 fed_model val loss 2.1855 val acc 14.8133 best val_acc 19.805195 te_acc 19.622565\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 50 fed_model val loss 2.1281 val acc 17.9586 best val_acc 19.805195 te_acc 19.622565\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 60 fed_model val loss 2.0724 val acc 19.8864 best val_acc 21.732955 te_acc 22.382305\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 70 fed_model val loss 2.2200 val acc 16.0106 best val_acc 21.732955 te_acc 22.382305\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 80 fed_model val loss 2.1300 val acc 17.5325 best val_acc 23.072240 te_acc 23.681006\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 90 fed_model val loss 2.1151 val acc 17.9789 best val_acc 24.025974 te_acc 24.289773\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 100 fed_model val loss 2.0263 val acc 21.0836 best val_acc 24.127435 te_acc 24.005682\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 110 fed_model val loss 2.0051 val acc 21.3474 best val_acc 26.258117 te_acc 26.907468\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 120 fed_model val loss 1.9432 val acc 23.8231 best val_acc 26.400162 te_acc 25.994318\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 130 fed_model val loss 2.0009 val acc 23.6404 best val_acc 26.400162 te_acc 25.994318\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 140 fed_model val loss 1.9336 val acc 26.6843 best val_acc 27.435065 te_acc 28.287338\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 150 fed_model val loss 1.9778 val acc 25.8320 best val_acc 27.678571 te_acc 28.652597\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 160 fed_model val loss 1.9693 val acc 25.7914 best val_acc 29.139610 te_acc 28.287338\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 170 fed_model val loss 1.8857 val acc 29.0381 best val_acc 29.667208 te_acc 30.418019\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 180 fed_model val loss 1.9029 val acc 29.6266 best val_acc 29.748377 te_acc 29.626623\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 190 fed_model val loss 1.9861 val acc 25.8117 best val_acc 32.487825 te_acc 32.508117\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 200 fed_model val loss 1.8385 val acc 31.0065 best val_acc 33.482143 te_acc 33.745942\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 210 fed_model val loss 1.8698 val acc 31.1891 best val_acc 33.482143 te_acc 33.745942\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 220 fed_model val loss 1.8308 val acc 33.6445 best val_acc 34.760552 te_acc 34.253247\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 230 fed_model val loss 1.9888 val acc 24.6347 best val_acc 35.166396 te_acc 36.343344\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 240 fed_model val loss 1.7637 val acc 34.6997 best val_acc 35.409903 te_acc 35.531656\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 250 fed_model val loss 1.7748 val acc 35.2273 best val_acc 35.714286 te_acc 36.282468\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 260 fed_model val loss 1.7695 val acc 34.8823 best val_acc 35.714286 te_acc 36.282468\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 270 fed_model val loss 1.7648 val acc 36.2622 best val_acc 36.262175 te_acc 36.444805\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 280 fed_model val loss 1.9033 val acc 28.5714 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 290 fed_model val loss 1.8778 val acc 32.8937 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 300 fed_model val loss 1.8764 val acc 31.0471 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 310 fed_model val loss 1.8608 val acc 30.9253 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 320 fed_model val loss 1.8703 val acc 32.2443 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 330 fed_model val loss 1.9240 val acc 29.4034 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 340 fed_model val loss 1.9184 val acc 29.7078 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 350 fed_model val loss 1.8817 val acc 30.4180 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 360 fed_model val loss 1.8107 val acc 32.8531 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 370 fed_model val loss 1.8060 val acc 34.1924 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 380 fed_model val loss 1.8038 val acc 34.1315 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 390 fed_model val loss 1.9109 val acc 29.9716 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 400 fed_model val loss 1.8663 val acc 30.5804 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 410 fed_model val loss 1.8918 val acc 30.0122 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 420 fed_model val loss 1.8470 val acc 31.5747 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 430 fed_model val loss 1.8568 val acc 31.2500 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 440 fed_model val loss 1.8216 val acc 33.1575 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 450 fed_model val loss 1.9043 val acc 29.2817 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 460 fed_model val loss 1.9044 val acc 29.3831 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 470 fed_model val loss 1.9433 val acc 28.3279 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 480 fed_model val loss 1.8990 val acc 30.1542 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 490 fed_model val loss 1.9249 val acc 29.3425 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 500 fed_model val loss 1.8627 val acc 31.9399 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 510 fed_model val loss 1.8898 val acc 30.7630 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 520 fed_model val loss 1.9175 val acc 29.5657 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 530 fed_model val loss 1.9365 val acc 27.3945 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 540 fed_model val loss 1.8655 val acc 31.5341 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 550 fed_model val loss 1.8957 val acc 29.9310 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 560 fed_model val loss 1.8798 val acc 30.7021 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 570 fed_model val loss 1.9022 val acc 28.9164 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 580 fed_model val loss 1.8950 val acc 28.6729 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 590 fed_model val loss 1.8813 val acc 30.1745 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 600 fed_model val loss 1.8934 val acc 29.6063 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 610 fed_model val loss 1.9224 val acc 27.1510 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 620 fed_model val loss 1.9200 val acc 28.7338 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 630 fed_model val loss 1.9503 val acc 27.4148 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 640 fed_model val loss 1.9136 val acc 29.9513 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 650 fed_model val loss 1.9454 val acc 26.8669 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 660 fed_model val loss 1.9239 val acc 26.7654 best val_acc 36.363636 te_acc 36.485390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bulyan: at lie n_at 10 n_mal_sel 10 e 670 fed_model val loss 1.9159 val acc 28.3076 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 680 fed_model val loss 1.9654 val acc 26.5016 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 690 fed_model val loss 1.9788 val acc 25.7508 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 700 fed_model val loss 1.9242 val acc 27.8409 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 710 fed_model val loss 1.9231 val acc 27.5365 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 720 fed_model val loss 2.0372 val acc 23.7419 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 730 fed_model val loss 1.9994 val acc 26.7451 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 740 fed_model val loss 1.9888 val acc 27.3742 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 750 fed_model val loss 2.0836 val acc 22.4432 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 760 fed_model val loss 2.0575 val acc 25.4058 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 770 fed_model val loss 2.0927 val acc 22.9099 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 780 fed_model val loss 2.0890 val acc 23.3563 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 790 fed_model val loss 2.0321 val acc 25.1218 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 800 fed_model val loss 2.0713 val acc 24.0463 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 810 fed_model val loss 2.0882 val acc 20.5357 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 820 fed_model val loss 2.0396 val acc 24.6956 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 830 fed_model val loss 2.0504 val acc 23.4578 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 840 fed_model val loss 2.1117 val acc 21.6518 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 850 fed_model val loss 2.0272 val acc 24.0463 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 860 fed_model val loss 2.0618 val acc 22.5244 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 870 fed_model val loss 2.0527 val acc 23.6201 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 880 fed_model val loss 2.0150 val acc 24.8174 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 890 fed_model val loss 2.0320 val acc 24.2086 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 900 fed_model val loss 2.0315 val acc 24.1071 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 910 fed_model val loss 2.0651 val acc 22.0373 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 920 fed_model val loss 2.0196 val acc 24.6550 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 930 fed_model val loss 1.9955 val acc 25.0812 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 940 fed_model val loss 2.0106 val acc 24.9797 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 950 fed_model val loss 1.9846 val acc 25.1015 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 960 fed_model val loss 1.9827 val acc 24.9391 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 970 fed_model val loss 2.1100 val acc 19.5211 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 980 fed_model val loss 2.0080 val acc 23.4172 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 990 fed_model val loss 1.9815 val acc 25.6088 best val_acc 36.363636 te_acc 36.485390\n",
      "New learnin rate  0.25\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 1000 fed_model val loss 1.9991 val acc 25.0406 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 1010 fed_model val loss 1.9735 val acc 25.3044 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 1020 fed_model val loss 1.9675 val acc 25.4667 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 1030 fed_model val loss 1.9874 val acc 25.2638 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 1040 fed_model val loss 1.9652 val acc 25.5479 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 9 e 1050 fed_model val loss 1.9936 val acc 24.8377 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 9 e 1060 fed_model val loss 1.9660 val acc 25.1015 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 1070 fed_model val loss 1.9856 val acc 25.0000 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 1080 fed_model val loss 1.9579 val acc 25.5073 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 1090 fed_model val loss 1.9996 val acc 24.3304 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 1100 fed_model val loss 1.9602 val acc 25.4667 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 1110 fed_model val loss 2.0029 val acc 24.2695 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 1120 fed_model val loss 1.9686 val acc 24.9594 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 1130 fed_model val loss 1.9576 val acc 25.7914 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 1140 fed_model val loss 1.9601 val acc 25.2841 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 1150 fed_model val loss 1.9841 val acc 25.0609 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 1160 fed_model val loss 1.9700 val acc 25.3247 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 1170 fed_model val loss 1.9789 val acc 25.0000 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 9 e 1180 fed_model val loss 2.0001 val acc 25.0000 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 1190 fed_model val loss 1.9754 val acc 25.3856 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 1199 fed_model val loss 1.9655 val acc 25.0203 best val_acc 36.363636 te_acc 36.485390\n",
      "bulyan: at lie n_at 10 n_mal_sel 10 e 1200 fed_model val loss 1.9828 val acc 25.1420 best val_acc 36.363636 te_acc 36.485390\n"
     ]
    }
   ],
   "source": [
    "batch_size=250\n",
    "resume=0\n",
    "nepochs=1200\n",
    "schedule=[1000]\n",
    "nbatches = user_tr_len//batch_size\n",
    "\n",
    "gamma=.5\n",
    "opt = 'sgd'\n",
    "fed_lr=0.5\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "aggregation='bulyan'\n",
    "multi_k = False\n",
    "candidates = []\n",
    "\n",
    "at_type='lie'\n",
    "z_values={3:0.69847, 5:0.7054, 8:0.71904, 10:0.72575, 12:0.73891}\n",
    "n_attackers=[10]\n",
    "\n",
    "arch='alexnet'\n",
    "chkpt='./'+aggregation\n",
    "\n",
    "for n_attacker in n_attackers:\n",
    "    epoch_num = 0\n",
    "    best_global_acc = 0\n",
    "    best_global_te_acc = 0\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    r=np.arange(user_tr_len)\n",
    "\n",
    "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
    "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        user_grads=[]\n",
    "        if not epoch_num and epoch_num%nbatches == 0:\n",
    "            np.random.shuffle(r)\n",
    "            for i in range(nusers):\n",
    "                user_tr_data_tensors[i]=user_tr_data_tensors[i][r]\n",
    "                user_tr_label_tensors[i]=user_tr_label_tensors[i][r]\n",
    "\n",
    "        for i in range(n_attacker, nusers):\n",
    "\n",
    "            inputs = user_tr_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "            targets = user_tr_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "            outputs = fed_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            fed_model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            param_grad=[]\n",
    "            for param in fed_model.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "\n",
    "        if epoch_num in schedule:\n",
    "            for param_group in optimizer_fed.param_groups:\n",
    "                param_group['lr'] *= gamma\n",
    "                print('New learnin rate ', param_group['lr'])\n",
    "\n",
    "        if n_attacker > 0:\n",
    "            n_attacker_ = max(1, n_attacker**2//nusers)\n",
    "            if at_type == 'lie':\n",
    "                mal_update = lie_attack(user_grads[:n_attacker], z_values[n_attacker])\n",
    "            \n",
    "        mal_updates = torch.stack([mal_update] * n_attacker)\n",
    "        malicious_grads = torch.cat((mal_updates, user_grads), 0)\n",
    "\n",
    "        if not (malicious_grads.shape[0]==50):\n",
    "            print(malicious_grads.shape)\n",
    "            sys.exit()\n",
    "            \n",
    "        agg_grads, krum_candidate=bulyan(malicious_grads, n_attacker)\n",
    "\n",
    "        del user_grads\n",
    "\n",
    "        start_idx=0\n",
    "\n",
    "        optimizer_fed.zero_grad()\n",
    "\n",
    "        model_grads=[]\n",
    "\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "\n",
    "        optimizer_fed.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if is_best:\n",
    "            best_global_te_acc = te_acc\n",
    "\n",
    "        if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "            print('%s: at %s n_at %d n_mal_sel %d e %d fed_model val loss %.4f val acc %.4f best val_acc %f te_acc %f'%(aggregation, at_type, n_attacker, np.sum(krum_candidate < n_attacker), epoch_num, val_loss, val_acc, best_global_acc,best_global_te_acc))\n",
    "\n",
    "        if val_loss > 10:\n",
    "            print('val loss %f too high'%val_loss)\n",
    "            break\n",
    "\n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for our AGR-tailored attack on Bulyan\n",
    "* Note that our attacks on multi-krum and Bulyan aggregations are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_attack_mkrum(all_updates, model_re, n_attackers, dev_type='unit_vec'):\n",
    "\n",
    "    if dev_type == 'unit_vec':\n",
    "        deviation = model_re / torch.norm(model_re)\n",
    "    elif dev_type == 'sign':\n",
    "        deviation = torch.sign(model_re)\n",
    "    elif dev_type == 'std':\n",
    "        deviation = torch.std(all_updates, 0)\n",
    "\n",
    "    lamda = torch.Tensor([3.0]).cuda()\n",
    "\n",
    "    threshold_diff = 1e-5\n",
    "    lamda_fail = lamda\n",
    "    lamda_succ = 0\n",
    "\n",
    "    while torch.abs(lamda_succ - lamda) > threshold_diff:\n",
    "        mal_update = (model_re - lamda * deviation)\n",
    "        mal_updates = torch.stack([mal_update] * n_attackers)\n",
    "        mal_updates = torch.cat((mal_updates, all_updates), 0)\n",
    "\n",
    "        agg_grads, krum_candidate = multi_krum(mal_updates, n_attackers, multi_k=True)\n",
    "        if np.sum(krum_candidate < n_attackers) == n_attackers:\n",
    "            # print('successful lamda is ', lamda)\n",
    "            lamda_succ = lamda\n",
    "            lamda = lamda + lamda_fail / 2\n",
    "        else:\n",
    "            lamda = lamda - lamda_fail / 2\n",
    "\n",
    "        lamda_fail = lamda_fail / 2\n",
    "\n",
    "    mal_update = (model_re - lamda_succ * deviation)\n",
    "    return mal_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute our AGR-tailored attack on Bulyan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bulyan: at our-agr n_at 10 n_mal_sel 9 e 0 | val loss 2.3028 val acc 10.2881 best val_acc 10.288149\n",
      "bulyan: at our-agr n_at 10 n_mal_sel 9 e 25 | val loss 2.3040 val acc 11.4854 best val_acc 20.028409\n",
      "bulyan: at our-agr n_at 10 n_mal_sel 10 e 50 | val loss 2.2243 val acc 17.4716 best val_acc 20.028409\n",
      "bulyan: at our-agr n_at 10 n_mal_sel 8 e 75 | val loss 2.3105 val acc 9.9635 best val_acc 21.611201\n",
      "bulyan: at our-agr n_at 10 n_mal_sel 9 e 100 | val loss 2.2664 val acc 10.2476 best val_acc 21.611201\n",
      "bulyan: at our-agr n_at 10 n_mal_sel 9 e 125 | val loss 2.2665 val acc 13.2305 best val_acc 21.611201\n",
      "bulyan: at our-agr n_at 10 n_mal_sel 8 e 150 | val loss 2.2019 val acc 20.2313 best val_acc 21.611201\n",
      "bulyan: at our-agr n_at 10 n_mal_sel 8 e 175 | val loss 2.3083 val acc 11.5057 best val_acc 21.611201\n",
      "bulyan: at our-agr n_at 10 n_mal_sel 9 e 200 | val loss 2.1817 val acc 17.7151 best val_acc 21.611201\n",
      "bulyan: at our-agr n_at 10 n_mal_sel 8 e 225 | val loss 2.2269 val acc 14.7524 best val_acc 21.611201\n",
      "bulyan: at our-agr n_at 10 n_mal_sel 10 e 250 | val loss 2.3175 val acc 9.8620 best val_acc 21.611201\n",
      "bulyan: at our-agr n_at 10 n_mal_sel 8 e 275 | val loss 2.2202 val acc 16.0917 best val_acc 21.611201\n",
      "bulyan: at our-agr n_at 10 n_mal_sel 10 e 300 | val loss 2.3261 val acc 10.2476 best val_acc 21.611201\n",
      "bulyan: at our-agr n_at 10 n_mal_sel 8 e 325 | val loss 2.2823 val acc 10.5722 best val_acc 21.611201\n",
      "bulyan: at our-agr n_at 10 n_mal_sel 9 e 350 | val loss 2.1494 val acc 17.4107 best val_acc 21.611201\n",
      "bulyan: at our-agr n_at 10 n_mal_sel 8 e 375 | val loss 2.3854 val acc 9.6997 best val_acc 21.611201\n",
      "bulyan: at our-agr n_at 10 n_mal_sel 9 e 400 | val loss 2.2728 val acc 16.2744 best val_acc 21.611201\n",
      "bulyan: at our-agr n_at 10 n_mal_sel 9 e 425 | val loss 2.1199 val acc 23.0925 best val_acc 23.092532\n",
      "bulyan: at our-agr n_at 10 n_mal_sel 9 e 450 | val loss 2.3172 val acc 9.6388 best val_acc 23.092532\n",
      "bulyan: at our-agr n_at 10 n_mal_sel 8 e 475 | val loss 2.1190 val acc 19.7240 best val_acc 23.092532\n",
      "bulyan: at our-agr n_at 10 n_mal_sel 9 e 500 | val loss 2.2949 val acc 11.8912 best val_acc 23.092532\n",
      "bulyan: at our-agr n_at 10 n_mal_sel 9 e 525 | val loss 2.1169 val acc 21.5909 best val_acc 23.092532\n",
      "bulyan: at our-agr n_at 10 n_mal_sel 10 e 550 | val loss 2.2155 val acc 19.3994 best val_acc 23.498377\n",
      "bulyan: at our-agr n_at 10 n_mal_sel 7 e 575 | val loss 2.1529 val acc 19.5008 best val_acc 23.498377\n",
      "bulyan: at our-agr n_at 10 n_mal_sel 9 e 600 | val loss 2.3068 val acc 11.9521 best val_acc 23.498377\n",
      "bulyan: at our-agr n_at 10 n_mal_sel 8 e 625 | val loss 2.3014 val acc 10.9375 best val_acc 23.498377\n",
      "bulyan: at our-agr n_at 10 n_mal_sel 8 e 650 | val loss 2.1397 val acc 17.4107 best val_acc 23.498377\n",
      "bulyan: at our-agr n_at 10 n_mal_sel 8 e 675 | val loss 2.3365 val acc 11.4651 best val_acc 23.498377\n",
      "bulyan: at our-agr n_at 10 n_mal_sel 8 e 700 | val loss 2.2813 val acc 15.1583 best val_acc 23.498377\n",
      "bulyan: at our-agr n_at 10 n_mal_sel 9 e 725 | val loss 2.1838 val acc 18.2021 best val_acc 23.498377\n",
      "bulyan: at our-agr n_at 10 n_mal_sel 10 e 750 | val loss 2.5982 val acc 13.0885 best val_acc 23.498377\n",
      "bulyan: at our-agr n_at 10 n_mal_sel 8 e 775 | val loss 2.2898 val acc 11.0795 best val_acc 23.498377\n",
      "bulyan: at our-agr n_at 10 n_mal_sel 9 e 800 | val loss 2.2716 val acc 13.5146 best val_acc 23.498377\n",
      "bulyan: at our-agr n_at 10 n_mal_sel 9 e 825 | val loss 2.1583 val acc 20.8401 best val_acc 23.498377\n",
      "val loss 43857.296000 too high\n"
     ]
    }
   ],
   "source": [
    "batch_size=250\n",
    "resume=0\n",
    "nepochs=1200\n",
    "schedule=[1000]\n",
    "nbatches = user_tr_len//batch_size\n",
    "\n",
    "gamma=.5\n",
    "opt = 'sgd'\n",
    "fed_lr=0.5\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "aggregation='bulyan'\n",
    "multi_k = False\n",
    "candidates = []\n",
    "\n",
    "at_type='our-agr'\n",
    "dev_type ='std'\n",
    "n_attackers=[10]\n",
    "\n",
    "arch='alexnet'\n",
    "chkpt='./'+aggregation\n",
    "\n",
    "\n",
    "for n_attacker in n_attackers:\n",
    "    epoch_num = 0\n",
    "    best_global_acc = 0\n",
    "    best_global_te_acc = 0\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    r=np.arange(user_tr_len)\n",
    "\n",
    "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
    "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        user_grads=[]\n",
    "        if not epoch_num and epoch_num%nbatches == 0:\n",
    "            np.random.shuffle(r)\n",
    "            for i in range(nusers):\n",
    "                user_tr_data_tensors[i]=user_tr_data_tensors[i][r]\n",
    "                user_tr_label_tensors[i]=user_tr_label_tensors[i][r]\n",
    "\n",
    "        for i in range(nusers):\n",
    "\n",
    "            inputs = user_tr_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "            targets = user_tr_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "            outputs = fed_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            fed_model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            param_grad=[]\n",
    "            for param in fed_model.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "\n",
    "        if epoch_num in schedule:\n",
    "            for param_group in optimizer_fed.param_groups:\n",
    "                param_group['lr'] *= gamma\n",
    "                print('New learnin rate ', param_group['lr'])\n",
    "\n",
    "        if n_attacker > 0:\n",
    "            n_attacker_ = max(1, n_attacker**2//nusers)\n",
    "            if at_type == 'lie':\n",
    "                mal_update = lie_attack(user_grads[:n_attacker], z_values[n_attacker])\n",
    "            elif at_type == 'fang':\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                deviation = torch.sign(agg_grads)\n",
    "                mal_update = get_malicious_updates_fang(user_grads[:n_attacker], agg_grads, deviation, n_attacker_)\n",
    "            elif at_type == 'our-agr':\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                mal_update = our_attack_mkrum(user_grads[:n_attacker], agg_grads, n_attacker_, dev_type)\n",
    "            elif at_type == 'min-max':\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                mal_update = our_attack_dist(user_grads[:n_attacker], agg_grads, n_attacker_, dev_type)\n",
    "            elif at_type == 'min-sum':\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                mal_update = our_attack_score(user_grads[:n_attacker], agg_grads, n_attacker_, dev_type)\n",
    "\n",
    "            mal_updates = torch.stack([mal_update] * n_attacker)\n",
    "            malicious_grads = torch.cat((mal_updates, user_grads[n_attacker:]), 0)\n",
    "            \n",
    "        if not (malicious_grads.shape[0]==50):\n",
    "            print(malicious_grads.shape)\n",
    "            sys.exit()\n",
    "        \n",
    "        agg_grads, krum_candidate=bulyan(malicious_grads, n_attacker)\n",
    "\n",
    "        del user_grads\n",
    "\n",
    "        start_idx=0\n",
    "\n",
    "        optimizer_fed.zero_grad()\n",
    "\n",
    "        model_grads=[]\n",
    "\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "\n",
    "        optimizer_fed.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if is_best:\n",
    "            best_global_te_acc = te_acc\n",
    "\n",
    "        if epoch_num%25==0 or epoch_num==nepochs-1:\n",
    "            print('%s: at %s n_at %d n_mal_sel %d e %d | val loss %.4f val acc %.4f best val_acc %f'%(aggregation, at_type, n_attacker, np.sum(krum_candidate < n_attacker), epoch_num, val_loss, val_acc, best_global_acc))\n",
    "\n",
    "        if val_loss > 1000:\n",
    "            print('val loss %f too high'%val_loss)\n",
    "            break\n",
    "            \n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for our first AGR-agnostic attack called Min-Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MIN-MAX attack\n",
    "'''\n",
    "def our_attack_dist(all_updates, model_re, n_attackers, dev_type='unit_vec'):\n",
    "\n",
    "    if dev_type == 'unit_vec':\n",
    "        deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "    elif dev_type == 'sign':\n",
    "        deviation = torch.sign(model_re)\n",
    "    elif dev_type == 'std':\n",
    "        deviation = torch.std(all_updates, 0)\n",
    "\n",
    "    lamda = torch.Tensor([10.0]).float().cuda()\n",
    "    # print(lamda)\n",
    "    threshold_diff = 1e-5\n",
    "    lamda_fail = lamda\n",
    "    lamda_succ = 0\n",
    "    \n",
    "    distances = []\n",
    "    for update in all_updates:\n",
    "        distance = torch.norm((all_updates - update), dim=1) ** 2\n",
    "        distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)\n",
    "    \n",
    "    max_distance = torch.max(distances)\n",
    "    del distances\n",
    "\n",
    "    while torch.abs(lamda_succ - lamda) > threshold_diff:\n",
    "        mal_update = (model_re - lamda * deviation)\n",
    "        distance = torch.norm((all_updates - mal_update), dim=1) ** 2\n",
    "        max_d = torch.max(distance)\n",
    "        \n",
    "        if max_d <= max_distance:\n",
    "            # print('successful lamda is ', lamda)\n",
    "            lamda_succ = lamda\n",
    "            lamda = lamda + lamda_fail / 2\n",
    "        else:\n",
    "            lamda = lamda - lamda_fail / 2\n",
    "\n",
    "        lamda_fail = lamda_fail / 2\n",
    "\n",
    "    mal_update = (model_re - lamda_succ * deviation)\n",
    "    \n",
    "    return mal_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Min-max attack on Bulyan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bulyan: at min-max n_at 10 n_mal_sel 6 e 0 | val loss 2.3028 val acc 9.7200 best val_acc 9.719968\n",
      "bulyan: at min-max n_at 10 n_mal_sel 6 e 25 | val loss 2.1601 val acc 21.0024 best val_acc 21.063312\n",
      "bulyan: at min-max n_at 10 n_mal_sel 7 e 50 | val loss 2.2783 val acc 10.0852 best val_acc 21.063312\n",
      "bulyan: at min-max n_at 10 n_mal_sel 8 e 75 | val loss 2.2781 val acc 11.9927 best val_acc 21.063312\n",
      "bulyan: at min-max n_at 10 n_mal_sel 8 e 100 | val loss 2.2382 val acc 15.9294 best val_acc 21.063312\n",
      "bulyan: at min-max n_at 10 n_mal_sel 8 e 125 | val loss 2.2680 val acc 16.0308 best val_acc 21.063312\n",
      "bulyan: at min-max n_at 10 n_mal_sel 6 e 150 | val loss 2.1953 val acc 17.9383 best val_acc 21.063312\n",
      "bulyan: at min-max n_at 10 n_mal_sel 7 e 175 | val loss 2.2740 val acc 16.0308 best val_acc 21.063312\n",
      "bulyan: at min-max n_at 10 n_mal_sel 7 e 200 | val loss 2.2124 val acc 16.9237 best val_acc 21.205357\n",
      "bulyan: at min-max n_at 10 n_mal_sel 7 e 225 | val loss 2.0993 val acc 17.5122 best val_acc 21.205357\n",
      "bulyan: at min-max n_at 10 n_mal_sel 6 e 250 | val loss 2.1321 val acc 20.8604 best val_acc 24.776786\n",
      "bulyan: at min-max n_at 10 n_mal_sel 7 e 275 | val loss 2.1841 val acc 16.4976 best val_acc 24.776786\n",
      "bulyan: at min-max n_at 10 n_mal_sel 6 e 300 | val loss 2.1033 val acc 22.3823 best val_acc 24.776786\n",
      "bulyan: at min-max n_at 10 n_mal_sel 6 e 325 | val loss 2.2046 val acc 16.5381 best val_acc 24.776786\n",
      "bulyan: at min-max n_at 10 n_mal_sel 5 e 350 | val loss 2.2130 val acc 18.5877 best val_acc 25.121753\n",
      "bulyan: at min-max n_at 10 n_mal_sel 8 e 375 | val loss 2.0875 val acc 20.0284 best val_acc 26.420455\n",
      "bulyan: at min-max n_at 10 n_mal_sel 8 e 400 | val loss 2.1145 val acc 22.5041 best val_acc 29.484578\n",
      "bulyan: at min-max n_at 10 n_mal_sel 7 e 425 | val loss 2.1395 val acc 19.8458 best val_acc 29.484578\n",
      "bulyan: at min-max n_at 10 n_mal_sel 7 e 450 | val loss 2.9904 val acc 14.0422 best val_acc 29.484578\n",
      "bulyan: at min-max n_at 10 n_mal_sel 7 e 475 | val loss 2.0511 val acc 22.7476 best val_acc 29.484578\n",
      "bulyan: at min-max n_at 10 n_mal_sel 8 e 500 | val loss 2.0049 val acc 22.7273 best val_acc 29.484578\n",
      "bulyan: at min-max n_at 10 n_mal_sel 5 e 525 | val loss 3.4853 val acc 9.8417 best val_acc 29.484578\n",
      "bulyan: at min-max n_at 10 n_mal_sel 7 e 550 | val loss 2.0491 val acc 20.9821 best val_acc 29.484578\n",
      "bulyan: at min-max n_at 10 n_mal_sel 7 e 575 | val loss 2.1051 val acc 23.2752 best val_acc 29.484578\n",
      "bulyan: at min-max n_at 10 n_mal_sel 7 e 600 | val loss 2.0474 val acc 22.4432 best val_acc 29.484578\n",
      "bulyan: at min-max n_at 10 n_mal_sel 6 e 625 | val loss 2.1401 val acc 18.7094 best val_acc 29.484578\n",
      "bulyan: at min-max n_at 10 n_mal_sel 6 e 650 | val loss 2.0226 val acc 21.9562 best val_acc 29.484578\n",
      "bulyan: at min-max n_at 10 n_mal_sel 7 e 675 | val loss 2.2318 val acc 15.2192 best val_acc 29.484578\n",
      "bulyan: at min-max n_at 10 n_mal_sel 7 e 700 | val loss 2.0580 val acc 19.8052 best val_acc 29.484578\n",
      "bulyan: at min-max n_at 10 n_mal_sel 6 e 725 | val loss 2.0923 val acc 19.3588 best val_acc 29.484578\n",
      "bulyan: at min-max n_at 10 n_mal_sel 7 e 750 | val loss 2.6201 val acc 11.8709 best val_acc 29.484578\n",
      "val loss 4599.555582 too high\n"
     ]
    }
   ],
   "source": [
    "batch_size=250\n",
    "resume=0\n",
    "nepochs=1200\n",
    "schedule=[1000]\n",
    "nbatches = user_tr_len//batch_size\n",
    "\n",
    "gamma=.5\n",
    "opt = 'sgd'\n",
    "fed_lr=0.5\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "aggregation='bulyan'\n",
    "multi_k = False\n",
    "candidates = []\n",
    "\n",
    "at_type='min-max'\n",
    "dev_type ='std'\n",
    "n_attackers=[10]\n",
    "\n",
    "arch='alexnet'\n",
    "chkpt='./'+aggregation\n",
    "\n",
    "for n_attacker in n_attackers:\n",
    "    candidates = []\n",
    "\n",
    "    epoch_num = 0\n",
    "    best_global_acc = 0\n",
    "    best_global_te_acc = 0\n",
    "\n",
    "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
    "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    r=np.arange(user_tr_len)\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        user_grads=[]\n",
    "        if not epoch_num and epoch_num%nbatches == 0:\n",
    "            np.random.shuffle(r)\n",
    "            for i in range(nusers):\n",
    "                user_tr_data_tensors[i]=user_tr_data_tensors[i][r]\n",
    "                user_tr_label_tensors[i]=user_tr_label_tensors[i][r]\n",
    "\n",
    "        for i in range(nusers):\n",
    "\n",
    "            inputs = user_tr_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "            targets = user_tr_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "            outputs = fed_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            fed_model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            param_grad=[]\n",
    "            for param in fed_model.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "\n",
    "        if epoch_num in schedule:\n",
    "            for param_group in optimizer_fed.param_groups:\n",
    "                param_group['lr'] *= gamma\n",
    "                print('New learnin rate ', param_group['lr'])\n",
    "\n",
    "        if n_attacker > 0:\n",
    "            n_attacker_ = max(1, n_attacker**2//nusers)\n",
    "            if at_type == 'lie':\n",
    "                mal_update = lie_attack(user_grads[:n_attacker], z_values[n_attacker])\n",
    "            elif at_type == 'fang':\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                deviation = torch.sign(agg_grads)\n",
    "                mal_update = get_malicious_updates_fang(user_grads[:n_attacker], agg_grads, deviation, n_attacker_)\n",
    "            elif at_type == 'our-agr':\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                mal_update = our_attack_mkrum(user_grads[:n_attacker], agg_grads, n_attacker_, dev_type)\n",
    "            elif at_type == 'min-max':\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                mal_update = our_attack_dist(user_grads[:n_attacker], agg_grads, n_attacker_, dev_type)\n",
    "            elif at_type == 'min-sum':\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                mal_update = our_attack_score(user_grads[:n_attacker], agg_grads, n_attacker_, dev_type)\n",
    "\n",
    "            mal_updates = torch.stack([mal_update] * n_attacker)\n",
    "            malicious_grads = torch.cat((mal_updates, user_grads[n_attacker:]), 0)\n",
    "\n",
    "        if not (malicious_grads.shape[0]==50):\n",
    "            print(malicious_grads.shape)\n",
    "            sys.exit()\n",
    "        \n",
    "        agg_grads, krum_candidate=bulyan(malicious_grads, n_attacker)\n",
    "\n",
    "        del user_grads\n",
    "\n",
    "        start_idx=0\n",
    "\n",
    "        optimizer_fed.zero_grad()\n",
    "\n",
    "        model_grads=[]\n",
    "\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "\n",
    "        optimizer_fed.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if is_best:\n",
    "            best_global_te_acc = te_acc\n",
    "\n",
    "        if epoch_num%25==0 or epoch_num==nepochs-1:\n",
    "            print('%s: at %s n_at %d n_mal_sel %d e %d | val loss %.4f val acc %.4f best val_acc %f'%(aggregation, at_type, n_attacker, np.sum(krum_candidate < n_attacker), epoch_num, val_loss, val_acc, best_global_acc))\n",
    "\n",
    "        if val_loss > 1000:\n",
    "            print('val loss %f too high'%val_loss)\n",
    "            break\n",
    "            \n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for our second AGR-agnostic attack called Min-Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MIN-SUM attack\n",
    "'''\n",
    "\n",
    "def our_attack_score(all_updates, model_re, n_attackers, dev_type='unit_vec'):\n",
    "\n",
    "    if dev_type == 'unit_vec':\n",
    "        deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "    elif dev_type == 'sign':\n",
    "        deviation = torch.sign(model_re)\n",
    "    elif dev_type == 'std':\n",
    "        deviation = torch.std(all_updates, 0)\n",
    "    \n",
    "    lamda = torch.Tensor([10.0]).float().cuda()\n",
    "    # print(lamda)\n",
    "    threshold_diff = 1e-5\n",
    "    lamda_fail = lamda\n",
    "    lamda_succ = 0\n",
    "    \n",
    "    distances = []\n",
    "    for update in all_updates:\n",
    "        distance = torch.norm((all_updates - update), dim=1) ** 2\n",
    "        distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)\n",
    "    \n",
    "    scores = torch.sum(distances, dim=1)\n",
    "    min_score = torch.min(scores)\n",
    "    del distances\n",
    "\n",
    "    while torch.abs(lamda_succ - lamda) > threshold_diff:\n",
    "        mal_update = (model_re - lamda * deviation)\n",
    "        distance = torch.norm((all_updates - mal_update), dim=1) ** 2\n",
    "        score = torch.sum(distance)\n",
    "        \n",
    "        if score <= min_score:\n",
    "            # print('successful lamda is ', lamda)\n",
    "            lamda_succ = lamda\n",
    "            lamda = lamda + lamda_fail / 2\n",
    "        else:\n",
    "            lamda = lamda - lamda_fail / 2\n",
    "\n",
    "        lamda_fail = lamda_fail / 2\n",
    "\n",
    "    # print(lamda_succ)\n",
    "    mal_update = (model_re - lamda_succ * deviation)\n",
    "    \n",
    "    return mal_update\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Min-Sum attack on Bulyan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 0 | val loss 2.3033 val acc 13.5755 best val_acc 13.575487\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 25 | val loss 2.2844 val acc 13.7175 best val_acc 18.445617\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 9 e 50 | val loss 2.2997 val acc 13.7378 best val_acc 21.692370\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 75 | val loss 2.2287 val acc 17.1063 best val_acc 21.692370\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 9 e 100 | val loss 2.2648 val acc 14.0422 best val_acc 21.753247\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 125 | val loss 2.0972 val acc 19.2979 best val_acc 22.402597\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 150 | val loss 2.1726 val acc 16.4773 best val_acc 22.463474\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 175 | val loss 2.3079 val acc 13.4131 best val_acc 25.405844\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 200 | val loss 2.3413 val acc 9.8417 best val_acc 25.405844\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 225 | val loss 2.3178 val acc 10.6534 best val_acc 25.405844\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 250 | val loss 2.3304 val acc 16.1932 best val_acc 25.405844\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 275 | val loss 2.2793 val acc 16.6802 best val_acc 25.405844\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 300 | val loss 2.1656 val acc 14.9959 best val_acc 25.405844\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 325 | val loss 2.3680 val acc 9.9229 best val_acc 25.405844\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 350 | val loss 2.0688 val acc 16.8628 best val_acc 25.405844\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 375 | val loss 2.2662 val acc 14.7119 best val_acc 25.405844\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 400 | val loss 2.2477 val acc 20.0893 best val_acc 25.405844\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 425 | val loss 2.3271 val acc 9.8417 best val_acc 25.405844\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 450 | val loss 2.2692 val acc 12.2768 best val_acc 25.405844\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 475 | val loss 2.2226 val acc 16.1729 best val_acc 25.405844\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 500 | val loss 2.3549 val acc 12.0536 best val_acc 25.405844\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 525 | val loss 2.4161 val acc 11.8912 best val_acc 25.405844\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 550 | val loss 2.1345 val acc 18.3847 best val_acc 25.405844\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 575 | val loss 2.0377 val acc 24.4115 best val_acc 25.405844\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 600 | val loss 2.2604 val acc 16.1120 best val_acc 25.507305\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 625 | val loss 2.4040 val acc 9.7403 best val_acc 25.507305\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 650 | val loss 2.1989 val acc 17.5731 best val_acc 25.507305\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 675 | val loss 2.3435 val acc 9.7403 best val_acc 25.507305\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 700 | val loss 2.3559 val acc 10.1055 best val_acc 25.507305\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 725 | val loss 2.2758 val acc 16.1526 best val_acc 25.507305\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 750 | val loss 2.3521 val acc 10.2679 best val_acc 25.507305\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 775 | val loss 2.3146 val acc 11.0390 best val_acc 25.507305\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 9 e 800 | val loss 2.1576 val acc 19.4196 best val_acc 25.507305\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 825 | val loss 2.2458 val acc 15.4830 best val_acc 25.507305\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 850 | val loss 2.6216 val acc 10.3896 best val_acc 25.507305\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 875 | val loss 2.2992 val acc 12.0333 best val_acc 25.507305\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 9 e 900 | val loss 2.3011 val acc 15.9497 best val_acc 25.507305\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 925 | val loss 2.2531 val acc 13.5146 best val_acc 25.507305\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 950 | val loss 2.3700 val acc 10.1867 best val_acc 25.507305\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 975 | val loss 2.1069 val acc 19.1558 best val_acc 25.507305\n",
      "New learnin rate  0.25\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 1000 | val loss 2.1188 val acc 16.9034 best val_acc 25.507305\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 9 e 1025 | val loss 2.3479 val acc 11.6071 best val_acc 25.507305\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 9 e 1050 | val loss 2.2086 val acc 16.4976 best val_acc 25.507305\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 1075 | val loss 2.0758 val acc 18.1209 best val_acc 25.507305\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 1100 | val loss 2.2179 val acc 14.2451 best val_acc 25.507305\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 1125 | val loss 2.4659 val acc 9.8417 best val_acc 25.507305\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 9 e 1150 | val loss 2.3613 val acc 10.2881 best val_acc 25.507305\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 1175 | val loss 2.2200 val acc 15.9903 best val_acc 25.507305\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 1199 | val loss 2.1330 val acc 19.0341 best val_acc 25.507305\n",
      "bulyan: at min-sum n_at 10 n_mal_sel 10 e 1200 | val loss 2.1540 val acc 18.5268 best val_acc 25.507305\n"
     ]
    }
   ],
   "source": [
    "batch_size=250\n",
    "resume=0\n",
    "nepochs=1200\n",
    "schedule=[1000]\n",
    "nbatches = user_tr_len//batch_size\n",
    "\n",
    "gamma=.5\n",
    "opt = 'sgd'\n",
    "fed_lr=0.5\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "aggregation='bulyan'\n",
    "multi_k = False\n",
    "candidates = []\n",
    "\n",
    "at_type='min-sum'\n",
    "dev_type ='std'\n",
    "z=0\n",
    "n_attackers=[10]\n",
    "\n",
    "arch='alexnet'\n",
    "chkpt='./'+aggregation\n",
    "\n",
    "for n_attacker in n_attackers:\n",
    "    candidates = []\n",
    "\n",
    "    epoch_num = 0\n",
    "    best_global_acc = 0\n",
    "    best_global_te_acc = 0\n",
    "\n",
    "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
    "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    r=np.arange(user_tr_len)\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        user_grads=[]\n",
    "        if not epoch_num and epoch_num%nbatches == 0:\n",
    "            np.random.shuffle(r)\n",
    "            for i in range(nusers):\n",
    "                user_tr_data_tensors[i]=user_tr_data_tensors[i][r]\n",
    "                user_tr_label_tensors[i]=user_tr_label_tensors[i][r]\n",
    "\n",
    "        for i in range(nusers):\n",
    "\n",
    "            inputs = user_tr_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "            targets = user_tr_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "            outputs = fed_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            fed_model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            param_grad=[]\n",
    "            for param in fed_model.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "\n",
    "        if epoch_num in schedule:\n",
    "            for param_group in optimizer_fed.param_groups:\n",
    "                param_group['lr'] *= gamma\n",
    "                print('New learnin rate ', param_group['lr'])\n",
    "\n",
    "        if n_attacker > 0:\n",
    "            n_attacker_ = max(1, n_attacker**2//nusers)\n",
    "            if at_type == 'lie':\n",
    "                mal_update = lie_attack(user_grads[:n_attacker], z_values[n_attacker])\n",
    "            elif at_type == 'fang':\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                deviation = torch.sign(agg_grads)\n",
    "                mal_update = get_malicious_updates_fang(user_grads[:n_attacker], agg_grads, deviation, n_attacker_)\n",
    "            elif at_type == 'our-agr':\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                mal_update = our_attack_mkrum(user_grads[:n_attacker], agg_grads, n_attacker_, dev_type)\n",
    "            elif at_type == 'min-max':\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                mal_update = our_attack_dist(user_grads[:n_attacker], agg_grads, n_attacker_, dev_type)\n",
    "            elif at_type == 'min-sum':\n",
    "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
    "                mal_update = our_attack_score(user_grads[:n_attacker], agg_grads, n_attacker_, dev_type)\n",
    "\n",
    "            mal_updates = torch.stack([mal_update] * n_attacker)\n",
    "            malicious_grads = torch.cat((mal_updates, user_grads[n_attacker:]), 0)\n",
    "\n",
    "        if not (malicious_grads.shape[0]==50):\n",
    "            print(malicious_grads.shape)\n",
    "            sys.exit()\n",
    "        \n",
    "        agg_grads, krum_candidate=bulyan(malicious_grads, n_attacker)\n",
    "\n",
    "        del user_grads\n",
    "\n",
    "        start_idx=0\n",
    "\n",
    "        optimizer_fed.zero_grad()\n",
    "\n",
    "        model_grads=[]\n",
    "\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "\n",
    "        optimizer_fed.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if is_best:\n",
    "            best_global_te_acc = te_acc\n",
    "\n",
    "        if epoch_num%25==0 or epoch_num==nepochs-1:\n",
    "            print('%s: at %s n_at %d n_mal_sel %d e %d | val loss %.4f val acc %.4f best val_acc %f'%(aggregation, at_type, n_attacker, np.sum(krum_candidate < n_attacker), epoch_num, val_loss, val_acc, best_global_acc))\n",
    "\n",
    "        if val_loss > 1000:\n",
    "            print('val loss %f too high'%val_loss)\n",
    "            break\n",
    "            \n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
