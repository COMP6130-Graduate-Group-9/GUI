{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The notebook contains\n",
    "### Code for _Trimmed-mean_ aggregation algorithm\n",
    "### Evaluation of all of the attacks (Fang, LIE, and our SOTA AGR-tailored and AGR-agnstic) on Trimmed-mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse, os, sys, csv, shutil, time, random, operator, pickle, ast, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.optim import Optimizer\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "sys.path.insert(0,'./../utils/')\n",
    "from logger import *\n",
    "from eval import *\n",
    "from misc import *\n",
    "\n",
    "from cifar10_normal_train import *\n",
    "from cifar10_util import *\n",
    "from adam import Adam\n",
    "from sgd import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get CIFAR10 data and split it in IID fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "data_loc='/mnt/nfs/work1/amir/vshejwalkar/cifar10_data/'\n",
    "# load the train dataset\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=train_transform)\n",
    "\n",
    "cifar10_test = datasets.CIFAR10(root=data_loc, train=False, download=True, transform=train_transform)\n",
    "\n",
    "X=[]\n",
    "Y=[]\n",
    "for i in range(len(cifar10_train)):\n",
    "    X.append(cifar10_train[i][0].numpy())\n",
    "    Y.append(cifar10_train[i][1])\n",
    "\n",
    "for i in range(len(cifar10_test)):\n",
    "    X.append(cifar10_test[i][0].numpy())\n",
    "    Y.append(cifar10_test[i][1])\n",
    "\n",
    "X=np.array(X)\n",
    "Y=np.array(Y)\n",
    "\n",
    "print('total data len: ',len(X))\n",
    "\n",
    "if not os.path.isfile('./cifar10_shuffle.pkl'):\n",
    "    all_indices = np.arange(len(X))\n",
    "    np.random.shuffle(all_indices)\n",
    "    pickle.dump(all_indices,open('./cifar10_shuffle.pkl','wb'))\n",
    "else:\n",
    "    all_indices=pickle.load(open('./cifar10_shuffle.pkl','rb'))\n",
    "\n",
    "X=X[all_indices]\n",
    "Y=Y[all_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data len:  60000\n",
      "total tr len 50000 | val len 5000 | test len 5000\n",
      "user 0 tr len 1000\n",
      "user 1 tr len 1000\n",
      "user 2 tr len 1000\n",
      "user 3 tr len 1000\n",
      "user 4 tr len 1000\n",
      "user 5 tr len 1000\n",
      "user 6 tr len 1000\n",
      "user 7 tr len 1000\n",
      "user 8 tr len 1000\n",
      "user 9 tr len 1000\n",
      "user 10 tr len 1000\n",
      "user 11 tr len 1000\n",
      "user 12 tr len 1000\n",
      "user 13 tr len 1000\n",
      "user 14 tr len 1000\n",
      "user 15 tr len 1000\n",
      "user 16 tr len 1000\n",
      "user 17 tr len 1000\n",
      "user 18 tr len 1000\n",
      "user 19 tr len 1000\n",
      "user 20 tr len 1000\n",
      "user 21 tr len 1000\n",
      "user 22 tr len 1000\n",
      "user 23 tr len 1000\n",
      "user 24 tr len 1000\n",
      "user 25 tr len 1000\n",
      "user 26 tr len 1000\n",
      "user 27 tr len 1000\n",
      "user 28 tr len 1000\n",
      "user 29 tr len 1000\n",
      "user 30 tr len 1000\n",
      "user 31 tr len 1000\n",
      "user 32 tr len 1000\n",
      "user 33 tr len 1000\n",
      "user 34 tr len 1000\n",
      "user 35 tr len 1000\n",
      "user 36 tr len 1000\n",
      "user 37 tr len 1000\n",
      "user 38 tr len 1000\n",
      "user 39 tr len 1000\n",
      "user 40 tr len 1000\n",
      "user 41 tr len 1000\n",
      "user 42 tr len 1000\n",
      "user 43 tr len 1000\n",
      "user 44 tr len 1000\n",
      "user 45 tr len 1000\n",
      "user 46 tr len 1000\n",
      "user 47 tr len 1000\n",
      "user 48 tr len 1000\n",
      "user 49 tr len 1000\n"
     ]
    }
   ],
   "source": [
    "# data loading\n",
    "\n",
    "nusers=50\n",
    "user_tr_len=1000\n",
    "\n",
    "total_tr_len=user_tr_len*nusers\n",
    "val_len=5000\n",
    "te_len=5000\n",
    "\n",
    "print('total data len: ',len(X))\n",
    "\n",
    "if not os.path.isfile('./cifar10_shuffle.pkl'):\n",
    "    all_indices = np.arange(len(X))\n",
    "    np.random.shuffle(all_indices)\n",
    "    pickle.dump(all_indices,open('./cifar10_shuffle.pkl','wb'))\n",
    "else:\n",
    "    all_indices=pickle.load(open('./cifar10_shuffle.pkl','rb'))\n",
    "\n",
    "total_tr_data=X[:total_tr_len]\n",
    "total_tr_label=Y[:total_tr_len]\n",
    "\n",
    "val_data=X[total_tr_len:(total_tr_len+val_len)]\n",
    "val_label=Y[total_tr_len:(total_tr_len+val_len)]\n",
    "\n",
    "te_data=X[(total_tr_len+val_len):(total_tr_len+val_len+te_len)]\n",
    "te_label=Y[(total_tr_len+val_len):(total_tr_len+val_len+te_len)]\n",
    "\n",
    "total_tr_data_tensor=torch.from_numpy(total_tr_data).type(torch.FloatTensor)\n",
    "total_tr_label_tensor=torch.from_numpy(total_tr_label).type(torch.LongTensor)\n",
    "\n",
    "val_data_tensor=torch.from_numpy(val_data).type(torch.FloatTensor)\n",
    "val_label_tensor=torch.from_numpy(val_label).type(torch.LongTensor)\n",
    "\n",
    "te_data_tensor=torch.from_numpy(te_data).type(torch.FloatTensor)\n",
    "te_label_tensor=torch.from_numpy(te_label).type(torch.LongTensor)\n",
    "\n",
    "print('total tr len %d | val len %d | test len %d'%(len(total_tr_data_tensor),len(val_data_tensor),len(te_data_tensor)))\n",
    "\n",
    "#==============================================================================================================\n",
    "\n",
    "user_tr_data_tensors=[]\n",
    "user_tr_label_tensors=[]\n",
    "\n",
    "for i in range(nusers):\n",
    "    \n",
    "    user_tr_data_tensor=torch.from_numpy(total_tr_data[user_tr_len*i:user_tr_len*(i+1)]).type(torch.FloatTensor)\n",
    "    user_tr_label_tensor=torch.from_numpy(total_tr_label[user_tr_len*i:user_tr_len*(i+1)]).type(torch.LongTensor)\n",
    "\n",
    "    user_tr_data_tensors.append(user_tr_data_tensor)\n",
    "    user_tr_label_tensors.append(user_tr_label_tensor)\n",
    "    print('user %d tr len %d'%(i,len(user_tr_data_tensor)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Trimmed-mean aggregation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_mean(all_updates, n_attackers):\n",
    "    sorted_updates = torch.sort(all_updates, 0)[0]\n",
    "    out = torch.mean(sorted_updates[n_attackers:-n_attackers], 0) if n_attackers else torch.mean(sorted_updates,0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full knowledge Fang attack on Trimmed mean aggregation\n",
    "### Note that Fang attacks on Trimmed-mean and median are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_malicious_updates_fang_trmean(all_updates, deviation, n_attackers, epoch_num, compression='none', q_level=2, norm='inf'):\n",
    "    b = 2\n",
    "    max_vector = torch.max(all_updates, 0)[0]\n",
    "    min_vector = torch.min(all_updates, 0)[0]\n",
    "\n",
    "    max_ = (max_vector > 0).type(torch.FloatTensor).cuda()\n",
    "    min_ = (min_vector < 0).type(torch.FloatTensor).cuda()\n",
    "\n",
    "    max_[max_ == 1] = b\n",
    "    max_[max_ == 0] = 1 / b\n",
    "    min_[min_ == 1] = b\n",
    "    min_[min_ == 0] = 1 / b\n",
    "\n",
    "    max_range = torch.cat((max_vector[:, None], (max_vector * max_)[:, None]), dim=1)\n",
    "    min_range = torch.cat(((min_vector * min_)[:, None], min_vector[:, None]), dim=1)\n",
    "\n",
    "    rand = torch.from_numpy(np.random.uniform(0, 1, [len(deviation), n_attackers])).type(torch.FloatTensor).cuda()\n",
    "\n",
    "    max_rand = torch.stack([max_range[:, 0]] * rand.shape[1]).T + rand * torch.stack([max_range[:, 1] - max_range[:, 0]] * rand.shape[1]).T\n",
    "    min_rand = torch.stack([min_range[:, 0]] * rand.shape[1]).T + rand * torch.stack([min_range[:, 1] - min_range[:, 0]] * rand.shape[1]).T\n",
    "\n",
    "    mal_vec = (torch.stack([(deviation > 0).type(torch.FloatTensor)] * max_rand.shape[1]).T.cuda() * max_rand + torch.stack(\n",
    "        [(deviation > 0).type(torch.FloatTensor)] * min_rand.shape[1]).T.cuda() * min_rand).T\n",
    "\n",
    "    quant_mal_vec = []\n",
    "    if compression != 'none':\n",
    "        if epoch_num == 0: print('compressing malicious update')\n",
    "        for i in range(mal_vec.shape[0]):\n",
    "            mal_ = mal_vec[i]\n",
    "            mal_quant = qsgd(mal_, s=q_level, norm=norm)\n",
    "            quant_mal_vec = mal_quant[None, :] if not len(quant_mal_vec) else torch.cat((quant_mal_vec, mal_quant[None, :]), 0)\n",
    "    else:\n",
    "        quant_mal_vec = mal_vec\n",
    "\n",
    "    mal_updates = torch.cat((quant_mal_vec, all_updates), 0)\n",
    "\n",
    "    return mal_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 2472266])\n",
      "trmean: at fang n_at 10 e 0 fed_model val loss 2.3023 val acc 9.8011 best val_acc 9.801136 te_acc 10.491071\n",
      "trmean: at fang n_at 10 e 10 fed_model val loss 2.2965 val acc 10.4911 best val_acc 10.693994 te_acc 11.708604\n",
      "trmean: at fang n_at 10 e 20 fed_model val loss 2.2621 val acc 18.3036 best val_acc 19.602273 te_acc 19.622565\n",
      "trmean: at fang n_at 10 e 30 fed_model val loss 2.1581 val acc 22.0982 best val_acc 22.098214 te_acc 21.773539\n",
      "trmean: at fang n_at 10 e 40 fed_model val loss 2.1621 val acc 20.6778 best val_acc 22.098214 te_acc 21.773539\n",
      "trmean: at fang n_at 10 e 50 fed_model val loss 2.1552 val acc 19.4805 best val_acc 23.011364 te_acc 22.199675\n",
      "trmean: at fang n_at 10 e 60 fed_model val loss 2.0702 val acc 20.8401 best val_acc 23.620130 te_acc 23.823052\n",
      "trmean: at fang n_at 10 e 70 fed_model val loss 2.0510 val acc 21.8344 best val_acc 23.620130 te_acc 23.823052\n",
      "trmean: at fang n_at 10 e 80 fed_model val loss 1.9517 val acc 27.2524 best val_acc 27.252435 te_acc 25.669643\n",
      "trmean: at fang n_at 10 e 90 fed_model val loss 1.9882 val acc 24.4927 best val_acc 27.495942 te_acc 27.495942\n",
      "trmean: at fang n_at 10 e 100 fed_model val loss 1.9397 val acc 26.2175 best val_acc 29.504870 te_acc 29.281656\n",
      "trmean: at fang n_at 10 e 110 fed_model val loss 1.8608 val acc 30.5601 best val_acc 30.560065 te_acc 30.905032\n",
      "trmean: at fang n_at 10 e 120 fed_model val loss 1.9091 val acc 28.9976 best val_acc 31.797890 te_acc 31.107955\n",
      "trmean: at fang n_at 10 e 130 fed_model val loss 1.9657 val acc 26.7451 best val_acc 32.609578 te_acc 33.056006\n",
      "trmean: at fang n_at 10 e 140 fed_model val loss 1.8640 val acc 31.3718 best val_acc 32.609578 te_acc 33.056006\n",
      "trmean: at fang n_at 10 e 150 fed_model val loss 1.7793 val acc 34.1721 best val_acc 34.172078 te_acc 34.415584\n",
      "trmean: at fang n_at 10 e 160 fed_model val loss 1.8980 val acc 29.8093 best val_acc 36.891234 te_acc 36.525974\n",
      "trmean: at fang n_at 10 e 170 fed_model val loss 1.7619 val acc 32.8734 best val_acc 36.891234 te_acc 36.525974\n",
      "trmean: at fang n_at 10 e 180 fed_model val loss 1.9607 val acc 28.2062 best val_acc 36.891234 te_acc 36.525974\n",
      "trmean: at fang n_at 10 e 190 fed_model val loss 1.7616 val acc 33.1981 best val_acc 39.001623 te_acc 38.311688\n",
      "trmean: at fang n_at 10 e 200 fed_model val loss 1.7338 val acc 35.1867 best val_acc 39.001623 te_acc 38.311688\n",
      "trmean: at fang n_at 10 e 210 fed_model val loss 1.6207 val acc 41.3352 best val_acc 41.335227 te_acc 41.112013\n",
      "trmean: at fang n_at 10 e 220 fed_model val loss 1.6897 val acc 36.9115 best val_acc 41.335227 te_acc 41.112013\n",
      "trmean: at fang n_at 10 e 230 fed_model val loss 1.6776 val acc 37.9464 best val_acc 41.822240 te_acc 41.375812\n",
      "trmean: at fang n_at 10 e 240 fed_model val loss 1.6198 val acc 41.5179 best val_acc 42.248377 te_acc 42.816558\n",
      "trmean: at fang n_at 10 e 250 fed_model val loss 1.6608 val acc 39.1031 best val_acc 42.715097 te_acc 42.207792\n",
      "trmean: at fang n_at 10 e 260 fed_model val loss 1.6846 val acc 39.1031 best val_acc 42.715097 te_acc 42.207792\n",
      "trmean: at fang n_at 10 e 270 fed_model val loss 1.5846 val acc 42.9789 best val_acc 42.978896 te_acc 42.735390\n",
      "trmean: at fang n_at 10 e 280 fed_model val loss 1.7322 val acc 39.3669 best val_acc 44.196429 te_acc 44.115260\n",
      "trmean: at fang n_at 10 e 290 fed_model val loss 1.5525 val acc 43.6080 best val_acc 44.196429 te_acc 44.115260\n",
      "trmean: at fang n_at 10 e 300 fed_model val loss 1.5926 val acc 42.4107 best val_acc 44.277597 te_acc 44.683442\n",
      "trmean: at fang n_at 10 e 310 fed_model val loss 1.5770 val acc 42.2484 best val_acc 45.616883 te_acc 45.616883\n",
      "trmean: at fang n_at 10 e 320 fed_model val loss 1.5098 val acc 46.0430 best val_acc 46.266234 te_acc 45.921266\n",
      "trmean: at fang n_at 10 e 330 fed_model val loss 1.6310 val acc 40.1380 best val_acc 46.550325 te_acc 46.550325\n",
      "trmean: at fang n_at 10 e 340 fed_model val loss 1.5236 val acc 45.1705 best val_acc 46.550325 te_acc 46.550325\n",
      "trmean: at fang n_at 10 e 350 fed_model val loss 1.5181 val acc 44.2979 best val_acc 47.017045 te_acc 46.448864\n",
      "trmean: at fang n_at 10 e 360 fed_model val loss 1.5499 val acc 43.6891 best val_acc 47.869318 te_acc 47.666396\n",
      "trmean: at fang n_at 10 e 370 fed_model val loss 1.5718 val acc 42.7963 best val_acc 47.869318 te_acc 47.666396\n",
      "trmean: at fang n_at 10 e 380 fed_model val loss 1.4740 val acc 46.1039 best val_acc 48.457792 te_acc 48.214286\n",
      "trmean: at fang n_at 10 e 390 fed_model val loss 1.4703 val acc 47.2606 best val_acc 48.457792 te_acc 48.214286\n",
      "trmean: at fang n_at 10 e 400 fed_model val loss 1.4725 val acc 47.6867 best val_acc 48.457792 te_acc 48.214286\n",
      "trmean: at fang n_at 10 e 410 fed_model val loss 1.4509 val acc 46.6112 best val_acc 48.457792 te_acc 48.214286\n",
      "trmean: at fang n_at 10 e 420 fed_model val loss 1.4393 val acc 48.1331 best val_acc 48.457792 te_acc 48.214286\n",
      "trmean: at fang n_at 10 e 430 fed_model val loss 1.5541 val acc 43.0195 best val_acc 49.168019 te_acc 49.857955\n",
      "trmean: at fang n_at 10 e 440 fed_model val loss 1.5246 val acc 45.4951 best val_acc 49.168019 te_acc 49.857955\n",
      "trmean: at fang n_at 10 e 450 fed_model val loss 1.4598 val acc 47.8287 best val_acc 49.168019 te_acc 49.857955\n",
      "trmean: at fang n_at 10 e 460 fed_model val loss 1.4514 val acc 47.6461 best val_acc 49.168019 te_acc 49.857955\n",
      "trmean: at fang n_at 10 e 470 fed_model val loss 1.4894 val acc 45.8401 best val_acc 49.168019 te_acc 49.857955\n",
      "trmean: at fang n_at 10 e 480 fed_model val loss 1.5605 val acc 43.2021 best val_acc 49.614448 te_acc 49.492695\n",
      "trmean: at fang n_at 10 e 490 fed_model val loss 1.4086 val acc 49.2898 best val_acc 49.614448 te_acc 49.492695\n",
      "trmean: at fang n_at 10 e 500 fed_model val loss 1.4126 val acc 49.3506 best val_acc 49.614448 te_acc 49.492695\n",
      "trmean: at fang n_at 10 e 510 fed_model val loss 1.4331 val acc 48.2346 best val_acc 49.614448 te_acc 49.492695\n",
      "trmean: at fang n_at 10 e 520 fed_model val loss 1.4409 val acc 48.4172 best val_acc 49.614448 te_acc 49.492695\n",
      "trmean: at fang n_at 10 e 530 fed_model val loss 1.4560 val acc 47.3417 best val_acc 49.614448 te_acc 49.492695\n",
      "trmean: at fang n_at 10 e 540 fed_model val loss 1.4089 val acc 48.8839 best val_acc 49.614448 te_acc 49.492695\n",
      "trmean: at fang n_at 10 e 550 fed_model val loss 1.4232 val acc 48.4984 best val_acc 49.614448 te_acc 49.492695\n",
      "trmean: at fang n_at 10 e 560 fed_model val loss 1.4483 val acc 47.3620 best val_acc 49.614448 te_acc 49.492695\n",
      "trmean: at fang n_at 10 e 570 fed_model val loss 1.4331 val acc 47.9708 best val_acc 49.614448 te_acc 49.492695\n",
      "trmean: at fang n_at 10 e 580 fed_model val loss 1.4580 val acc 47.2606 best val_acc 49.614448 te_acc 49.492695\n",
      "trmean: at fang n_at 10 e 590 fed_model val loss 1.4461 val acc 47.1185 best val_acc 49.614448 te_acc 49.492695\n",
      "trmean: at fang n_at 10 e 600 fed_model val loss 1.3906 val acc 49.0869 best val_acc 49.655032 te_acc 49.512987\n",
      "trmean: at fang n_at 10 e 610 fed_model val loss 1.4455 val acc 47.5649 best val_acc 49.655032 te_acc 49.512987\n",
      "trmean: at fang n_at 10 e 620 fed_model val loss 1.4496 val acc 47.3214 best val_acc 50.608766 te_acc 49.878247\n",
      "trmean: at fang n_at 10 e 630 fed_model val loss 1.3897 val acc 49.6956 best val_acc 50.771104 te_acc 51.237825\n",
      "trmean: at fang n_at 10 e 640 fed_model val loss 1.4286 val acc 48.2549 best val_acc 50.771104 te_acc 51.237825\n",
      "trmean: at fang n_at 10 e 650 fed_model val loss 1.5143 val acc 45.4343 best val_acc 50.771104 te_acc 51.237825\n",
      "trmean: at fang n_at 10 e 660 fed_model val loss 1.4248 val acc 48.7622 best val_acc 50.771104 te_acc 51.237825\n",
      "trmean: at fang n_at 10 e 670 fed_model val loss 1.4249 val acc 49.2289 best val_acc 50.771104 te_acc 51.237825\n",
      "trmean: at fang n_at 10 e 680 fed_model val loss 1.4126 val acc 49.4927 best val_acc 50.771104 te_acc 51.237825\n",
      "trmean: at fang n_at 10 e 690 fed_model val loss 1.3883 val acc 50.1826 best val_acc 50.771104 te_acc 51.237825\n",
      "trmean: at fang n_at 10 e 700 fed_model val loss 1.4217 val acc 49.1274 best val_acc 50.771104 te_acc 51.237825\n",
      "trmean: at fang n_at 10 e 710 fed_model val loss 1.4745 val acc 47.4229 best val_acc 50.771104 te_acc 51.237825\n",
      "trmean: at fang n_at 10 e 720 fed_model val loss 1.3862 val acc 50.3856 best val_acc 50.771104 te_acc 51.237825\n",
      "trmean: at fang n_at 10 e 730 fed_model val loss 1.4333 val acc 49.0463 best val_acc 50.771104 te_acc 51.237825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trmean: at fang n_at 10 e 740 fed_model val loss 1.4170 val acc 49.4927 best val_acc 50.771104 te_acc 51.237825\n",
      "trmean: at fang n_at 10 e 750 fed_model val loss 1.3967 val acc 49.5739 best val_acc 50.771104 te_acc 51.237825\n",
      "trmean: at fang n_at 10 e 760 fed_model val loss 1.4202 val acc 48.9042 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 770 fed_model val loss 1.3929 val acc 50.1218 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 780 fed_model val loss 1.3900 val acc 50.3044 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 790 fed_model val loss 1.4255 val acc 48.2955 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 800 fed_model val loss 1.4077 val acc 49.9391 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 810 fed_model val loss 1.4020 val acc 49.1883 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 820 fed_model val loss 1.4265 val acc 49.6347 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 830 fed_model val loss 1.4197 val acc 49.6956 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 840 fed_model val loss 1.3983 val acc 49.8580 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 850 fed_model val loss 1.4698 val acc 47.1591 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 860 fed_model val loss 1.4428 val acc 49.1477 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 870 fed_model val loss 1.4691 val acc 48.1128 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 880 fed_model val loss 1.4466 val acc 47.9302 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 890 fed_model val loss 1.4288 val acc 49.3912 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 900 fed_model val loss 1.4253 val acc 49.0666 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 910 fed_model val loss 1.4452 val acc 48.9245 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 920 fed_model val loss 1.4321 val acc 49.4927 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 930 fed_model val loss 1.4439 val acc 48.7622 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 940 fed_model val loss 1.4225 val acc 50.0406 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 950 fed_model val loss 1.4746 val acc 47.7476 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 960 fed_model val loss 1.4520 val acc 48.7013 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 970 fed_model val loss 1.4842 val acc 47.3011 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 980 fed_model val loss 1.4786 val acc 48.0317 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 990 fed_model val loss 1.4301 val acc 49.3912 best val_acc 50.933442 te_acc 50.669643\n",
      "New learnin rate  0.25\n",
      "trmean: at fang n_at 10 e 1000 fed_model val loss 1.4304 val acc 49.7362 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 1010 fed_model val loss 1.4188 val acc 49.7565 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 1020 fed_model val loss 1.4206 val acc 50.3653 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 1030 fed_model val loss 1.4357 val acc 50.1015 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 1110 fed_model val loss 1.4605 val acc 49.6753 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 1120 fed_model val loss 1.4671 val acc 49.4927 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 1130 fed_model val loss 1.4784 val acc 49.0463 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 1140 fed_model val loss 1.4725 val acc 48.7622 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 1150 fed_model val loss 1.4813 val acc 49.1274 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 1160 fed_model val loss 1.4740 val acc 49.7565 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 1170 fed_model val loss 1.4866 val acc 48.8636 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 1180 fed_model val loss 1.4746 val acc 49.5536 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 1190 fed_model val loss 1.4867 val acc 48.8636 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 1199 fed_model val loss 1.4922 val acc 48.8839 best val_acc 50.933442 te_acc 50.669643\n",
      "trmean: at fang n_at 10 e 1200 fed_model val loss 1.4978 val acc 49.1274 best val_acc 50.933442 te_acc 50.669643\n"
     ]
    }
   ],
   "source": [
    "batch_size=250\n",
    "resume=0\n",
    "nepochs=1200\n",
    "schedule=[1000]\n",
    "nbatches = user_tr_len//batch_size\n",
    "\n",
    "gamma=.5\n",
    "opt = 'sgd'\n",
    "fed_lr=0.5\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "aggregation='trmean'\n",
    "multi_k = False\n",
    "candidates = []\n",
    "\n",
    "at_type='fang'\n",
    "z_values=[0.0]\n",
    "n_attackers=[10]\n",
    "\n",
    "arch='alexnet'\n",
    "chkpt='./'+aggregation\n",
    "\n",
    "for n_attacker in n_attackers:\n",
    "    epoch_num = 0\n",
    "    best_global_acc = 0\n",
    "    best_global_te_acc = 0\n",
    "\n",
    "    for z in z_values:\n",
    "        fed_file='alexnet_checkpoint_%s_%s_%d_%.2f.pth.tar'%(aggregation,at_type,n_attacker,z)\n",
    "        fed_best_file='alexnet_best_%s_%s_%d_%.2f.pth.tar'%(aggregation,at_type,n_attacker,z)\n",
    "\n",
    "        if resume:\n",
    "            fed_checkpoint = chkpt+'/'+fed_file\n",
    "            assert os.path.isfile(fed_checkpoint), 'Error: no user checkpoint at %s'%(fed_checkpoint)\n",
    "            checkpoint = torch.load(fed_checkpoint, map_location='cuda:%d'%torch.cuda.current_device())\n",
    "            fed_model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer_fed.load_state_dict(checkpoint['optimizer'])\n",
    "            resume = 0\n",
    "            best_global_acc=checkpoint['best_acc']\n",
    "            best_global_te_acc=checkpoint['best_te_acc']\n",
    "            val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "            epoch_num += checkpoint['epoch']\n",
    "            print('resuming from epoch %d | val acc %.4f | best acc %.3f | best te acc %.3f'%(epoch_num, val_acc, best_global_acc, best_global_te_acc))\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        r=np.arange(user_tr_len)\n",
    "\n",
    "        fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
    "        optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "        while epoch_num <= nepochs:\n",
    "            user_grads=[]\n",
    "            if not epoch_num and epoch_num%nbatches == 0:\n",
    "                np.random.shuffle(r)\n",
    "                for i in range(nusers):\n",
    "                    user_tr_data_tensors[i]=user_tr_data_tensors[i][r]\n",
    "                    user_tr_label_tensors[i]=user_tr_label_tensors[i][r]\n",
    "\n",
    "            for i in range(n_attacker, nusers):\n",
    "\n",
    "                inputs = user_tr_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "                targets = user_tr_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "                outputs = fed_model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                fed_model.zero_grad()\n",
    "                loss.backward(retain_graph=True)\n",
    "\n",
    "                param_grad=[]\n",
    "                for param in fed_model.parameters():\n",
    "                    param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "                user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "\n",
    "            malicious_grads = user_grads\n",
    "\n",
    "            if epoch_num in schedule:\n",
    "                for param_group in optimizer_fed.param_groups:\n",
    "                    param_group['lr'] *= gamma\n",
    "                    print('New learnin rate ', param_group['lr'])\n",
    "\n",
    "            if n_attacker > 0:\n",
    "                if at_type == 'paf':\n",
    "                    malicious_grads=get_malicious_predictions_poison_all_far_sign(malicious_grads,nusers,n_attacker)\n",
    "                elif at_type == 'lie':\n",
    "                    malicious_grads = get_malicious_updates_lie(malicious_grads, n_attacker, z, epoch_num)\n",
    "                elif at_type == 'fang':\n",
    "                    agg_grads = torch.mean(malicious_grads, 0)\n",
    "                    deviation = torch.sign(agg_grads)\n",
    "                    malicious_grads = get_malicious_updates_fang_trmean(malicious_grads, deviation, n_attacker, epoch_num)\n",
    "                elif at_type == 'our':\n",
    "                    agg_grads = torch.mean(malicious_grads, 0)\n",
    "                    malicious_grads = our_attack_krum(malicious_grads, agg_grads, n_attacker, compression=compression, q_level=q_level, norm=norm)\n",
    "\n",
    "            if not epoch_num : \n",
    "                print(malicious_grads.shape)\n",
    "                \n",
    "            if aggregation=='median':\n",
    "                agg_grads=torch.median(malicious_grads,dim=0)[0]\n",
    "\n",
    "            elif aggregation=='average':\n",
    "                agg_grads=torch.mean(malicious_grads,dim=0)\n",
    "\n",
    "            elif aggregation=='trmean':\n",
    "                agg_grads=tr_mean(malicious_grads, n_attacker)\n",
    "\n",
    "            elif aggregation=='krum' or aggregation=='mkrum':\n",
    "                multi_k = True if aggregation == 'mkrum' else False\n",
    "                if epoch_num == 0: print('multi krum is ', multi_k)\n",
    "                agg_grads, krum_candidate = multi_krum(malicious_grads, n_attacker, multi_k=multi_k)\n",
    "                \n",
    "            elif aggregation=='bulyan':\n",
    "                agg_grads, krum_candidate=bulyan(malicious_grads, n_attacker)\n",
    "\n",
    "            del user_grads\n",
    "\n",
    "            start_idx=0\n",
    "\n",
    "            optimizer_fed.zero_grad()\n",
    "\n",
    "            model_grads=[]\n",
    "\n",
    "            for i, param in enumerate(fed_model.parameters()):\n",
    "                param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "                start_idx=start_idx+len(param.data.view(-1))\n",
    "                param_=param_.cuda()\n",
    "                model_grads.append(param_)\n",
    "\n",
    "            optimizer_fed.step(model_grads)\n",
    "\n",
    "            val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "            te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "            is_best = best_global_acc < val_acc\n",
    "\n",
    "            best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "            if is_best:\n",
    "                best_global_te_acc = te_acc\n",
    "\n",
    "            if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "                print('%s: at %s n_at %d e %d fed_model val loss %.4f val acc %.4f best val_acc %f te_acc %f'%(aggregation, at_type, n_attacker, epoch_num, val_loss, val_acc, best_global_acc,best_global_te_acc))\n",
    "\n",
    "            if val_loss > 10:\n",
    "                print('val loss %f too high'%val_loss)\n",
    "                break\n",
    "            \n",
    "            epoch_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for LIE attack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lie_attack(all_updates, z):\n",
    "    avg = torch.mean(all_updates, dim=0)\n",
    "    std = torch.std(all_updates, dim=0)\n",
    "    return avg + z * std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 2472266])\n",
      "trmean: at LIE n_at 10 e 0 fed_model val loss 2.3031 val acc 9.6794 best val_acc 9.679383 te_acc 10.064935\n",
      "trmean: at LIE n_at 10 e 10 fed_model val loss 2.2939 val acc 12.7029 best val_acc 13.879870 te_acc 14.103084\n",
      "trmean: at LIE n_at 10 e 20 fed_model val loss 2.2019 val acc 19.9269 best val_acc 22.321429 te_acc 22.828734\n",
      "trmean: at LIE n_at 10 e 30 fed_model val loss 2.2290 val acc 15.0365 best val_acc 22.321429 te_acc 22.828734\n",
      "trmean: at LIE n_at 10 e 40 fed_model val loss 2.1578 val acc 20.0284 best val_acc 22.706981 te_acc 22.646104\n",
      "trmean: at LIE n_at 10 e 50 fed_model val loss 2.1634 val acc 18.8312 best val_acc 24.249188 te_acc 23.457792\n",
      "trmean: at LIE n_at 10 e 60 fed_model val loss 2.0806 val acc 19.2573 best val_acc 24.249188 te_acc 23.457792\n",
      "trmean: at LIE n_at 10 e 70 fed_model val loss 2.2866 val acc 13.5552 best val_acc 24.249188 te_acc 23.457792\n",
      "trmean: at LIE n_at 10 e 80 fed_model val loss 2.2637 val acc 14.8539 best val_acc 24.249188 te_acc 23.457792\n",
      "trmean: at LIE n_at 10 e 90 fed_model val loss 2.1755 val acc 18.2427 best val_acc 24.249188 te_acc 23.457792\n",
      "trmean: at LIE n_at 10 e 100 fed_model val loss 2.1898 val acc 17.6136 best val_acc 26.643669 te_acc 25.872565\n",
      "trmean: at LIE n_at 10 e 110 fed_model val loss 2.0418 val acc 24.0260 best val_acc 26.643669 te_acc 25.872565\n",
      "trmean: at LIE n_at 10 e 120 fed_model val loss 2.1768 val acc 15.6047 best val_acc 26.643669 te_acc 25.872565\n",
      "trmean: at LIE n_at 10 e 130 fed_model val loss 2.0277 val acc 21.9156 best val_acc 26.643669 te_acc 25.872565\n",
      "trmean: at LIE n_at 10 e 140 fed_model val loss 1.9313 val acc 27.3336 best val_acc 27.333604 te_acc 27.435065\n",
      "trmean: at LIE n_at 10 e 150 fed_model val loss 1.9589 val acc 27.9221 best val_acc 27.922078 te_acc 27.617695\n",
      "trmean: at LIE n_at 10 e 160 fed_model val loss 2.0615 val acc 25.1623 best val_acc 31.452922 te_acc 30.336851\n",
      "trmean: at LIE n_at 10 e 170 fed_model val loss 1.7931 val acc 34.6185 best val_acc 34.618506 te_acc 32.690747\n",
      "trmean: at LIE n_at 10 e 180 fed_model val loss 1.9805 val acc 24.9594 best val_acc 34.618506 te_acc 32.690747\n",
      "trmean: at LIE n_at 10 e 190 fed_model val loss 1.9096 val acc 32.1429 best val_acc 35.511364 te_acc 35.592532\n",
      "trmean: at LIE n_at 10 e 200 fed_model val loss 1.8464 val acc 31.8588 best val_acc 35.511364 te_acc 35.592532\n",
      "trmean: at LIE n_at 10 e 210 fed_model val loss 1.8291 val acc 34.5170 best val_acc 35.511364 te_acc 35.592532\n",
      "trmean: at LIE n_at 10 e 220 fed_model val loss 1.8693 val acc 27.3336 best val_acc 35.511364 te_acc 35.592532\n",
      "trmean: at LIE n_at 10 e 230 fed_model val loss 1.7263 val acc 37.5812 best val_acc 38.311688 te_acc 38.230519\n",
      "trmean: at LIE n_at 10 e 240 fed_model val loss 1.6840 val acc 37.5406 best val_acc 39.082792 te_acc 39.143669\n",
      "trmean: at LIE n_at 10 e 250 fed_model val loss 1.7511 val acc 35.9375 best val_acc 40.807630 te_acc 40.361201\n",
      "trmean: at LIE n_at 10 e 260 fed_model val loss 1.6143 val acc 40.4018 best val_acc 40.807630 te_acc 40.361201\n",
      "trmean: at LIE n_at 10 e 270 fed_model val loss 1.6406 val acc 39.9148 best val_acc 40.807630 te_acc 40.361201\n",
      "trmean: at LIE n_at 10 e 280 fed_model val loss 1.6904 val acc 38.3929 best val_acc 42.065747 te_acc 41.700487\n",
      "trmean: at LIE n_at 10 e 290 fed_model val loss 1.6471 val acc 40.1177 best val_acc 42.065747 te_acc 41.700487\n",
      "trmean: at LIE n_at 10 e 300 fed_model val loss 1.6093 val acc 39.8336 best val_acc 42.857143 te_acc 42.370130\n",
      "trmean: at LIE n_at 10 e 310 fed_model val loss 1.6288 val acc 40.3409 best val_acc 42.857143 te_acc 42.370130\n",
      "trmean: at LIE n_at 10 e 320 fed_model val loss 1.5383 val acc 43.4862 best val_acc 43.486201 te_acc 43.039773\n",
      "trmean: at LIE n_at 10 e 330 fed_model val loss 1.5344 val acc 44.2167 best val_acc 45.738636 te_acc 44.338474\n",
      "trmean: at LIE n_at 10 e 340 fed_model val loss 1.9017 val acc 31.2906 best val_acc 45.738636 te_acc 44.338474\n",
      "trmean: at LIE n_at 10 e 350 fed_model val loss 1.7728 val acc 37.3985 best val_acc 45.738636 te_acc 44.338474\n",
      "trmean: at LIE n_at 10 e 360 fed_model val loss 1.6287 val acc 41.0917 best val_acc 45.738636 te_acc 44.338474\n",
      "trmean: at LIE n_at 10 e 370 fed_model val loss 1.4842 val acc 45.2313 best val_acc 45.738636 te_acc 44.338474\n",
      "trmean: at LIE n_at 10 e 380 fed_model val loss 1.4981 val acc 44.7646 best val_acc 46.367695 te_acc 46.732955\n",
      "trmean: at LIE n_at 10 e 390 fed_model val loss 1.5438 val acc 44.3588 best val_acc 46.367695 te_acc 46.732955\n",
      "trmean: at LIE n_at 10 e 400 fed_model val loss 1.4166 val acc 47.9911 best val_acc 48.944805 te_acc 49.370942\n",
      "trmean: at LIE n_at 10 e 410 fed_model val loss 1.6478 val acc 40.2800 best val_acc 48.944805 te_acc 49.370942\n",
      "trmean: at LIE n_at 10 e 420 fed_model val loss 1.5934 val acc 41.6193 best val_acc 48.944805 te_acc 49.370942\n",
      "trmean: at LIE n_at 10 e 430 fed_model val loss 1.4194 val acc 48.8839 best val_acc 50.202922 te_acc 49.776786\n",
      "trmean: at LIE n_at 10 e 440 fed_model val loss 1.4905 val acc 46.1851 best val_acc 51.075487 te_acc 50.365260\n",
      "trmean: at LIE n_at 10 e 450 fed_model val loss 1.4566 val acc 46.9359 best val_acc 51.075487 te_acc 50.365260\n",
      "trmean: at LIE n_at 10 e 460 fed_model val loss 1.3627 val acc 50.8117 best val_acc 51.075487 te_acc 50.365260\n",
      "trmean: at LIE n_at 10 e 470 fed_model val loss 1.3832 val acc 49.4318 best val_acc 51.075487 te_acc 50.365260\n",
      "trmean: at LIE n_at 10 e 480 fed_model val loss 1.3933 val acc 49.6144 best val_acc 51.075487 te_acc 50.365260\n",
      "trmean: at LIE n_at 10 e 490 fed_model val loss 1.4166 val acc 48.7419 best val_acc 52.110390 te_acc 52.313312\n",
      "trmean: at LIE n_at 10 e 500 fed_model val loss 1.3862 val acc 50.0000 best val_acc 52.881494 te_acc 52.739448\n",
      "trmean: at LIE n_at 10 e 510 fed_model val loss 1.5057 val acc 47.7679 best val_acc 52.881494 te_acc 52.739448\n",
      "trmean: at LIE n_at 10 e 520 fed_model val loss 1.4035 val acc 49.8782 best val_acc 52.881494 te_acc 52.739448\n",
      "trmean: at LIE n_at 10 e 530 fed_model val loss 1.3221 val acc 52.2321 best val_acc 53.551136 te_acc 53.754058\n",
      "trmean: at LIE n_at 10 e 540 fed_model val loss 1.3016 val acc 53.7338 best val_acc 53.733766 te_acc 54.200487\n",
      "trmean: at LIE n_at 10 e 550 fed_model val loss 1.4980 val acc 45.4748 best val_acc 53.733766 te_acc 54.200487\n",
      "trmean: at LIE n_at 10 e 560 fed_model val loss 1.2477 val acc 54.5860 best val_acc 55.783279 te_acc 56.960227\n",
      "trmean: at LIE n_at 10 e 570 fed_model val loss 1.2802 val acc 53.9976 best val_acc 55.783279 te_acc 56.960227\n",
      "trmean: at LIE n_at 10 e 580 fed_model val loss 1.3080 val acc 52.3336 best val_acc 55.783279 te_acc 56.960227\n",
      "trmean: at LIE n_at 10 e 590 fed_model val loss 1.5834 val acc 45.1907 best val_acc 55.783279 te_acc 56.960227\n",
      "trmean: at LIE n_at 10 e 600 fed_model val loss 1.2623 val acc 53.7744 best val_acc 55.783279 te_acc 56.960227\n",
      "trmean: at LIE n_at 10 e 610 fed_model val loss 1.3096 val acc 53.8352 best val_acc 56.655844 te_acc 56.452922\n",
      "trmean: at LIE n_at 10 e 620 fed_model val loss 1.4709 val acc 49.0057 best val_acc 56.655844 te_acc 56.452922\n",
      "trmean: at LIE n_at 10 e 630 fed_model val loss 1.3552 val acc 51.8263 best val_acc 56.757305 te_acc 56.960227\n",
      "trmean: at LIE n_at 10 e 640 fed_model val loss 1.3213 val acc 52.6786 best val_acc 56.757305 te_acc 56.960227\n",
      "trmean: at LIE n_at 10 e 650 fed_model val loss 1.2874 val acc 53.0032 best val_acc 57.366071 te_acc 57.873377\n",
      "trmean: at LIE n_at 10 e 660 fed_model val loss 1.6497 val acc 41.7005 best val_acc 57.366071 te_acc 57.873377\n",
      "trmean: at LIE n_at 10 e 670 fed_model val loss 1.3622 val acc 52.3742 best val_acc 57.366071 te_acc 57.873377\n",
      "trmean: at LIE n_at 10 e 680 fed_model val loss 1.3633 val acc 51.3393 best val_acc 57.548701 te_acc 57.954545\n",
      "trmean: at LIE n_at 10 e 690 fed_model val loss 1.2441 val acc 55.4180 best val_acc 58.015422 te_acc 57.771916\n",
      "trmean: at LIE n_at 10 e 700 fed_model val loss 1.2484 val acc 56.3718 best val_acc 58.015422 te_acc 57.771916\n",
      "trmean: at LIE n_at 10 e 710 fed_model val loss 1.4080 val acc 51.9075 best val_acc 58.015422 te_acc 57.771916\n",
      "trmean: at LIE n_at 10 e 720 fed_model val loss 1.2797 val acc 55.0122 best val_acc 58.015422 te_acc 57.771916\n",
      "trmean: at LIE n_at 10 e 730 fed_model val loss 1.2872 val acc 53.3279 best val_acc 58.015422 te_acc 57.771916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trmean: at LIE n_at 10 e 740 fed_model val loss 1.4977 val acc 48.9651 best val_acc 59.638799 te_acc 59.780844\n",
      "trmean: at LIE n_at 10 e 750 fed_model val loss 1.2149 val acc 56.7370 best val_acc 59.638799 te_acc 59.780844\n",
      "trmean: at LIE n_at 10 e 760 fed_model val loss 1.2588 val acc 55.3774 best val_acc 59.638799 te_acc 59.780844\n",
      "trmean: at LIE n_at 10 e 770 fed_model val loss 1.1938 val acc 57.2646 best val_acc 59.638799 te_acc 59.780844\n",
      "trmean: at LIE n_at 10 e 780 fed_model val loss 1.3489 val acc 52.6989 best val_acc 59.638799 te_acc 59.780844\n",
      "trmean: at LIE n_at 10 e 790 fed_model val loss 1.2654 val acc 57.1834 best val_acc 59.638799 te_acc 59.780844\n",
      "trmean: at LIE n_at 10 e 800 fed_model val loss 1.2386 val acc 57.0617 best val_acc 59.638799 te_acc 59.780844\n",
      "trmean: at LIE n_at 10 e 810 fed_model val loss 1.2901 val acc 55.4180 best val_acc 59.882305 te_acc 60.186688\n",
      "trmean: at LIE n_at 10 e 820 fed_model val loss 1.4314 val acc 51.3190 best val_acc 59.882305 te_acc 60.186688\n",
      "trmean: at LIE n_at 10 e 830 fed_model val loss 1.2602 val acc 57.1226 best val_acc 60.673701 te_acc 60.288149\n",
      "trmean: at LIE n_at 10 e 840 fed_model val loss 1.2434 val acc 58.6445 best val_acc 60.673701 te_acc 60.288149\n",
      "trmean: at LIE n_at 10 e 850 fed_model val loss 1.2215 val acc 57.2443 best val_acc 60.673701 te_acc 60.288149\n",
      "trmean: at LIE n_at 10 e 860 fed_model val loss 1.2801 val acc 56.6558 best val_acc 60.673701 te_acc 60.288149\n",
      "trmean: at LIE n_at 10 e 870 fed_model val loss 1.1586 val acc 59.7200 best val_acc 60.673701 te_acc 60.288149\n",
      "trmean: at LIE n_at 10 e 880 fed_model val loss 1.1923 val acc 58.0154 best val_acc 60.673701 te_acc 60.288149\n",
      "trmean: at LIE n_at 10 e 890 fed_model val loss 1.2555 val acc 56.7573 best val_acc 61.444805 te_acc 61.302760\n",
      "trmean: at LIE n_at 10 e 900 fed_model val loss 1.4025 val acc 54.2614 best val_acc 61.444805 te_acc 61.302760\n",
      "trmean: at LIE n_at 10 e 910 fed_model val loss 1.3639 val acc 54.9310 best val_acc 61.444805 te_acc 61.302760\n",
      "trmean: at LIE n_at 10 e 920 fed_model val loss 1.2839 val acc 55.8847 best val_acc 61.444805 te_acc 61.302760\n",
      "trmean: at LIE n_at 10 e 930 fed_model val loss 1.1612 val acc 60.6940 best val_acc 61.444805 te_acc 61.302760\n",
      "trmean: at LIE n_at 10 e 940 fed_model val loss 1.3224 val acc 54.0990 best val_acc 61.708604 te_acc 60.754870\n",
      "trmean: at LIE n_at 10 e 950 fed_model val loss 1.2341 val acc 58.4821 best val_acc 61.708604 te_acc 60.754870\n",
      "trmean: at LIE n_at 10 e 960 fed_model val loss 1.1375 val acc 62.0130 best val_acc 62.012987 te_acc 60.876623\n",
      "trmean: at LIE n_at 10 e 970 fed_model val loss 1.1831 val acc 60.1055 best val_acc 62.012987 te_acc 60.876623\n",
      "trmean: at LIE n_at 10 e 980 fed_model val loss 1.2748 val acc 57.4878 best val_acc 62.337662 te_acc 61.891234\n",
      "trmean: at LIE n_at 10 e 990 fed_model val loss 1.2346 val acc 60.6737 best val_acc 62.337662 te_acc 61.891234\n",
      "New learnin rate  0.25\n",
      "trmean: at LIE n_at 10 e 1000 fed_model val loss 1.0775 val acc 62.5203 best val_acc 62.520292 te_acc 63.149351\n",
      "trmean: at LIE n_at 10 e 1010 fed_model val loss 1.1064 val acc 64.6104 best val_acc 64.610390 te_acc 64.427760\n",
      "trmean: at LIE n_at 10 e 1020 fed_model val loss 1.1440 val acc 64.4886 best val_acc 64.691558 te_acc 64.366883\n",
      "trmean: at LIE n_at 10 e 1030 fed_model val loss 1.1948 val acc 63.5755 best val_acc 64.691558 te_acc 64.366883\n",
      "trmean: at LIE n_at 10 e 1040 fed_model val loss 1.2014 val acc 64.0016 best val_acc 64.691558 te_acc 64.366883\n",
      "trmean: at LIE n_at 10 e 1050 fed_model val loss 1.2265 val acc 63.3320 best val_acc 64.691558 te_acc 64.366883\n",
      "trmean: at LIE n_at 10 e 1060 fed_model val loss 1.2732 val acc 63.4131 best val_acc 64.691558 te_acc 64.366883\n",
      "trmean: at LIE n_at 10 e 1070 fed_model val loss 1.2187 val acc 63.7581 best val_acc 64.691558 te_acc 64.366883\n",
      "trmean: at LIE n_at 10 e 1080 fed_model val loss 1.2867 val acc 62.8247 best val_acc 64.691558 te_acc 64.366883\n",
      "trmean: at LIE n_at 10 e 1090 fed_model val loss 1.3045 val acc 62.4594 best val_acc 64.691558 te_acc 64.366883\n",
      "trmean: at LIE n_at 10 e 1100 fed_model val loss 1.2805 val acc 63.7175 best val_acc 64.691558 te_acc 64.366883\n",
      "trmean: at LIE n_at 10 e 1110 fed_model val loss 1.5077 val acc 60.3490 best val_acc 64.691558 te_acc 64.366883\n",
      "trmean: at LIE n_at 10 e 1120 fed_model val loss 1.3512 val acc 62.3985 best val_acc 64.691558 te_acc 64.366883\n",
      "trmean: at LIE n_at 10 e 1130 fed_model val loss 1.2956 val acc 63.7175 best val_acc 64.691558 te_acc 64.366883\n",
      "trmean: at LIE n_at 10 e 1140 fed_model val loss 1.6319 val acc 58.9894 best val_acc 64.691558 te_acc 64.366883\n",
      "trmean: at LIE n_at 10 e 1150 fed_model val loss 1.2483 val acc 63.7784 best val_acc 64.691558 te_acc 64.366883\n",
      "trmean: at LIE n_at 10 e 1160 fed_model val loss 1.4130 val acc 61.8304 best val_acc 64.691558 te_acc 64.366883\n",
      "trmean: at LIE n_at 10 e 1170 fed_model val loss 1.3528 val acc 63.7175 best val_acc 64.691558 te_acc 64.366883\n",
      "trmean: at LIE n_at 10 e 1180 fed_model val loss 1.4733 val acc 60.9172 best val_acc 64.691558 te_acc 64.366883\n",
      "trmean: at LIE n_at 10 e 1190 fed_model val loss 1.3873 val acc 63.2914 best val_acc 64.691558 te_acc 64.366883\n",
      "trmean: at LIE n_at 10 e 1199 fed_model val loss 1.4343 val acc 63.0682 best val_acc 64.691558 te_acc 64.366883\n",
      "trmean: at LIE n_at 10 e 1200 fed_model val loss 1.4795 val acc 62.6623 best val_acc 64.691558 te_acc 64.366883\n"
     ]
    }
   ],
   "source": [
    "batch_size=250\n",
    "resume=0\n",
    "nepochs=1200\n",
    "schedule=[1000]\n",
    "nbatches = user_tr_len//batch_size\n",
    "\n",
    "gamma=.5\n",
    "opt = 'sgd'\n",
    "fed_lr=0.5\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "aggregation='trmean'\n",
    "multi_k = False\n",
    "candidates = []\n",
    "\n",
    "at_type='LIE'\n",
    "z_values={3:0.69847, 5:0.7054, 8:0.71904, 10:0.72575, 12:0.73891}\n",
    "n_attackers=[10]\n",
    "\n",
    "arch='alexnet'\n",
    "chkpt='./'+aggregation\n",
    "\n",
    "for n_attacker in n_attackers:\n",
    "    epoch_num = 0\n",
    "    best_global_acc = 0\n",
    "    best_global_te_acc = 0\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    r=np.arange(user_tr_len)\n",
    "\n",
    "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
    "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        user_grads=[]\n",
    "        if not epoch_num and epoch_num%nbatches == 0:\n",
    "            np.random.shuffle(r)\n",
    "            for i in range(nusers):\n",
    "                user_tr_data_tensors[i]=user_tr_data_tensors[i][r]\n",
    "                user_tr_label_tensors[i]=user_tr_label_tensors[i][r]\n",
    "\n",
    "        for i in range(n_attacker, nusers):\n",
    "\n",
    "            inputs = user_tr_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "            targets = user_tr_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "            outputs = fed_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            fed_model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            param_grad=[]\n",
    "            for param in fed_model.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "\n",
    "        malicious_grads = user_grads\n",
    "\n",
    "        if epoch_num in schedule:\n",
    "            for param_group in optimizer_fed.param_groups:\n",
    "                param_group['lr'] *= gamma\n",
    "                print('New learnin rate ', param_group['lr'])\n",
    "\n",
    "        if n_attacker > 0:\n",
    "            if at_type == 'lie':\n",
    "                mal_update = lie_attack(malicious_grads, z_values[n_attacker])\n",
    "                malicious_grads = torch.cat((torch.stack([mal_update]*n_attacker), malicious_grads))\n",
    "            elif at_type == 'fang':\n",
    "                agg_grads = torch.mean(malicious_grads, 0)\n",
    "                deviation = torch.sign(agg_grads)\n",
    "                malicious_grads = get_malicious_updates_fang_trmean(malicious_grads, deviation, n_attacker, epoch_num)\n",
    "            elif at_type == 'our-agr':\n",
    "                agg_grads = torch.mean(malicious_grads, 0)\n",
    "                malicious_grads = our_attack_krum(malicious_grads, agg_grads, n_attacker, compression=compression, q_level=q_level, norm=norm)\n",
    "\n",
    "        if not epoch_num : \n",
    "            print(malicious_grads.shape)\n",
    "\n",
    "        if aggregation=='median':\n",
    "            agg_grads=torch.median(malicious_grads,dim=0)[0]\n",
    "\n",
    "        elif aggregation=='average':\n",
    "            agg_grads=torch.mean(malicious_grads,dim=0)\n",
    "\n",
    "        elif aggregation=='trmean':\n",
    "            agg_grads=tr_mean(malicious_grads, n_attacker)\n",
    "\n",
    "        elif aggregation=='krum' or aggregation=='mkrum':\n",
    "            multi_k = True if aggregation == 'mkrum' else False\n",
    "            if epoch_num == 0: print('multi krum is ', multi_k)\n",
    "            agg_grads, krum_candidate = multi_krum(malicious_grads, n_attacker, multi_k=multi_k)\n",
    "\n",
    "        elif aggregation=='bulyan':\n",
    "            agg_grads, krum_candidate=bulyan(malicious_grads, n_attacker)\n",
    "\n",
    "        del user_grads\n",
    "\n",
    "        start_idx=0\n",
    "\n",
    "        optimizer_fed.zero_grad()\n",
    "\n",
    "        model_grads=[]\n",
    "\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "\n",
    "        optimizer_fed.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if is_best:\n",
    "            best_global_te_acc = te_acc\n",
    "\n",
    "        if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "            print('%s: at %s n_at %d e %d fed_model val loss %.4f val acc %.4f best val_acc %f te_acc %f'%(aggregation, at_type, n_attacker, epoch_num, val_loss, val_acc, best_global_acc,best_global_te_acc))\n",
    "\n",
    "        if val_loss > 10:\n",
    "            print('val loss %f too high'%val_loss)\n",
    "            break\n",
    "\n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for our AGR-tailored attack on Trimmed-mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_attack_trmean(all_updates, model_re, n_attackers, dev_type='unit_vec'):\n",
    "\n",
    "    if dev_type == 'unit_vec':\n",
    "        deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "    elif dev_type == 'sign':\n",
    "        deviation = torch.sign(model_re)\n",
    "    elif dev_type == 'std':\n",
    "        deviation = torch.std(all_updates, 0)\n",
    "\n",
    "    lamda = torch.Tensor([10.0]).cuda() #compute_lambda_our(all_updates, model_re, n_attackers)\n",
    "    # print(lamda)\n",
    "    threshold_diff = 1e-5\n",
    "    prev_loss = -1\n",
    "    lamda_fail = lamda\n",
    "    lamda_succ = 0\n",
    "    iters = 0 \n",
    "    while torch.abs(lamda_succ - lamda) > threshold_diff:\n",
    "        mal_update = (model_re - lamda * deviation)\n",
    "        mal_updates = torch.stack([mal_update] * n_attackers)\n",
    "        mal_updates = torch.cat((mal_updates, all_updates), 0)\n",
    "\n",
    "        agg_grads = tr_mean(mal_updates, n_attackers)\n",
    "        \n",
    "        loss = torch.norm(agg_grads - model_re)\n",
    "        \n",
    "        if prev_loss < loss:\n",
    "            # print('successful lamda is ', lamda)\n",
    "            lamda_succ = lamda\n",
    "            lamda = lamda + lamda_fail / 2\n",
    "        else:\n",
    "            lamda = lamda - lamda_fail / 2\n",
    "\n",
    "        lamda_fail = lamda_fail / 2\n",
    "        prev_loss = loss\n",
    "        \n",
    "    mal_update = (model_re - lamda_succ * deviation)\n",
    "    mal_updates = torch.stack([mal_update] * n_attackers)\n",
    "    mal_updates = torch.cat((mal_updates, all_updates), 0)\n",
    "\n",
    "    return mal_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 2472266])\n",
      "trmean: at our-agr n_at 10 e 0 fed_model val loss 2.3025 val acc 9.6388 best val_acc 9.638799 te_acc 9.659091\n",
      "trmean: at our-agr n_at 10 e 10 fed_model val loss 2.2920 val acc 12.6623 best val_acc 12.662338 te_acc 13.413149\n",
      "trmean: at our-agr n_at 10 e 20 fed_model val loss 2.5542 val acc 12.9058 best val_acc 19.500812 te_acc 20.758929\n",
      "trmean: at our-agr n_at 10 e 30 fed_model val loss 2.2986 val acc 12.2362 best val_acc 19.500812 te_acc 20.758929\n",
      "trmean: at our-agr n_at 10 e 40 fed_model val loss 2.2566 val acc 17.8369 best val_acc 21.509740 te_acc 21.022727\n",
      "trmean: at our-agr n_at 10 e 50 fed_model val loss 2.2911 val acc 11.7695 best val_acc 21.509740 te_acc 21.022727\n",
      "trmean: at our-agr n_at 10 e 60 fed_model val loss 2.2098 val acc 16.8831 best val_acc 21.509740 te_acc 21.022727\n",
      "trmean: at our-agr n_at 10 e 70 fed_model val loss 2.3257 val acc 11.1810 best val_acc 21.509740 te_acc 21.022727\n",
      "trmean: at our-agr n_at 10 e 80 fed_model val loss 2.1418 val acc 21.1445 best val_acc 21.509740 te_acc 21.022727\n",
      "trmean: at our-agr n_at 10 e 90 fed_model val loss 2.3846 val acc 10.0041 best val_acc 22.625812 te_acc 21.976461\n",
      "trmean: at our-agr n_at 10 e 100 fed_model val loss 2.2688 val acc 11.9724 best val_acc 22.625812 te_acc 21.976461\n",
      "trmean: at our-agr n_at 10 e 110 fed_model val loss 2.2950 val acc 12.4391 best val_acc 22.625812 te_acc 21.976461\n",
      "trmean: at our-agr n_at 10 e 120 fed_model val loss 2.2216 val acc 16.8425 best val_acc 22.625812 te_acc 21.976461\n",
      "trmean: at our-agr n_at 10 e 130 fed_model val loss 2.2249 val acc 17.6745 best val_acc 22.625812 te_acc 21.976461\n",
      "trmean: at our-agr n_at 10 e 140 fed_model val loss 2.1564 val acc 18.6485 best val_acc 22.625812 te_acc 21.976461\n",
      "trmean: at our-agr n_at 10 e 150 fed_model val loss 2.2159 val acc 14.6510 best val_acc 22.625812 te_acc 21.976461\n",
      "trmean: at our-agr n_at 10 e 160 fed_model val loss 2.1992 val acc 18.3442 best val_acc 22.625812 te_acc 21.976461\n",
      "trmean: at our-agr n_at 10 e 170 fed_model val loss 2.2111 val acc 14.7727 best val_acc 22.625812 te_acc 21.976461\n",
      "trmean: at our-agr n_at 10 e 180 fed_model val loss 2.1744 val acc 16.3961 best val_acc 22.625812 te_acc 21.976461\n",
      "trmean: at our-agr n_at 10 e 190 fed_model val loss 2.2534 val acc 12.9870 best val_acc 22.625812 te_acc 21.976461\n",
      "trmean: at our-agr n_at 10 e 200 fed_model val loss 2.2120 val acc 17.7151 best val_acc 22.625812 te_acc 21.976461\n",
      "trmean: at our-agr n_at 10 e 210 fed_model val loss 2.1948 val acc 16.6802 best val_acc 22.625812 te_acc 21.976461\n",
      "trmean: at our-agr n_at 10 e 220 fed_model val loss 2.2673 val acc 14.6104 best val_acc 22.625812 te_acc 21.976461\n",
      "trmean: at our-agr n_at 10 e 230 fed_model val loss 2.2811 val acc 13.3929 best val_acc 22.625812 te_acc 21.976461\n",
      "trmean: at our-agr n_at 10 e 240 fed_model val loss 2.1716 val acc 15.8076 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 250 fed_model val loss 2.2157 val acc 13.9002 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 260 fed_model val loss 2.3371 val acc 9.9838 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 270 fed_model val loss 2.2527 val acc 11.2216 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 280 fed_model val loss 2.7984 val acc 11.7695 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 290 fed_model val loss 2.2154 val acc 14.6916 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 300 fed_model val loss 2.1314 val acc 17.7963 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 310 fed_model val loss 2.3319 val acc 9.5982 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 320 fed_model val loss 2.1892 val acc 15.8482 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 330 fed_model val loss 2.1420 val acc 17.8774 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 340 fed_model val loss 2.1380 val acc 18.6080 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 350 fed_model val loss 2.3492 val acc 9.9026 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 360 fed_model val loss 2.2603 val acc 14.5292 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 370 fed_model val loss 2.2126 val acc 13.6364 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 380 fed_model val loss 2.3587 val acc 8.1778 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 390 fed_model val loss 2.1250 val acc 21.1242 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 400 fed_model val loss 2.2666 val acc 15.8888 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 410 fed_model val loss 2.1884 val acc 18.0601 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 420 fed_model val loss 2.1735 val acc 16.1120 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 430 fed_model val loss 2.3536 val acc 9.6388 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 440 fed_model val loss 2.2690 val acc 10.4708 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 450 fed_model val loss 2.3131 val acc 9.7403 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 460 fed_model val loss 2.1762 val acc 16.4367 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 470 fed_model val loss 2.1752 val acc 17.1266 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 480 fed_model val loss 2.1691 val acc 16.3352 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 490 fed_model val loss 2.1612 val acc 17.6339 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 500 fed_model val loss 2.0985 val acc 21.8547 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 510 fed_model val loss 2.1955 val acc 15.2597 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 520 fed_model val loss 2.1584 val acc 16.2946 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 530 fed_model val loss 2.3725 val acc 9.9229 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 540 fed_model val loss 2.2816 val acc 14.3669 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 550 fed_model val loss 2.2998 val acc 11.4854 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 560 fed_model val loss 2.2867 val acc 15.3003 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 570 fed_model val loss 2.2114 val acc 16.4367 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 580 fed_model val loss 2.2772 val acc 13.9002 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 590 fed_model val loss 2.2694 val acc 12.9667 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 600 fed_model val loss 2.1833 val acc 16.0511 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 610 fed_model val loss 2.1910 val acc 15.6250 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 620 fed_model val loss 2.2271 val acc 15.0568 best val_acc 23.762175 te_acc 22.767857\n",
      "trmean: at our-agr n_at 10 e 630 fed_model val loss 2.2753 val acc 12.8653 best val_acc 23.762175 te_acc 22.767857\n",
      "val loss 1365899.336039 too high\n"
     ]
    }
   ],
   "source": [
    "batch_size=250\n",
    "resume=0\n",
    "nepochs=1200\n",
    "schedule=[1000]\n",
    "nbatches = user_tr_len//batch_size\n",
    "\n",
    "gamma=.5\n",
    "opt = 'sgd'\n",
    "fed_lr=0.5\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "aggregation='trmean'\n",
    "multi_k = False\n",
    "candidates = []\n",
    "\n",
    "at_type='our-agr'\n",
    "dev_type = 'std'\n",
    "z_values={3:0.69847, 5:0.7054, 8:0.71904, 10:0.72575, 12:0.73891}\n",
    "n_attackers=[10]\n",
    "\n",
    "arch='alexnet'\n",
    "chkpt='./'+aggregation\n",
    "\n",
    "for n_attacker in n_attackers:\n",
    "    epoch_num = 0\n",
    "    best_global_acc = 0\n",
    "    best_global_te_acc = 0\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    r=np.arange(user_tr_len)\n",
    "\n",
    "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
    "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        user_grads=[]\n",
    "        if not epoch_num and epoch_num%nbatches == 0:\n",
    "            np.random.shuffle(r)\n",
    "            for i in range(nusers):\n",
    "                user_tr_data_tensors[i]=user_tr_data_tensors[i][r]\n",
    "                user_tr_label_tensors[i]=user_tr_label_tensors[i][r]\n",
    "\n",
    "        for i in range(n_attacker, nusers):\n",
    "\n",
    "            inputs = user_tr_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "            targets = user_tr_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "            outputs = fed_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            fed_model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            param_grad=[]\n",
    "            for param in fed_model.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "\n",
    "        malicious_grads = user_grads\n",
    "\n",
    "        if epoch_num in schedule:\n",
    "            for param_group in optimizer_fed.param_groups:\n",
    "                param_group['lr'] *= gamma\n",
    "                print('New learnin rate ', param_group['lr'])\n",
    "\n",
    "        if n_attacker > 0:\n",
    "            if at_type == 'lie':\n",
    "                mal_update = lie_attack(malicious_grads, z_values[n_attacker])\n",
    "                malicious_grads = torch.cat((torch.stack([mal_update]*n_attacker), malicious_grads))\n",
    "            elif at_type == 'fang':\n",
    "                agg_grads = torch.mean(malicious_grads, 0)\n",
    "                deviation = torch.sign(agg_grads)\n",
    "                malicious_grads = get_malicious_updates_fang_trmean(malicious_grads, deviation, n_attacker, epoch_num)\n",
    "            elif at_type == 'our-agr':\n",
    "                agg_grads = torch.mean(malicious_grads, 0)\n",
    "                malicious_grads = our_attack_trmean(malicious_grads, agg_grads, n_attacker, dev_type=dev_type)\n",
    "\n",
    "        if not epoch_num : \n",
    "            print(malicious_grads.shape)\n",
    "\n",
    "        if aggregation=='median':\n",
    "            agg_grads=torch.median(malicious_grads,dim=0)[0]\n",
    "\n",
    "        elif aggregation=='average':\n",
    "            agg_grads=torch.mean(malicious_grads,dim=0)\n",
    "\n",
    "        elif aggregation=='trmean':\n",
    "            agg_grads=tr_mean(malicious_grads, n_attacker)\n",
    "\n",
    "        elif aggregation=='krum' or aggregation=='mkrum':\n",
    "            multi_k = True if aggregation == 'mkrum' else False\n",
    "            if epoch_num == 0: print('multi krum is ', multi_k)\n",
    "            agg_grads, krum_candidate = multi_krum(malicious_grads, n_attacker, multi_k=multi_k)\n",
    "\n",
    "        elif aggregation=='bulyan':\n",
    "            agg_grads, krum_candidate=bulyan(malicious_grads, n_attacker)\n",
    "\n",
    "        del user_grads\n",
    "\n",
    "        start_idx=0\n",
    "\n",
    "        optimizer_fed.zero_grad()\n",
    "\n",
    "        model_grads=[]\n",
    "\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "\n",
    "        optimizer_fed.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if is_best:\n",
    "            best_global_te_acc = te_acc\n",
    "\n",
    "        if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "            print('%s: at %s n_at %d e %d fed_model val loss %.4f val acc %.4f best val_acc %f te_acc %f'%(aggregation, at_type, n_attacker, epoch_num, val_loss, val_acc, best_global_acc,best_global_te_acc))\n",
    "\n",
    "        if val_loss > 1000:\n",
    "            print('val loss %f too high'%val_loss)\n",
    "            break\n",
    "\n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for our first AGR-agnostic attack - Min-max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MIN-MAX attack\n",
    "'''\n",
    "def our_attack_dist(all_updates, model_re, n_attackers, dev_type='unit_vec'):\n",
    "\n",
    "    if dev_type == 'unit_vec':\n",
    "        deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "    elif dev_type == 'sign':\n",
    "        deviation = torch.sign(model_re)\n",
    "    elif dev_type == 'std':\n",
    "        deviation = torch.std(all_updates, 0)\n",
    "\n",
    "    lamda = torch.Tensor([10.0]).float().cuda()\n",
    "    # print(lamda)\n",
    "    threshold_diff = 1e-5\n",
    "    lamda_fail = lamda\n",
    "    lamda_succ = 0\n",
    "    \n",
    "    distances = []\n",
    "    for update in all_updates:\n",
    "        distance = torch.norm((all_updates - update), dim=1) ** 2\n",
    "        distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)\n",
    "    \n",
    "    max_distance = torch.max(distances)\n",
    "    del distances\n",
    "\n",
    "    while torch.abs(lamda_succ - lamda) > threshold_diff:\n",
    "        mal_update = (model_re - lamda * deviation)\n",
    "        distance = torch.norm((all_updates - mal_update), dim=1) ** 2\n",
    "        max_d = torch.max(distance)\n",
    "        \n",
    "        if max_d <= max_distance:\n",
    "            # print('successful lamda is ', lamda)\n",
    "            lamda_succ = lamda\n",
    "            lamda = lamda + lamda_fail / 2\n",
    "        else:\n",
    "            lamda = lamda - lamda_fail / 2\n",
    "\n",
    "        lamda_fail = lamda_fail / 2\n",
    "\n",
    "    mal_update = (model_re - lamda_succ * deviation)\n",
    "    \n",
    "    return mal_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malicious_grads shape  torch.Size([50, 2472266])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vshejwalkar/fed-quant-robustness/code/cifar10/sgd.py:108: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729138878/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  p.data.add_(-group['lr'], d_p)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trmean: at min-max n_at 10 | e 0 fed_model val loss 2.3025 val acc 9.9229 best val_acc 9.922890 te_acc 9.435877\n",
      "trmean: at min-max n_at 10 | e 10 fed_model val loss 2.2891 val acc 10.8969 best val_acc 16.152597 te_acc 15.909091\n",
      "trmean: at min-max n_at 10 | e 20 fed_model val loss 2.3295 val acc 9.9432 best val_acc 20.474838 te_acc 20.474838\n",
      "trmean: at min-max n_at 10 | e 30 fed_model val loss 2.2610 val acc 10.1461 best val_acc 20.474838 te_acc 20.474838\n",
      "trmean: at min-max n_at 10 | e 40 fed_model val loss 2.2000 val acc 18.0804 best val_acc 20.474838 te_acc 20.474838\n",
      "trmean: at min-max n_at 10 | e 50 fed_model val loss 2.2149 val acc 17.0455 best val_acc 20.474838 te_acc 20.474838\n",
      "trmean: at min-max n_at 10 | e 60 fed_model val loss 2.1762 val acc 19.9472 best val_acc 21.509740 te_acc 21.205357\n",
      "trmean: at min-max n_at 10 | e 70 fed_model val loss 2.2078 val acc 16.0714 best val_acc 21.509740 te_acc 21.205357\n",
      "trmean: at min-max n_at 10 | e 80 fed_model val loss 2.3448 val acc 9.3953 best val_acc 23.965097 te_acc 23.660714\n",
      "trmean: at min-max n_at 10 | e 90 fed_model val loss 2.3395 val acc 10.3896 best val_acc 23.965097 te_acc 23.660714\n",
      "trmean: at min-max n_at 10 | e 100 fed_model val loss 2.0735 val acc 22.5244 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 110 fed_model val loss 2.2885 val acc 13.6567 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 120 fed_model val loss 2.2042 val acc 18.5268 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 130 fed_model val loss 2.2942 val acc 8.5227 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 140 fed_model val loss 2.0937 val acc 20.3734 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 150 fed_model val loss 2.1852 val acc 15.9497 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 160 fed_model val loss 3.2767 val acc 14.3669 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 170 fed_model val loss 2.3075 val acc 9.9026 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 180 fed_model val loss 2.3109 val acc 9.9026 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 190 fed_model val loss 2.2707 val acc 11.8912 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 200 fed_model val loss 2.1873 val acc 14.7321 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 210 fed_model val loss 2.3580 val acc 12.7232 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 220 fed_model val loss 2.2426 val acc 16.1120 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 230 fed_model val loss 2.2804 val acc 13.5958 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 240 fed_model val loss 2.2161 val acc 16.0308 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 250 fed_model val loss 2.0792 val acc 18.3239 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 260 fed_model val loss 2.1610 val acc 15.6250 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 270 fed_model val loss 2.4323 val acc 14.8133 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 280 fed_model val loss 2.4265 val acc 13.7784 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 290 fed_model val loss 2.1874 val acc 16.4773 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 300 fed_model val loss 2.1454 val acc 18.2021 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 310 fed_model val loss 2.3023 val acc 10.0446 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 320 fed_model val loss 2.3308 val acc 11.6883 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 330 fed_model val loss 2.2341 val acc 16.4773 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 340 fed_model val loss 2.2069 val acc 18.8718 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 350 fed_model val loss 2.0844 val acc 18.5877 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 360 fed_model val loss 2.1401 val acc 17.9180 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 370 fed_model val loss 2.2204 val acc 11.7492 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 380 fed_model val loss 2.3437 val acc 13.4334 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 390 fed_model val loss 2.2577 val acc 10.1055 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 400 fed_model val loss 2.1566 val acc 17.9992 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 410 fed_model val loss 2.2086 val acc 18.0601 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 420 fed_model val loss 2.3026 val acc 10.3287 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 430 fed_model val loss 2.6291 val acc 11.4448 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 440 fed_model val loss 2.2396 val acc 17.7151 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 450 fed_model val loss 2.1880 val acc 17.9383 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 460 fed_model val loss 2.0979 val acc 19.2167 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 470 fed_model val loss 2.2550 val acc 16.1323 best val_acc 24.188312 te_acc 22.970779\n",
      "trmean: at min-max n_at 10 | e 480 fed_model val loss 2.3566 val acc 14.1843 best val_acc 25.040584 te_acc 24.634740\n",
      "trmean: at min-max n_at 10 | e 490 fed_model val loss 2.1513 val acc 19.3385 best val_acc 25.040584 te_acc 24.634740\n",
      "trmean: at min-max n_at 10 | e 500 fed_model val loss 2.1148 val acc 17.6745 best val_acc 25.040584 te_acc 24.634740\n",
      "trmean: at min-max n_at 10 | e 510 fed_model val loss 2.0420 val acc 20.1502 best val_acc 25.040584 te_acc 24.634740\n",
      "trmean: at min-max n_at 10 | e 520 fed_model val loss 2.5591 val acc 14.5292 best val_acc 27.800325 te_acc 26.948052\n",
      "trmean: at min-max n_at 10 | e 530 fed_model val loss 2.2343 val acc 18.1006 best val_acc 27.800325 te_acc 26.948052\n",
      "trmean: at min-max n_at 10 | e 540 fed_model val loss 2.5892 val acc 14.7321 best val_acc 27.800325 te_acc 26.948052\n",
      "trmean: at min-max n_at 10 | e 550 fed_model val loss 2.0657 val acc 20.2313 best val_acc 27.800325 te_acc 26.948052\n",
      "trmean: at min-max n_at 10 | e 560 fed_model val loss 2.0613 val acc 19.8052 best val_acc 27.800325 te_acc 26.948052\n",
      "trmean: at min-max n_at 10 | e 570 fed_model val loss 1.9926 val acc 17.7354 best val_acc 27.800325 te_acc 26.948052\n",
      "trmean: at min-max n_at 10 | e 580 fed_model val loss 2.0449 val acc 21.7938 best val_acc 27.800325 te_acc 26.948052\n",
      "trmean: at min-max n_at 10 | e 590 fed_model val loss 2.1033 val acc 23.2143 best val_acc 29.180195 te_acc 28.409091\n",
      "trmean: at min-max n_at 10 | e 600 fed_model val loss 2.0080 val acc 20.9619 best val_acc 29.180195 te_acc 28.409091\n",
      "trmean: at min-max n_at 10 | e 610 fed_model val loss 2.0668 val acc 20.8807 best val_acc 29.180195 te_acc 28.409091\n",
      "trmean: at min-max n_at 10 | e 620 fed_model val loss 1.9857 val acc 22.9708 best val_acc 29.180195 te_acc 28.409091\n",
      "trmean: at min-max n_at 10 | e 630 fed_model val loss 2.1758 val acc 17.6339 best val_acc 29.180195 te_acc 28.409091\n",
      "trmean: at min-max n_at 10 | e 640 fed_model val loss 2.0802 val acc 23.3969 best val_acc 29.180195 te_acc 28.409091\n",
      "trmean: at min-max n_at 10 | e 650 fed_model val loss 1.9510 val acc 26.1364 best val_acc 29.180195 te_acc 28.409091\n",
      "trmean: at min-max n_at 10 | e 660 fed_model val loss 1.9389 val acc 24.9797 best val_acc 29.586039 te_acc 29.606331\n",
      "trmean: at min-max n_at 10 | e 670 fed_model val loss 2.4515 val acc 17.2078 best val_acc 29.586039 te_acc 29.606331\n",
      "trmean: at min-max n_at 10 | e 680 fed_model val loss 2.0044 val acc 22.6055 best val_acc 29.586039 te_acc 29.606331\n",
      "trmean: at min-max n_at 10 | e 690 fed_model val loss 1.9415 val acc 26.9278 best val_acc 29.586039 te_acc 29.606331\n",
      "trmean: at min-max n_at 10 | e 700 fed_model val loss 1.9169 val acc 27.5974 best val_acc 29.586039 te_acc 29.606331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trmean: at min-max n_at 10 | e 710 fed_model val loss 1.9609 val acc 28.6526 best val_acc 30.641234 te_acc 28.774351\n",
      "trmean: at min-max n_at 10 | e 720 fed_model val loss 1.9986 val acc 23.2955 best val_acc 30.641234 te_acc 28.774351\n",
      "trmean: at min-max n_at 10 | e 730 fed_model val loss 2.1788 val acc 21.4083 best val_acc 30.641234 te_acc 28.774351\n",
      "trmean: at min-max n_at 10 | e 740 fed_model val loss 2.1673 val acc 18.7906 best val_acc 30.641234 te_acc 28.774351\n",
      "trmean: at min-max n_at 10 | e 750 fed_model val loss 2.1418 val acc 23.2752 best val_acc 32.021104 te_acc 31.554383\n",
      "trmean: at min-max n_at 10 | e 760 fed_model val loss 2.5925 val acc 18.1006 best val_acc 32.609578 te_acc 32.974838\n",
      "trmean: at min-max n_at 10 | e 770 fed_model val loss 1.8563 val acc 30.3977 best val_acc 32.609578 te_acc 32.974838\n",
      "trmean: at min-max n_at 10 | e 780 fed_model val loss 2.1515 val acc 21.1039 best val_acc 32.609578 te_acc 32.974838\n",
      "trmean: at min-max n_at 10 | e 790 fed_model val loss 1.9447 val acc 26.7654 best val_acc 32.609578 te_acc 32.974838\n",
      "trmean: at min-max n_at 10 | e 800 fed_model val loss 1.8921 val acc 28.6526 best val_acc 32.609578 te_acc 32.974838\n",
      "trmean: at min-max n_at 10 | e 810 fed_model val loss 1.9820 val acc 27.2119 best val_acc 32.609578 te_acc 32.974838\n",
      "trmean: at min-max n_at 10 | e 820 fed_model val loss 2.3487 val acc 15.6859 best val_acc 33.218344 te_acc 33.137175\n",
      "trmean: at min-max n_at 10 | e 830 fed_model val loss 2.3560 val acc 10.8360 best val_acc 33.218344 te_acc 33.137175\n",
      "trmean: at min-max n_at 10 | e 840 fed_model val loss 2.8933 val acc 9.9026 best val_acc 33.218344 te_acc 33.137175\n",
      "trmean: at min-max n_at 10 | e 850 fed_model val loss 2.1637 val acc 17.8166 best val_acc 33.218344 te_acc 33.137175\n",
      "trmean: at min-max n_at 10 | e 860 fed_model val loss 2.1481 val acc 16.9440 best val_acc 33.218344 te_acc 33.137175\n",
      "trmean: at min-max n_at 10 | e 870 fed_model val loss 2.2445 val acc 17.1266 best val_acc 33.218344 te_acc 33.137175\n",
      "trmean: at min-max n_at 10 | e 880 fed_model val loss 1.9178 val acc 26.9075 best val_acc 33.218344 te_acc 33.137175\n",
      "trmean: at min-max n_at 10 | e 890 fed_model val loss 2.2485 val acc 17.8166 best val_acc 33.218344 te_acc 33.137175\n",
      "trmean: at min-max n_at 10 | e 900 fed_model val loss 1.9610 val acc 25.3247 best val_acc 33.218344 te_acc 33.137175\n",
      "trmean: at min-max n_at 10 | e 910 fed_model val loss 1.9970 val acc 23.0519 best val_acc 33.218344 te_acc 33.137175\n",
      "trmean: at min-max n_at 10 | e 920 fed_model val loss 2.0648 val acc 27.5771 best val_acc 33.218344 te_acc 33.137175\n",
      "trmean: at min-max n_at 10 | e 930 fed_model val loss 2.0983 val acc 21.0836 best val_acc 33.218344 te_acc 33.137175\n",
      "trmean: at min-max n_at 10 | e 940 fed_model val loss 2.0151 val acc 20.0487 best val_acc 33.218344 te_acc 33.137175\n",
      "trmean: at min-max n_at 10 | e 950 fed_model val loss 1.9564 val acc 25.3653 best val_acc 33.218344 te_acc 33.137175\n",
      "trmean: at min-max n_at 10 | e 960 fed_model val loss 2.0225 val acc 24.9188 best val_acc 33.218344 te_acc 33.137175\n",
      "trmean: at min-max n_at 10 | e 970 fed_model val loss 2.1812 val acc 17.8166 best val_acc 33.218344 te_acc 33.137175\n",
      "trmean: at min-max n_at 10 | e 980 fed_model val loss 2.2306 val acc 22.0373 best val_acc 33.319805 te_acc 31.412338\n",
      "trmean: at min-max n_at 10 | e 990 fed_model val loss 2.2616 val acc 16.3961 best val_acc 33.319805 te_acc 31.412338\n",
      "New learnin rate  0.25\n",
      "trmean: at min-max n_at 10 | e 1000 fed_model val loss 2.1295 val acc 19.5617 best val_acc 33.319805 te_acc 31.412338\n",
      "trmean: at min-max n_at 10 | e 1010 fed_model val loss 2.1029 val acc 23.2346 best val_acc 33.319805 te_acc 31.412338\n",
      "trmean: at min-max n_at 10 | e 1020 fed_model val loss 1.8480 val acc 28.4294 best val_acc 33.319805 te_acc 31.412338\n",
      "trmean: at min-max n_at 10 | e 1030 fed_model val loss 1.9541 val acc 25.5073 best val_acc 33.603896 te_acc 31.594968\n",
      "trmean: at min-max n_at 10 | e 1040 fed_model val loss 1.9577 val acc 29.5252 best val_acc 36.627435 te_acc 35.734578\n",
      "trmean: at min-max n_at 10 | e 1050 fed_model val loss 1.8802 val acc 30.7427 best val_acc 36.627435 te_acc 35.734578\n",
      "trmean: at min-max n_at 10 | e 1060 fed_model val loss 1.8874 val acc 27.2321 best val_acc 37.297078 te_acc 36.404221\n",
      "trmean: at min-max n_at 10 | e 1070 fed_model val loss 1.8386 val acc 31.5747 best val_acc 37.297078 te_acc 36.404221\n",
      "trmean: at min-max n_at 10 | e 1080 fed_model val loss 1.9529 val acc 29.4846 best val_acc 37.297078 te_acc 36.404221\n",
      "trmean: at min-max n_at 10 | e 1090 fed_model val loss 2.2299 val acc 26.3393 best val_acc 37.297078 te_acc 36.404221\n",
      "trmean: at min-max n_at 10 | e 1100 fed_model val loss 1.7197 val acc 35.5925 best val_acc 37.297078 te_acc 36.404221\n",
      "trmean: at min-max n_at 10 | e 1110 fed_model val loss 1.8129 val acc 30.7021 best val_acc 37.297078 te_acc 36.404221\n",
      "trmean: at min-max n_at 10 | e 1120 fed_model val loss 1.8849 val acc 27.5568 best val_acc 37.297078 te_acc 36.404221\n",
      "trmean: at min-max n_at 10 | e 1130 fed_model val loss 1.8771 val acc 31.7979 best val_acc 37.662338 te_acc 37.094156\n",
      "trmean: at min-max n_at 10 | e 1140 fed_model val loss 2.6680 val acc 23.2752 best val_acc 37.662338 te_acc 37.094156\n",
      "trmean: at min-max n_at 10 | e 1150 fed_model val loss 1.9662 val acc 26.9886 best val_acc 37.662338 te_acc 37.094156\n",
      "trmean: at min-max n_at 10 | e 1160 fed_model val loss 2.2589 val acc 22.0170 best val_acc 37.662338 te_acc 37.094156\n",
      "trmean: at min-max n_at 10 | e 1170 fed_model val loss 2.1154 val acc 25.6696 best val_acc 37.662338 te_acc 37.094156\n",
      "trmean: at min-max n_at 10 | e 1180 fed_model val loss 2.0741 val acc 20.3531 best val_acc 37.662338 te_acc 37.094156\n",
      "trmean: at min-max n_at 10 | e 1190 fed_model val loss 1.9147 val acc 30.8239 best val_acc 37.662338 te_acc 37.094156\n",
      "trmean: at min-max n_at 10 | e 1199 fed_model val loss 1.7844 val acc 32.6299 best val_acc 37.662338 te_acc 37.094156\n",
      "trmean: at min-max n_at 10 | e 1200 fed_model val loss 1.8448 val acc 31.5950 best val_acc 37.662338 te_acc 37.094156\n"
     ]
    }
   ],
   "source": [
    "batch_size=250\n",
    "resume=0\n",
    "nepochs=1200\n",
    "schedule=[1000]\n",
    "nbatches = user_tr_len//batch_size\n",
    "\n",
    "gamma=.5\n",
    "opt = 'sgd'\n",
    "fed_lr=0.5\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "aggregation='trmean'\n",
    "multi_k = False\n",
    "candidates = []\n",
    "\n",
    "at_type='min-max'\n",
    "dev_type ='std'\n",
    "z=0\n",
    "n_attackers=[10]\n",
    "\n",
    "arch='alexnet'\n",
    "chkpt='./'+aggregation\n",
    "\n",
    "for n_attacker in n_attackers:\n",
    "    candidates = []\n",
    "\n",
    "    epoch_num = 0\n",
    "    best_global_acc = 0\n",
    "    best_global_te_acc = 0\n",
    "\n",
    "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
    "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    r=np.arange(user_tr_len)\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        user_grads=[]\n",
    "        if not epoch_num and epoch_num%nbatches == 0:\n",
    "            np.random.shuffle(r)\n",
    "            for i in range(nusers):\n",
    "                user_tr_data_tensors[i]=user_tr_data_tensors[i][r]\n",
    "                user_tr_label_tensors[i]=user_tr_label_tensors[i][r]\n",
    "\n",
    "        for i in range(n_attacker, nusers):\n",
    "\n",
    "            inputs = user_tr_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "            targets = user_tr_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "            outputs = fed_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            fed_model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            param_grad=[]\n",
    "            for param in fed_model.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "\n",
    "        malicious_grads = user_grads\n",
    "\n",
    "        if epoch_num in schedule:\n",
    "            for param_group in optimizer_fed.param_groups:\n",
    "                param_group['lr'] *= gamma\n",
    "                print('New learnin rate ', param_group['lr'])\n",
    "\n",
    "        if n_attacker > 0:\n",
    "            if at_type == 'lie':\n",
    "                malicious_grads = get_malicious_updates_lie(malicious_grads, n_attacker, z, epoch_num)\n",
    "            elif at_type == 'fang':\n",
    "                agg_grads = torch.mean(malicious_grads, 0)\n",
    "                deviation = torch.sign(agg_grads)\n",
    "                malicious_grads = get_malicious_updates_fang(malicious_grads, agg_grads, deviation, n_attacker)\n",
    "            elif at_type == 'our-agr':\n",
    "                agg_grads = torch.mean(malicious_grads, 0)\n",
    "                mal_update = our_attack_median(malicious_grads, agg_grads, n_attacker, dev_type)\n",
    "            elif at_type == 'min-max':\n",
    "                agg_grads = torch.mean(malicious_grads, 0)\n",
    "                mal_update = our_attack_dist(malicious_grads, agg_grads, n_attacker, dev_type)\n",
    "            elif at_type == 'min-sum':\n",
    "                agg_grads = torch.mean(malicious_grads, 0)\n",
    "                mal_update = our_attack_score(malicious_grads, agg_grads, n_attacker, dev_type)\n",
    "\n",
    "            mal_updates = torch.stack([mal_update] * n_attacker)\n",
    "            malicious_grads = torch.cat((mal_updates, user_grads), 0)\n",
    "\n",
    "        if epoch_num==0: print('malicious_grads shape ', malicious_grads.shape)\n",
    "\n",
    "        if aggregation=='median':\n",
    "            agg_grads=torch.median(malicious_grads,dim=0)[0]\n",
    "\n",
    "        elif aggregation=='trmean':\n",
    "            agg_grads=tr_mean(malicious_grads, n_attacker)\n",
    "            \n",
    "        elif aggregation=='average':\n",
    "            agg_grads=torch.mean(malicious_grads,dim=0)\n",
    "\n",
    "        elif aggregation=='krum' or aggregation=='mkrum':\n",
    "            multi_k = True if aggregation == 'mkrum' else False\n",
    "            if epoch_num == 0: print('multi krum is ', multi_k)\n",
    "            agg_grads, krum_candidate = multi_krum(malicious_grads, n_attacker, multi_k=multi_k)\n",
    "\n",
    "        elif aggregation=='bulyan':\n",
    "            agg_grads,bulyan_candidate=bulyan(malicious_grads, n_attacker)\n",
    "\n",
    "        del user_grads\n",
    "\n",
    "        start_idx=0\n",
    "\n",
    "        optimizer_fed.zero_grad()\n",
    "\n",
    "        model_grads=[]\n",
    "\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "\n",
    "        optimizer_fed.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if is_best:\n",
    "            best_global_te_acc = te_acc\n",
    "\n",
    "        if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "            print('%s: at %s n_at %d | e %d fed_model val loss %.4f val acc %.4f best val_acc %f te_acc %f'%(aggregation, at_type, n_attacker, epoch_num, val_loss, val_acc, best_global_acc,best_global_te_acc))\n",
    "\n",
    "        if val_loss > 1000:\n",
    "            print('val loss %f too high'%val_loss)\n",
    "            break\n",
    "            \n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for our second AGR-agnostic attack - Min-sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MIN-SUM attack\n",
    "'''\n",
    "\n",
    "def our_attack_score(all_updates, model_re, n_attackers, dev_type='unit_vec'):\n",
    "\n",
    "    if dev_type == 'unit_vec':\n",
    "        deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "    elif dev_type == 'sign':\n",
    "        deviation = torch.sign(model_re)\n",
    "    elif dev_type == 'std':\n",
    "        deviation = torch.std(all_updates, 0)\n",
    "    \n",
    "    lamda = torch.Tensor([10.0]).float().cuda()\n",
    "    # print(lamda)\n",
    "    threshold_diff = 1e-5\n",
    "    lamda_fail = lamda\n",
    "    lamda_succ = 0\n",
    "    \n",
    "    distances = []\n",
    "    for update in all_updates:\n",
    "        distance = torch.norm((all_updates - update), dim=1) ** 2\n",
    "        distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)\n",
    "    \n",
    "    scores = torch.sum(distances, dim=1)\n",
    "    min_score = torch.min(scores)\n",
    "    del distances\n",
    "\n",
    "    while torch.abs(lamda_succ - lamda) > threshold_diff:\n",
    "        mal_update = (model_re - lamda * deviation)\n",
    "        distance = torch.norm((all_updates - mal_update), dim=1) ** 2\n",
    "        score = torch.sum(distance)\n",
    "        \n",
    "        if score <= min_score:\n",
    "            # print('successful lamda is ', lamda)\n",
    "            lamda_succ = lamda\n",
    "            lamda = lamda + lamda_fail / 2\n",
    "        else:\n",
    "            lamda = lamda - lamda_fail / 2\n",
    "\n",
    "        lamda_fail = lamda_fail / 2\n",
    "\n",
    "    # print(lamda_succ)\n",
    "    mal_update = (model_re - lamda_succ * deviation)\n",
    "    \n",
    "    return mal_update\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malicious_grads shape  torch.Size([50, 2472266])\n",
      "trmean: at min-sum n_at 10 | e 0 fed_model val loss 2.3028 val acc 10.0649 best val_acc 10.064935 te_acc 9.780844\n",
      "trmean: at min-sum n_at 10 | e 10 fed_model val loss 2.2909 val acc 13.1494 best val_acc 14.671266 te_acc 15.056818\n",
      "trmean: at min-sum n_at 10 | e 20 fed_model val loss 2.6275 val acc 11.5463 best val_acc 20.271916 te_acc 21.144481\n",
      "trmean: at min-sum n_at 10 | e 30 fed_model val loss 2.2922 val acc 12.5609 best val_acc 20.271916 te_acc 21.144481\n",
      "trmean: at min-sum n_at 10 | e 40 fed_model val loss 2.1474 val acc 24.2695 best val_acc 24.269481 te_acc 23.538961\n",
      "trmean: at min-sum n_at 10 | e 50 fed_model val loss 2.2958 val acc 11.7695 best val_acc 24.269481 te_acc 23.538961\n",
      "trmean: at min-sum n_at 10 | e 60 fed_model val loss 2.2730 val acc 11.3636 best val_acc 24.269481 te_acc 23.538961\n",
      "trmean: at min-sum n_at 10 | e 70 fed_model val loss 2.1983 val acc 19.2573 best val_acc 24.269481 te_acc 23.538961\n",
      "trmean: at min-sum n_at 10 | e 80 fed_model val loss 2.2778 val acc 11.8912 best val_acc 24.269481 te_acc 23.538961\n",
      "trmean: at min-sum n_at 10 | e 90 fed_model val loss 2.1438 val acc 20.0690 best val_acc 24.269481 te_acc 23.538961\n",
      "trmean: at min-sum n_at 10 | e 100 fed_model val loss 2.2945 val acc 17.1266 best val_acc 26.034903 te_acc 25.020292\n",
      "trmean: at min-sum n_at 10 | e 110 fed_model val loss 2.3042 val acc 9.9026 best val_acc 26.034903 te_acc 25.020292\n",
      "trmean: at min-sum n_at 10 | e 120 fed_model val loss 2.2988 val acc 10.4099 best val_acc 26.034903 te_acc 25.020292\n",
      "trmean: at min-sum n_at 10 | e 130 fed_model val loss 2.2672 val acc 15.4424 best val_acc 26.034903 te_acc 25.020292\n",
      "trmean: at min-sum n_at 10 | e 140 fed_model val loss 2.3059 val acc 11.4042 best val_acc 26.034903 te_acc 25.020292\n",
      "trmean: at min-sum n_at 10 | e 150 fed_model val loss 2.2446 val acc 13.5552 best val_acc 26.034903 te_acc 25.020292\n",
      "trmean: at min-sum n_at 10 | e 160 fed_model val loss 2.3045 val acc 9.7606 best val_acc 26.034903 te_acc 25.020292\n",
      "trmean: at min-sum n_at 10 | e 170 fed_model val loss 2.1171 val acc 19.2573 best val_acc 26.034903 te_acc 25.020292\n",
      "trmean: at min-sum n_at 10 | e 180 fed_model val loss 2.0991 val acc 18.8718 best val_acc 26.034903 te_acc 25.020292\n",
      "trmean: at min-sum n_at 10 | e 190 fed_model val loss 2.0887 val acc 17.2890 best val_acc 26.034903 te_acc 25.020292\n",
      "trmean: at min-sum n_at 10 | e 200 fed_model val loss 2.0909 val acc 16.4773 best val_acc 26.034903 te_acc 25.020292\n",
      "trmean: at min-sum n_at 10 | e 210 fed_model val loss 2.0159 val acc 19.7443 best val_acc 26.034903 te_acc 25.020292\n",
      "trmean: at min-sum n_at 10 | e 220 fed_model val loss 2.0633 val acc 21.0227 best val_acc 26.034903 te_acc 25.020292\n",
      "trmean: at min-sum n_at 10 | e 230 fed_model val loss 2.1747 val acc 18.9529 best val_acc 26.034903 te_acc 25.020292\n",
      "trmean: at min-sum n_at 10 | e 240 fed_model val loss 2.1504 val acc 16.0511 best val_acc 26.034903 te_acc 25.020292\n",
      "trmean: at min-sum n_at 10 | e 250 fed_model val loss 2.1555 val acc 16.9643 best val_acc 26.034903 te_acc 25.020292\n",
      "trmean: at min-sum n_at 10 | e 260 fed_model val loss 2.0664 val acc 19.1761 best val_acc 26.034903 te_acc 25.020292\n",
      "trmean: at min-sum n_at 10 | e 270 fed_model val loss 2.1204 val acc 19.2776 best val_acc 26.034903 te_acc 25.020292\n",
      "trmean: at min-sum n_at 10 | e 280 fed_model val loss 1.9281 val acc 25.6899 best val_acc 26.034903 te_acc 25.020292\n",
      "trmean: at min-sum n_at 10 | e 290 fed_model val loss 2.0709 val acc 24.2898 best val_acc 26.034903 te_acc 25.020292\n",
      "trmean: at min-sum n_at 10 | e 300 fed_model val loss 2.0976 val acc 19.3385 best val_acc 26.866883 te_acc 25.953734\n",
      "trmean: at min-sum n_at 10 | e 310 fed_model val loss 1.9964 val acc 21.6924 best val_acc 26.866883 te_acc 25.953734\n",
      "trmean: at min-sum n_at 10 | e 320 fed_model val loss 1.9188 val acc 24.8377 best val_acc 26.866883 te_acc 25.953734\n",
      "trmean: at min-sum n_at 10 | e 330 fed_model val loss 1.8556 val acc 27.8409 best val_acc 28.733766 te_acc 28.632305\n",
      "trmean: at min-sum n_at 10 | e 340 fed_model val loss 1.9342 val acc 25.0203 best val_acc 28.733766 te_acc 28.632305\n",
      "trmean: at min-sum n_at 10 | e 350 fed_model val loss 2.0564 val acc 22.8693 best val_acc 28.733766 te_acc 28.632305\n",
      "trmean: at min-sum n_at 10 | e 360 fed_model val loss 1.9312 val acc 25.4058 best val_acc 28.733766 te_acc 28.632305\n",
      "trmean: at min-sum n_at 10 | e 370 fed_model val loss 2.0117 val acc 23.1331 best val_acc 31.107955 te_acc 29.687500\n",
      "trmean: at min-sum n_at 10 | e 380 fed_model val loss 1.8610 val acc 30.1339 best val_acc 31.107955 te_acc 29.687500\n",
      "trmean: at min-sum n_at 10 | e 390 fed_model val loss 2.0664 val acc 24.2086 best val_acc 31.107955 te_acc 29.687500\n",
      "trmean: at min-sum n_at 10 | e 400 fed_model val loss 1.9053 val acc 29.0787 best val_acc 31.534091 te_acc 31.189123\n",
      "trmean: at min-sum n_at 10 | e 410 fed_model val loss 2.1032 val acc 21.2256 best val_acc 31.534091 te_acc 31.189123\n",
      "trmean: at min-sum n_at 10 | e 420 fed_model val loss 2.1142 val acc 15.3612 best val_acc 31.534091 te_acc 31.189123\n",
      "trmean: at min-sum n_at 10 | e 430 fed_model val loss 2.0435 val acc 21.5503 best val_acc 31.534091 te_acc 31.189123\n",
      "trmean: at min-sum n_at 10 | e 440 fed_model val loss 1.9028 val acc 28.2873 best val_acc 31.534091 te_acc 31.189123\n",
      "trmean: at min-sum n_at 10 | e 450 fed_model val loss 1.9438 val acc 25.5073 best val_acc 31.534091 te_acc 31.189123\n",
      "trmean: at min-sum n_at 10 | e 460 fed_model val loss 1.9755 val acc 25.5073 best val_acc 31.534091 te_acc 31.189123\n",
      "trmean: at min-sum n_at 10 | e 470 fed_model val loss 1.8758 val acc 27.7192 best val_acc 31.534091 te_acc 31.189123\n",
      "trmean: at min-sum n_at 10 | e 480 fed_model val loss 1.7971 val acc 31.1485 best val_acc 31.534091 te_acc 31.189123\n",
      "trmean: at min-sum n_at 10 | e 490 fed_model val loss 1.9520 val acc 22.9099 best val_acc 31.534091 te_acc 31.189123\n",
      "trmean: at min-sum n_at 10 | e 500 fed_model val loss 1.8219 val acc 30.9659 best val_acc 31.534091 te_acc 31.189123\n",
      "trmean: at min-sum n_at 10 | e 510 fed_model val loss 2.0115 val acc 26.4813 best val_acc 31.534091 te_acc 31.189123\n",
      "trmean: at min-sum n_at 10 | e 520 fed_model val loss 1.9227 val acc 24.3506 best val_acc 31.534091 te_acc 31.189123\n",
      "trmean: at min-sum n_at 10 | e 530 fed_model val loss 2.0075 val acc 24.3912 best val_acc 35.024351 te_acc 34.273539\n",
      "trmean: at min-sum n_at 10 | e 540 fed_model val loss 1.9908 val acc 23.5187 best val_acc 35.024351 te_acc 34.273539\n",
      "trmean: at min-sum n_at 10 | e 550 fed_model val loss 1.8855 val acc 26.9075 best val_acc 35.024351 te_acc 34.273539\n",
      "trmean: at min-sum n_at 10 | e 560 fed_model val loss 1.8655 val acc 29.6672 best val_acc 35.024351 te_acc 34.273539\n",
      "trmean: at min-sum n_at 10 | e 570 fed_model val loss 1.8966 val acc 26.2987 best val_acc 35.024351 te_acc 34.273539\n",
      "trmean: at min-sum n_at 10 | e 580 fed_model val loss 1.9265 val acc 27.1104 best val_acc 36.870942 te_acc 36.688312\n",
      "trmean: at min-sum n_at 10 | e 590 fed_model val loss 2.0150 val acc 25.1420 best val_acc 36.870942 te_acc 36.688312\n",
      "trmean: at min-sum n_at 10 | e 600 fed_model val loss 1.6839 val acc 38.3726 best val_acc 38.372565 te_acc 38.636364\n",
      "trmean: at min-sum n_at 10 | e 610 fed_model val loss 1.8268 val acc 31.1282 best val_acc 38.372565 te_acc 38.636364\n",
      "trmean: at min-sum n_at 10 | e 620 fed_model val loss 1.6817 val acc 37.0942 best val_acc 38.372565 te_acc 38.636364\n",
      "trmean: at min-sum n_at 10 | e 630 fed_model val loss 1.7229 val acc 33.3807 best val_acc 38.372565 te_acc 38.636364\n",
      "trmean: at min-sum n_at 10 | e 640 fed_model val loss 1.9722 val acc 24.8985 best val_acc 38.372565 te_acc 38.636364\n",
      "trmean: at min-sum n_at 10 | e 650 fed_model val loss 2.0039 val acc 24.4318 best val_acc 38.372565 te_acc 38.636364\n",
      "trmean: at min-sum n_at 10 | e 660 fed_model val loss 1.8163 val acc 31.7979 best val_acc 38.372565 te_acc 38.636364\n",
      "trmean: at min-sum n_at 10 | e 670 fed_model val loss 2.0161 val acc 27.6989 best val_acc 38.372565 te_acc 38.636364\n",
      "trmean: at min-sum n_at 10 | e 680 fed_model val loss 1.7342 val acc 33.7459 best val_acc 38.372565 te_acc 38.636364\n",
      "trmean: at min-sum n_at 10 | e 690 fed_model val loss 1.8248 val acc 33.0357 best val_acc 38.372565 te_acc 38.636364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trmean: at min-sum n_at 10 | e 700 fed_model val loss 2.4183 val acc 19.9878 best val_acc 38.372565 te_acc 38.636364\n",
      "trmean: at min-sum n_at 10 | e 710 fed_model val loss 2.0139 val acc 27.5568 best val_acc 38.372565 te_acc 38.636364\n",
      "trmean: at min-sum n_at 10 | e 720 fed_model val loss 1.8418 val acc 31.1688 best val_acc 38.372565 te_acc 38.636364\n",
      "trmean: at min-sum n_at 10 | e 730 fed_model val loss 1.7134 val acc 37.7435 best val_acc 40.036526 te_acc 39.285714\n",
      "trmean: at min-sum n_at 10 | e 740 fed_model val loss 2.1116 val acc 21.8953 best val_acc 40.036526 te_acc 39.285714\n",
      "trmean: at min-sum n_at 10 | e 750 fed_model val loss 1.8158 val acc 30.3166 best val_acc 40.036526 te_acc 39.285714\n",
      "trmean: at min-sum n_at 10 | e 760 fed_model val loss 1.8922 val acc 29.5657 best val_acc 40.036526 te_acc 39.285714\n",
      "trmean: at min-sum n_at 10 | e 770 fed_model val loss 1.6282 val acc 40.9700 best val_acc 40.969968 te_acc 41.355519\n",
      "trmean: at min-sum n_at 10 | e 780 fed_model val loss 1.5809 val acc 43.0804 best val_acc 43.080357 te_acc 42.715097\n",
      "trmean: at min-sum n_at 10 | e 790 fed_model val loss 1545.6694 val acc 10.5925 best val_acc 43.080357 te_acc 42.715097\n",
      "val loss 1545.669423 too high\n"
     ]
    }
   ],
   "source": [
    "batch_size=250\n",
    "resume=0\n",
    "nepochs=1200\n",
    "schedule=[1000]\n",
    "nbatches = user_tr_len//batch_size\n",
    "\n",
    "gamma=.5\n",
    "opt = 'sgd'\n",
    "fed_lr=0.5\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "aggregation='trmean'\n",
    "multi_k = False\n",
    "candidates = []\n",
    "\n",
    "at_type='min-sum'\n",
    "dev_type ='std'\n",
    "z=0\n",
    "n_attackers=[10]\n",
    "\n",
    "arch='alexnet'\n",
    "chkpt='./'+aggregation\n",
    "\n",
    "for n_attacker in n_attackers:\n",
    "    candidates = []\n",
    "\n",
    "    epoch_num = 0\n",
    "    best_global_acc = 0\n",
    "    best_global_te_acc = 0\n",
    "\n",
    "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
    "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    r=np.arange(user_tr_len)\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        user_grads=[]\n",
    "        if not epoch_num and epoch_num%nbatches == 0:\n",
    "            np.random.shuffle(r)\n",
    "            for i in range(nusers):\n",
    "                user_tr_data_tensors[i]=user_tr_data_tensors[i][r]\n",
    "                user_tr_label_tensors[i]=user_tr_label_tensors[i][r]\n",
    "\n",
    "        for i in range(n_attacker, nusers):\n",
    "\n",
    "            inputs = user_tr_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "            targets = user_tr_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "            outputs = fed_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            fed_model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            param_grad=[]\n",
    "            for param in fed_model.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "\n",
    "        malicious_grads = user_grads\n",
    "\n",
    "        if epoch_num in schedule:\n",
    "            for param_group in optimizer_fed.param_groups:\n",
    "                param_group['lr'] *= gamma\n",
    "                print('New learnin rate ', param_group['lr'])\n",
    "\n",
    "        if n_attacker > 0:\n",
    "            if at_type == 'lie':\n",
    "                malicious_grads = get_malicious_updates_lie(malicious_grads, n_attacker, z, epoch_num)\n",
    "            elif at_type == 'fang':\n",
    "                agg_grads = torch.mean(malicious_grads, 0)\n",
    "                deviation = torch.sign(agg_grads)\n",
    "                malicious_grads = get_malicious_updates_fang(malicious_grads, agg_grads, deviation, n_attacker)\n",
    "            elif at_type == 'our-agr':\n",
    "                agg_grads = torch.mean(malicious_grads, 0)\n",
    "                mal_update = our_attack_median(malicious_grads, agg_grads, n_attacker, dev_type)\n",
    "            elif at_type == 'min-max':\n",
    "                agg_grads = torch.mean(malicious_grads, 0)\n",
    "                mal_update = our_attack_dist(malicious_grads, agg_grads, n_attacker, dev_type)\n",
    "            elif at_type == 'min-sum':\n",
    "                agg_grads = torch.mean(malicious_grads, 0)\n",
    "                mal_update = our_attack_score(malicious_grads, agg_grads, n_attacker, dev_type)\n",
    "\n",
    "            mal_updates = torch.stack([mal_update] * n_attacker)\n",
    "            malicious_grads = torch.cat((mal_updates, user_grads), 0)\n",
    "\n",
    "        if epoch_num==0: print('malicious_grads shape ', malicious_grads.shape)\n",
    "\n",
    "        if aggregation=='median':\n",
    "            agg_grads=torch.median(malicious_grads,dim=0)[0]\n",
    "\n",
    "        elif aggregation=='trmean':\n",
    "            agg_grads=tr_mean(malicious_grads, n_attacker)\n",
    "            \n",
    "        elif aggregation=='average':\n",
    "            agg_grads=torch.mean(malicious_grads,dim=0)\n",
    "\n",
    "        elif aggregation=='krum' or aggregation=='mkrum':\n",
    "            multi_k = True if aggregation == 'mkrum' else False\n",
    "            if epoch_num == 0: print('multi krum is ', multi_k)\n",
    "            agg_grads, krum_candidate = multi_krum(malicious_grads, n_attacker, multi_k=multi_k)\n",
    "\n",
    "        elif aggregation=='bulyan':\n",
    "            agg_grads,bulyan_candidate=bulyan(malicious_grads, n_attacker)\n",
    "\n",
    "        del user_grads\n",
    "\n",
    "        start_idx=0\n",
    "\n",
    "        optimizer_fed.zero_grad()\n",
    "\n",
    "        model_grads=[]\n",
    "\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "\n",
    "        optimizer_fed.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if is_best:\n",
    "            best_global_te_acc = te_acc\n",
    "\n",
    "        if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "            print('%s: at %s n_at %d | e %d fed_model val loss %.4f val acc %.4f best val_acc %f te_acc %f'%(aggregation, at_type, n_attacker, epoch_num, val_loss, val_acc, best_global_acc,best_global_te_acc))\n",
    "\n",
    "        if val_loss > 1000:\n",
    "            print('val loss %f too high'%val_loss)\n",
    "            break\n",
    "            \n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
