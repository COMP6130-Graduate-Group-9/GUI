{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The notebook contains\n",
    "### Code for _Median_ aggregation algorithm\n",
    "### Evaluation of all of the attacks (Fang, LIE, and our SOTA AGR-tailored and AGR-agnstic) on Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse, os, sys, csv, shutil, time, random, operator, pickle, ast, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.optim import Optimizer\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "sys.path.insert(0,'./../utils/')\n",
    "from logger import *\n",
    "from eval import *\n",
    "from misc import *\n",
    "\n",
    "from cifar10_normal_train import *\n",
    "from cifar10_util import *\n",
    "from adam import Adam\n",
    "from sgd import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get CIFAR10 data and split it in IID fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "data_loc='/mnt/nfs/work1/amir/vshejwalkar/cifar10_data/'\n",
    "# load the train dataset\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=train_transform)\n",
    "\n",
    "cifar10_test = datasets.CIFAR10(root=data_loc, train=False, download=True, transform=train_transform)\n",
    "\n",
    "X=[]\n",
    "Y=[]\n",
    "for i in range(len(cifar10_train)):\n",
    "    X.append(cifar10_train[i][0].numpy())\n",
    "    Y.append(cifar10_train[i][1])\n",
    "\n",
    "for i in range(len(cifar10_test)):\n",
    "    X.append(cifar10_test[i][0].numpy())\n",
    "    Y.append(cifar10_test[i][1])\n",
    "\n",
    "X=np.array(X)\n",
    "Y=np.array(Y)\n",
    "\n",
    "print('total data len: ',len(X))\n",
    "\n",
    "if not os.path.isfile('./cifar10_shuffle.pkl'):\n",
    "    all_indices = np.arange(len(X))\n",
    "    np.random.shuffle(all_indices)\n",
    "    pickle.dump(all_indices,open('./cifar10_shuffle.pkl','wb'))\n",
    "else:\n",
    "    all_indices=pickle.load(open('./cifar10_shuffle.pkl','rb'))\n",
    "\n",
    "X=X[all_indices]\n",
    "Y=Y[all_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data len:  60000\n",
      "total tr len 50000 | val len 5000 | test len 5000\n",
      "user 0 tr len 1000\n",
      "user 1 tr len 1000\n",
      "user 2 tr len 1000\n",
      "user 3 tr len 1000\n",
      "user 4 tr len 1000\n",
      "user 5 tr len 1000\n",
      "user 6 tr len 1000\n",
      "user 7 tr len 1000\n",
      "user 8 tr len 1000\n",
      "user 9 tr len 1000\n",
      "user 10 tr len 1000\n",
      "user 11 tr len 1000\n",
      "user 12 tr len 1000\n",
      "user 13 tr len 1000\n",
      "user 14 tr len 1000\n",
      "user 15 tr len 1000\n",
      "user 16 tr len 1000\n",
      "user 17 tr len 1000\n",
      "user 18 tr len 1000\n",
      "user 19 tr len 1000\n",
      "user 20 tr len 1000\n",
      "user 21 tr len 1000\n",
      "user 22 tr len 1000\n",
      "user 23 tr len 1000\n",
      "user 24 tr len 1000\n",
      "user 25 tr len 1000\n",
      "user 26 tr len 1000\n",
      "user 27 tr len 1000\n",
      "user 28 tr len 1000\n",
      "user 29 tr len 1000\n",
      "user 30 tr len 1000\n",
      "user 31 tr len 1000\n",
      "user 32 tr len 1000\n",
      "user 33 tr len 1000\n",
      "user 34 tr len 1000\n",
      "user 35 tr len 1000\n",
      "user 36 tr len 1000\n",
      "user 37 tr len 1000\n",
      "user 38 tr len 1000\n",
      "user 39 tr len 1000\n",
      "user 40 tr len 1000\n",
      "user 41 tr len 1000\n",
      "user 42 tr len 1000\n",
      "user 43 tr len 1000\n",
      "user 44 tr len 1000\n",
      "user 45 tr len 1000\n",
      "user 46 tr len 1000\n",
      "user 47 tr len 1000\n",
      "user 48 tr len 1000\n",
      "user 49 tr len 1000\n"
     ]
    }
   ],
   "source": [
    "# data loading\n",
    "\n",
    "nusers=50\n",
    "user_tr_len=1000\n",
    "\n",
    "total_tr_len=user_tr_len*nusers\n",
    "val_len=5000\n",
    "te_len=5000\n",
    "\n",
    "print('total data len: ',len(X))\n",
    "\n",
    "if not os.path.isfile('./cifar10_shuffle.pkl'):\n",
    "    all_indices = np.arange(len(X))\n",
    "    np.random.shuffle(all_indices)\n",
    "    pickle.dump(all_indices,open('./cifar10_shuffle.pkl','wb'))\n",
    "else:\n",
    "    all_indices=pickle.load(open('./cifar10_shuffle.pkl','rb'))\n",
    "\n",
    "total_tr_data=X[:total_tr_len]\n",
    "total_tr_label=Y[:total_tr_len]\n",
    "\n",
    "val_data=X[total_tr_len:(total_tr_len+val_len)]\n",
    "val_label=Y[total_tr_len:(total_tr_len+val_len)]\n",
    "\n",
    "te_data=X[(total_tr_len+val_len):(total_tr_len+val_len+te_len)]\n",
    "te_label=Y[(total_tr_len+val_len):(total_tr_len+val_len+te_len)]\n",
    "\n",
    "total_tr_data_tensor=torch.from_numpy(total_tr_data).type(torch.FloatTensor)\n",
    "total_tr_label_tensor=torch.from_numpy(total_tr_label).type(torch.LongTensor)\n",
    "\n",
    "val_data_tensor=torch.from_numpy(val_data).type(torch.FloatTensor)\n",
    "val_label_tensor=torch.from_numpy(val_label).type(torch.LongTensor)\n",
    "\n",
    "te_data_tensor=torch.from_numpy(te_data).type(torch.FloatTensor)\n",
    "te_label_tensor=torch.from_numpy(te_label).type(torch.LongTensor)\n",
    "\n",
    "print('total tr len %d | val len %d | test len %d'%(len(total_tr_data_tensor),len(val_data_tensor),len(te_data_tensor)))\n",
    "\n",
    "#==============================================================================================================\n",
    "\n",
    "user_tr_data_tensors=[]\n",
    "user_tr_label_tensors=[]\n",
    "\n",
    "for i in range(nusers):\n",
    "    \n",
    "    user_tr_data_tensor=torch.from_numpy(total_tr_data[user_tr_len*i:user_tr_len*(i+1)]).type(torch.FloatTensor)\n",
    "    user_tr_label_tensor=torch.from_numpy(total_tr_label[user_tr_len*i:user_tr_len*(i+1)]).type(torch.LongTensor)\n",
    "\n",
    "    user_tr_data_tensors.append(user_tr_data_tensor)\n",
    "    user_tr_label_tensors.append(user_tr_label_tensor)\n",
    "    print('user %d tr len %d'%(i,len(user_tr_data_tensor)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full knowledge Fang attack on median aggregation\n",
    "### Note that Fang attacks on Trimmed-mean and median are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_malicious_updates_fang_trmean(all_updates, deviation, n_attackers, epoch_num, compression='none', q_level=2, norm='inf'):\n",
    "    b = 2\n",
    "    max_vector = torch.max(all_updates, 0)[0]\n",
    "    min_vector = torch.min(all_updates, 0)[0]\n",
    "\n",
    "    max_ = (max_vector > 0).type(torch.FloatTensor).cuda()\n",
    "    min_ = (min_vector < 0).type(torch.FloatTensor).cuda()\n",
    "\n",
    "    max_[max_ == 1] = b\n",
    "    max_[max_ == 0] = 1 / b\n",
    "    min_[min_ == 1] = b\n",
    "    min_[min_ == 0] = 1 / b\n",
    "\n",
    "    max_range = torch.cat((max_vector[:, None], (max_vector * max_)[:, None]), dim=1)\n",
    "    min_range = torch.cat(((min_vector * min_)[:, None], min_vector[:, None]), dim=1)\n",
    "\n",
    "    rand = torch.from_numpy(np.random.uniform(0, 1, [len(deviation), n_attackers])).type(torch.FloatTensor).cuda()\n",
    "\n",
    "    max_rand = torch.stack([max_range[:, 0]] * rand.shape[1]).T + rand * torch.stack([max_range[:, 1] - max_range[:, 0]] * rand.shape[1]).T\n",
    "    min_rand = torch.stack([min_range[:, 0]] * rand.shape[1]).T + rand * torch.stack([min_range[:, 1] - min_range[:, 0]] * rand.shape[1]).T\n",
    "\n",
    "    mal_vec = (torch.stack([(deviation > 0).type(torch.FloatTensor)] * max_rand.shape[1]).T.cuda() * max_rand + torch.stack(\n",
    "        [(deviation > 0).type(torch.FloatTensor)] * min_rand.shape[1]).T.cuda() * min_rand).T\n",
    "\n",
    "    quant_mal_vec = []\n",
    "    if compression != 'none':\n",
    "        if epoch_num == 0: print('compressing malicious update')\n",
    "        for i in range(mal_vec.shape[0]):\n",
    "            mal_ = mal_vec[i]\n",
    "            mal_quant = qsgd(mal_, s=q_level, norm=norm)\n",
    "            quant_mal_vec = mal_quant[None, :] if not len(quant_mal_vec) else torch.cat((quant_mal_vec, mal_quant[None, :]), 0)\n",
    "    else:\n",
    "        quant_mal_vec = mal_vec\n",
    "\n",
    "    mal_updates = torch.cat((quant_mal_vec, all_updates), 0)\n",
    "\n",
    "    return mal_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 2472266])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vshejwalkar/fed-quant-robustness/code/cifar10/sgd.py:108: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729138878/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  p.data.add_(-group['lr'], d_p)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median: at fang n_at 10 e 0 fed_model val loss 2.3026 val acc 10.5925 best val_acc 10.592532 te_acc 10.146104\n",
      "median: at fang n_at 10 e 10 fed_model val loss 2.2991 val acc 16.3149 best val_acc 16.375812 te_acc 16.923701\n",
      "median: at fang n_at 10 e 20 fed_model val loss 2.2894 val acc 19.4602 best val_acc 20.860390 te_acc 21.124188\n",
      "median: at fang n_at 10 e 30 fed_model val loss 2.3053 val acc 10.6128 best val_acc 20.880682 te_acc 21.245942\n",
      "median: at fang n_at 10 e 40 fed_model val loss 3.7148 val acc 9.9026 best val_acc 23.072240 te_acc 23.092532\n",
      "median: at fang n_at 10 e 50 fed_model val loss 2.3023 val acc 10.4708 best val_acc 23.072240 te_acc 23.092532\n",
      "median: at fang n_at 10 e 60 fed_model val loss 2.2964 val acc 9.2938 best val_acc 23.072240 te_acc 23.092532\n",
      "median: at fang n_at 10 e 70 fed_model val loss 2.2697 val acc 12.4391 best val_acc 23.072240 te_acc 23.092532\n",
      "median: at fang n_at 10 e 80 fed_model val loss 2.2708 val acc 14.7119 best val_acc 23.072240 te_acc 23.092532\n",
      "median: at fang n_at 10 e 90 fed_model val loss 2.2806 val acc 11.5260 best val_acc 23.072240 te_acc 23.092532\n",
      "median: at fang n_at 10 e 100 fed_model val loss 2.2526 val acc 14.8742 best val_acc 23.072240 te_acc 23.092532\n",
      "median: at fang n_at 10 e 110 fed_model val loss 2.1671 val acc 20.5560 best val_acc 23.072240 te_acc 23.092532\n",
      "median: at fang n_at 10 e 120 fed_model val loss 2.1276 val acc 21.9765 best val_acc 24.492695 te_acc 23.254870\n",
      "median: at fang n_at 10 e 130 fed_model val loss 2.0827 val acc 20.9619 best val_acc 24.492695 te_acc 23.254870\n",
      "median: at fang n_at 10 e 140 fed_model val loss 2.1424 val acc 18.0601 best val_acc 25.081169 te_acc 23.518669\n",
      "median: at fang n_at 10 e 150 fed_model val loss 1.9846 val acc 25.9943 best val_acc 25.994318 te_acc 23.782468\n",
      "median: at fang n_at 10 e 160 fed_model val loss 1.9714 val acc 23.9245 best val_acc 25.994318 te_acc 23.782468\n",
      "median: at fang n_at 10 e 170 fed_model val loss 2.0479 val acc 22.3823 best val_acc 25.994318 te_acc 23.782468\n",
      "median: at fang n_at 10 e 180 fed_model val loss 2.0765 val acc 17.2890 best val_acc 28.165584 te_acc 26.927760\n",
      "median: at fang n_at 10 e 190 fed_model val loss 2.1643 val acc 16.3149 best val_acc 29.058442 te_acc 27.252435\n",
      "median: at fang n_at 10 e 200 fed_model val loss 1.9746 val acc 28.2468 best val_acc 29.586039 te_acc 28.875812\n",
      "median: at fang n_at 10 e 210 fed_model val loss 1.8795 val acc 30.6006 best val_acc 31.351461 te_acc 31.960227\n",
      "median: at fang n_at 10 e 220 fed_model val loss 1.8667 val acc 30.7224 best val_acc 31.351461 te_acc 31.960227\n",
      "median: at fang n_at 10 e 230 fed_model val loss 1.8127 val acc 32.6705 best val_acc 33.664773 te_acc 32.406656\n",
      "median: at fang n_at 10 e 240 fed_model val loss 1.7937 val acc 33.0154 best val_acc 35.024351 te_acc 34.415584\n",
      "median: at fang n_at 10 e 250 fed_model val loss 1.9023 val acc 29.7687 best val_acc 35.186688 te_acc 35.064935\n",
      "median: at fang n_at 10 e 260 fed_model val loss 1.7917 val acc 33.8474 best val_acc 35.186688 te_acc 35.064935\n",
      "median: at fang n_at 10 e 270 fed_model val loss 1.8046 val acc 32.7110 best val_acc 37.195617 te_acc 35.978084\n",
      "median: at fang n_at 10 e 280 fed_model val loss 1.6844 val acc 37.6218 best val_acc 38.798701 te_acc 37.195617\n",
      "median: at fang n_at 10 e 290 fed_model val loss 1.7629 val acc 33.6445 best val_acc 38.798701 te_acc 37.195617\n",
      "median: at fang n_at 10 e 300 fed_model val loss 1.7404 val acc 35.8766 best val_acc 38.879870 te_acc 37.784091\n",
      "median: at fang n_at 10 e 310 fed_model val loss 1.6397 val acc 38.9002 best val_acc 39.448052 te_acc 39.346591\n",
      "median: at fang n_at 10 e 320 fed_model val loss 1.6879 val acc 36.4042 best val_acc 39.610390 te_acc 39.427760\n",
      "median: at fang n_at 10 e 330 fed_model val loss 1.7995 val acc 31.5950 best val_acc 39.935065 te_acc 40.259740\n",
      "median: at fang n_at 10 e 340 fed_model val loss 1.6569 val acc 40.5235 best val_acc 40.990260 te_acc 40.625000\n",
      "median: at fang n_at 10 e 350 fed_model val loss 1.8523 val acc 33.1372 best val_acc 40.990260 te_acc 40.625000\n",
      "median: at fang n_at 10 e 360 fed_model val loss 1.6607 val acc 39.5089 best val_acc 41.517857 te_acc 40.036526\n",
      "median: at fang n_at 10 e 370 fed_model val loss 1.5855 val acc 41.6396 best val_acc 41.964286 te_acc 40.929383\n",
      "median: at fang n_at 10 e 380 fed_model val loss 1.6818 val acc 38.0276 best val_acc 42.471591 te_acc 41.923701\n",
      "median: at fang n_at 10 e 390 fed_model val loss 1.5708 val acc 42.7557 best val_acc 42.755682 te_acc 41.700487\n",
      "median: at fang n_at 10 e 400 fed_model val loss 1.6937 val acc 37.9261 best val_acc 44.135552 te_acc 44.013799\n",
      "median: at fang n_at 10 e 410 fed_model val loss 1.6424 val acc 40.3003 best val_acc 44.135552 te_acc 44.013799\n",
      "median: at fang n_at 10 e 420 fed_model val loss 1.6012 val acc 41.8019 best val_acc 44.135552 te_acc 44.013799\n",
      "median: at fang n_at 10 e 430 fed_model val loss 1.5616 val acc 42.4919 best val_acc 44.135552 te_acc 44.013799\n",
      "median: at fang n_at 10 e 440 fed_model val loss 1.5647 val acc 43.6282 best val_acc 44.318182 te_acc 43.628247\n",
      "median: at fang n_at 10 e 450 fed_model val loss 1.5609 val acc 42.5122 best val_acc 44.318182 te_acc 43.628247\n",
      "median: at fang n_at 10 e 460 fed_model val loss 1.4886 val acc 46.0430 best val_acc 46.043019 te_acc 45.556006\n",
      "median: at fang n_at 10 e 470 fed_model val loss 1.5080 val acc 44.9878 best val_acc 46.043019 te_acc 45.556006\n",
      "median: at fang n_at 10 e 480 fed_model val loss 1.5608 val acc 41.7208 best val_acc 46.043019 te_acc 45.556006\n",
      "median: at fang n_at 10 e 490 fed_model val loss 1.5301 val acc 44.5414 best val_acc 46.063312 te_acc 45.738636\n",
      "median: at fang n_at 10 e 500 fed_model val loss 1.5622 val acc 42.0049 best val_acc 46.672078 te_acc 46.347403\n",
      "median: at fang n_at 10 e 510 fed_model val loss 1.5419 val acc 43.8718 best val_acc 46.672078 te_acc 46.347403\n",
      "median: at fang n_at 10 e 520 fed_model val loss 1.5001 val acc 46.0430 best val_acc 46.672078 te_acc 46.347403\n",
      "median: at fang n_at 10 e 530 fed_model val loss 1.5537 val acc 43.4253 best val_acc 47.402597 te_acc 47.199675\n",
      "median: at fang n_at 10 e 540 fed_model val loss 1.5119 val acc 44.8458 best val_acc 47.402597 te_acc 47.199675\n",
      "median: at fang n_at 10 e 550 fed_model val loss 1.8204 val acc 35.7549 best val_acc 47.402597 te_acc 47.199675\n",
      "median: at fang n_at 10 e 560 fed_model val loss 1.5594 val acc 44.5617 best val_acc 47.402597 te_acc 47.199675\n",
      "median: at fang n_at 10 e 570 fed_model val loss 1.4993 val acc 45.6981 best val_acc 47.402597 te_acc 47.199675\n",
      "median: at fang n_at 10 e 580 fed_model val loss 1.4875 val acc 45.5154 best val_acc 47.402597 te_acc 47.199675\n",
      "median: at fang n_at 10 e 590 fed_model val loss 1.5052 val acc 45.1502 best val_acc 47.402597 te_acc 47.199675\n",
      "median: at fang n_at 10 e 600 fed_model val loss 1.5066 val acc 44.3588 best val_acc 47.402597 te_acc 47.199675\n",
      "median: at fang n_at 10 e 610 fed_model val loss 1.4788 val acc 45.4748 best val_acc 47.402597 te_acc 47.199675\n",
      "median: at fang n_at 10 e 620 fed_model val loss 1.5005 val acc 45.8401 best val_acc 47.402597 te_acc 47.199675\n",
      "median: at fang n_at 10 e 630 fed_model val loss 1.4275 val acc 48.2752 best val_acc 48.275162 te_acc 48.660714\n",
      "median: at fang n_at 10 e 640 fed_model val loss 1.5525 val acc 42.9789 best val_acc 48.376623 te_acc 48.457792\n",
      "median: at fang n_at 10 e 650 fed_model val loss 1.5092 val acc 44.6631 best val_acc 48.376623 te_acc 48.457792\n",
      "median: at fang n_at 10 e 660 fed_model val loss 1.4749 val acc 46.4692 best val_acc 48.376623 te_acc 48.457792\n",
      "median: at fang n_at 10 e 670 fed_model val loss 1.4958 val acc 46.3068 best val_acc 48.376623 te_acc 48.457792\n",
      "median: at fang n_at 10 e 680 fed_model val loss 1.5267 val acc 43.8920 best val_acc 48.376623 te_acc 48.457792\n",
      "median: at fang n_at 10 e 690 fed_model val loss 1.4900 val acc 46.8953 best val_acc 48.376623 te_acc 48.457792\n",
      "median: at fang n_at 10 e 700 fed_model val loss 1.4665 val acc 47.3214 best val_acc 48.640422 te_acc 48.417208\n",
      "median: at fang n_at 10 e 710 fed_model val loss 1.4369 val acc 47.2808 best val_acc 48.640422 te_acc 48.417208\n",
      "median: at fang n_at 10 e 720 fed_model val loss 1.5316 val acc 45.3531 best val_acc 48.640422 te_acc 48.417208\n",
      "median: at fang n_at 10 e 730 fed_model val loss 1.5087 val acc 45.6372 best val_acc 48.640422 te_acc 48.417208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median: at fang n_at 10 e 740 fed_model val loss 1.4620 val acc 47.1997 best val_acc 48.640422 te_acc 48.417208\n",
      "median: at fang n_at 10 e 750 fed_model val loss 1.4385 val acc 47.9708 best val_acc 48.640422 te_acc 48.417208\n",
      "median: at fang n_at 10 e 760 fed_model val loss 1.4782 val acc 46.4692 best val_acc 48.640422 te_acc 48.417208\n",
      "median: at fang n_at 10 e 770 fed_model val loss 1.4658 val acc 46.6112 best val_acc 48.741883 te_acc 49.127435\n",
      "median: at fang n_at 10 e 780 fed_model val loss 1.4926 val acc 46.2662 best val_acc 48.741883 te_acc 49.127435\n",
      "median: at fang n_at 10 e 790 fed_model val loss 1.5236 val acc 44.9067 best val_acc 48.741883 te_acc 49.127435\n",
      "median: at fang n_at 10 e 800 fed_model val loss 1.4818 val acc 45.5560 best val_acc 48.741883 te_acc 49.127435\n",
      "median: at fang n_at 10 e 810 fed_model val loss 1.4132 val acc 48.3563 best val_acc 48.741883 te_acc 49.127435\n",
      "median: at fang n_at 10 e 820 fed_model val loss 1.4821 val acc 45.7386 best val_acc 48.741883 te_acc 49.127435\n",
      "median: at fang n_at 10 e 830 fed_model val loss 1.4565 val acc 47.1794 best val_acc 48.741883 te_acc 49.127435\n",
      "median: at fang n_at 10 e 840 fed_model val loss 1.4243 val acc 48.1331 best val_acc 48.741883 te_acc 49.127435\n",
      "median: at fang n_at 10 e 850 fed_model val loss 1.4340 val acc 48.3563 best val_acc 48.741883 te_acc 49.127435\n",
      "median: at fang n_at 10 e 860 fed_model val loss 1.4348 val acc 47.1794 best val_acc 49.127435 te_acc 49.533279\n",
      "median: at fang n_at 10 e 870 fed_model val loss 1.4848 val acc 46.3677 best val_acc 49.127435 te_acc 49.533279\n",
      "median: at fang n_at 10 e 880 fed_model val loss 1.4955 val acc 45.6778 best val_acc 49.127435 te_acc 49.533279\n",
      "median: at fang n_at 10 e 890 fed_model val loss 1.4570 val acc 46.5909 best val_acc 49.127435 te_acc 49.533279\n",
      "median: at fang n_at 10 e 900 fed_model val loss 1.4482 val acc 47.7273 best val_acc 49.269481 te_acc 49.736201\n",
      "median: at fang n_at 10 e 910 fed_model val loss 1.5573 val acc 44.3182 best val_acc 49.269481 te_acc 49.736201\n",
      "median: at fang n_at 10 e 920 fed_model val loss 1.4891 val acc 46.0633 best val_acc 49.269481 te_acc 49.736201\n",
      "median: at fang n_at 10 e 930 fed_model val loss 1.4871 val acc 46.3880 best val_acc 49.269481 te_acc 49.736201\n",
      "median: at fang n_at 10 e 1010 fed_model val loss 1.3888 val acc 49.9797 best val_acc 50.223214 te_acc 50.730519\n",
      "median: at fang n_at 10 e 1020 fed_model val loss 1.3842 val acc 50.2841 best val_acc 50.385552 te_acc 50.811688\n",
      "median: at fang n_at 10 e 1030 fed_model val loss 1.3815 val acc 50.3247 best val_acc 50.568182 te_acc 50.831981\n",
      "median: at fang n_at 10 e 1040 fed_model val loss 1.3801 val acc 50.4058 best val_acc 50.568182 te_acc 50.831981\n",
      "median: at fang n_at 10 e 1050 fed_model val loss 1.3800 val acc 50.6899 best val_acc 50.689935 te_acc 51.217532\n",
      "median: at fang n_at 10 e 1060 fed_model val loss 1.3795 val acc 50.7711 best val_acc 50.771104 te_acc 50.913149\n",
      "median: at fang n_at 10 e 1070 fed_model val loss 1.3784 val acc 50.8320 best val_acc 50.913149 te_acc 51.156656\n",
      "median: at fang n_at 10 e 1080 fed_model val loss 1.3768 val acc 50.7508 best val_acc 50.994318 te_acc 51.258117\n",
      "median: at fang n_at 10 e 1090 fed_model val loss 1.3798 val acc 50.6696 best val_acc 51.055195 te_acc 51.318994\n",
      "median: at fang n_at 10 e 1100 fed_model val loss 1.3771 val acc 50.7508 best val_acc 51.055195 te_acc 51.318994\n",
      "median: at fang n_at 10 e 1110 fed_model val loss 1.3787 val acc 50.9131 best val_acc 51.075487 te_acc 51.359578\n",
      "median: at fang n_at 10 e 1120 fed_model val loss 1.3762 val acc 50.6291 best val_acc 51.075487 te_acc 51.359578\n",
      "median: at fang n_at 10 e 1130 fed_model val loss 1.3761 val acc 50.9740 best val_acc 51.075487 te_acc 51.359578\n",
      "median: at fang n_at 10 e 1140 fed_model val loss 1.3780 val acc 50.6696 best val_acc 51.116071 te_acc 51.501623\n",
      "median: at fang n_at 10 e 1150 fed_model val loss 1.3843 val acc 50.6088 best val_acc 51.116071 te_acc 51.501623\n",
      "median: at fang n_at 10 e 1160 fed_model val loss 1.3851 val acc 50.4261 best val_acc 51.116071 te_acc 51.501623\n",
      "median: at fang n_at 10 e 1170 fed_model val loss 1.3780 val acc 50.8117 best val_acc 51.116071 te_acc 51.501623\n",
      "median: at fang n_at 10 e 1180 fed_model val loss 1.3798 val acc 50.1826 best val_acc 51.116071 te_acc 51.501623\n",
      "median: at fang n_at 10 e 1190 fed_model val loss 1.3843 val acc 50.1623 best val_acc 51.116071 te_acc 51.501623\n",
      "median: at fang n_at 10 e 1199 fed_model val loss 1.3749 val acc 50.8929 best val_acc 51.176948 te_acc 51.339286\n",
      "median: at fang n_at 10 e 1200 fed_model val loss 1.3779 val acc 50.6696 best val_acc 51.176948 te_acc 51.339286\n"
     ]
    }
   ],
   "source": [
    "batch_size=250\n",
    "resume=0\n",
    "nepochs=1200\n",
    "schedule=[1000]\n",
    "nbatches = user_tr_len//batch_size\n",
    "\n",
    "gamma=.5\n",
    "opt = 'sgd'\n",
    "fed_lr=0.5\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "aggregation='median'\n",
    "multi_k = False\n",
    "candidates = []\n",
    "\n",
    "at_type='fang'\n",
    "z_values=[0.0]\n",
    "n_attackers=[10]\n",
    "\n",
    "arch='alexnet'\n",
    "chkpt='./'+aggregation\n",
    "\n",
    "for n_attacker in n_attackers:\n",
    "    epoch_num = 0\n",
    "    best_global_acc = 0\n",
    "    best_global_te_acc = 0\n",
    "\n",
    "    for z in z_values:\n",
    "        fed_file='alexnet_checkpoint_%s_%s_%d_%.2f.pth.tar'%(aggregation,at_type,n_attacker,z)\n",
    "        fed_best_file='alexnet_best_%s_%s_%d_%.2f.pth.tar'%(aggregation,at_type,n_attacker,z)\n",
    "\n",
    "        if resume:\n",
    "            fed_checkpoint = chkpt+'/'+fed_file\n",
    "            assert os.path.isfile(fed_checkpoint), 'Error: no user checkpoint at %s'%(fed_checkpoint)\n",
    "            checkpoint = torch.load(fed_checkpoint, map_location='cuda:%d'%torch.cuda.current_device())\n",
    "            fed_model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer_fed.load_state_dict(checkpoint['optimizer'])\n",
    "            resume = 0\n",
    "            best_global_acc=checkpoint['best_acc']\n",
    "            best_global_te_acc=checkpoint['best_te_acc']\n",
    "            val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "            epoch_num += checkpoint['epoch']\n",
    "            print('resuming from epoch %d | val acc %.4f | best acc %.3f | best te acc %.3f'%(epoch_num, val_acc, best_global_acc, best_global_te_acc))\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        r=np.arange(user_tr_len)\n",
    "\n",
    "        fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
    "        optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "        while epoch_num <= nepochs:\n",
    "            user_grads=[]\n",
    "            if not epoch_num and epoch_num%nbatches == 0:\n",
    "                np.random.shuffle(r)\n",
    "                for i in range(nusers):\n",
    "                    user_tr_data_tensors[i]=user_tr_data_tensors[i][r]\n",
    "                    user_tr_label_tensors[i]=user_tr_label_tensors[i][r]\n",
    "\n",
    "            for i in range(n_attacker, nusers):\n",
    "\n",
    "                inputs = user_tr_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "                targets = user_tr_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "                outputs = fed_model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                fed_model.zero_grad()\n",
    "                loss.backward(retain_graph=True)\n",
    "\n",
    "                param_grad=[]\n",
    "                for param in fed_model.parameters():\n",
    "                    param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "                user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "\n",
    "            malicious_grads = user_grads\n",
    "\n",
    "            if epoch_num in schedule:\n",
    "                for param_group in optimizer_fed.param_groups:\n",
    "                    param_group['lr'] *= gamma\n",
    "                    print('New learnin rate ', param_group['lr'])\n",
    "\n",
    "            if n_attacker > 0:\n",
    "                if at_type == 'paf':\n",
    "                    malicious_grads=get_malicious_predictions_poison_all_far_sign(malicious_grads,nusers,n_attacker)\n",
    "                elif at_type == 'lie':\n",
    "                    malicious_grads = get_malicious_updates_lie(malicious_grads, n_attacker, z, epoch_num)\n",
    "                elif at_type == 'fang':\n",
    "                    agg_grads = torch.mean(malicious_grads, 0)\n",
    "                    deviation = torch.sign(agg_grads)\n",
    "                    malicious_grads = get_malicious_updates_fang_trmean(malicious_grads, deviation, n_attacker, epoch_num)\n",
    "                elif at_type == 'our':\n",
    "                    agg_grads = torch.mean(malicious_grads, 0)\n",
    "                    malicious_grads = our_attack_krum(malicious_grads, agg_grads, n_attacker, compression=compression, q_level=q_level, norm=norm)\n",
    "\n",
    "            if not epoch_num : \n",
    "                print(malicious_grads.shape)\n",
    "                \n",
    "            if aggregation=='median':\n",
    "                agg_grads=torch.median(malicious_grads,dim=0)[0]\n",
    "\n",
    "            elif aggregation=='average':\n",
    "                agg_grads=torch.mean(malicious_grads,dim=0)\n",
    "\n",
    "            elif aggregation=='trmean':\n",
    "                agg_grads=tr_mean(malicious_grads, n_attacker)\n",
    "\n",
    "            elif aggregation=='krum' or aggregation=='mkrum':\n",
    "                multi_k = True if aggregation == 'mkrum' else False\n",
    "                if epoch_num == 0: print('multi krum is ', multi_k)\n",
    "                agg_grads, krum_candidate = multi_krum(malicious_grads, n_attacker, multi_k=multi_k)\n",
    "                \n",
    "            elif aggregation=='bulyan':\n",
    "                agg_grads, krum_candidate=bulyan(malicious_grads, n_attacker)\n",
    "\n",
    "            del user_grads\n",
    "\n",
    "            start_idx=0\n",
    "\n",
    "            optimizer_fed.zero_grad()\n",
    "\n",
    "            model_grads=[]\n",
    "\n",
    "            for i, param in enumerate(fed_model.parameters()):\n",
    "                param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "                start_idx=start_idx+len(param.data.view(-1))\n",
    "                param_=param_.cuda()\n",
    "                model_grads.append(param_)\n",
    "\n",
    "            optimizer_fed.step(model_grads)\n",
    "\n",
    "            val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "            te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "            is_best = best_global_acc < val_acc\n",
    "\n",
    "            best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "            if is_best:\n",
    "                best_global_te_acc = te_acc\n",
    "\n",
    "            if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "                print('%s: at %s n_at %d e %d fed_model val loss %.4f val acc %.4f best val_acc %f te_acc %f'%(aggregation, at_type, n_attacker, epoch_num, val_loss, val_acc, best_global_acc,best_global_te_acc))\n",
    "\n",
    "            if val_loss > 10:\n",
    "                print('val loss %f too high'%val_loss)\n",
    "                break\n",
    "            \n",
    "            epoch_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for LIE attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lie_attack(all_updates, z):\n",
    "    avg = torch.mean(all_updates, dim=0)\n",
    "    std = torch.std(all_updates, dim=0)\n",
    "    return avg + z * std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 2472266])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vshejwalkar/fed-quant-robustness/code/cifar10/sgd.py:108: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729138878/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  p.data.add_(-group['lr'], d_p)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median: at LIE n_at 10 e 0 fed_model val loss 2.3035 val acc 9.5982 best val_acc 9.598214 te_acc 9.577922\n",
      "median: at LIE n_at 10 e 10 fed_model val loss 2.2962 val acc 13.7378 best val_acc 13.879870 te_acc 13.940747\n",
      "median: at LIE n_at 10 e 20 fed_model val loss 2.2283 val acc 18.3036 best val_acc 20.698052 te_acc 20.515422\n",
      "median: at LIE n_at 10 e 30 fed_model val loss 2.2397 val acc 12.5203 best val_acc 20.698052 te_acc 20.515422\n",
      "median: at LIE n_at 10 e 40 fed_model val loss 2.6411 val acc 9.5576 best val_acc 23.478084 te_acc 23.498377\n",
      "median: at LIE n_at 10 e 50 fed_model val loss 2.3050 val acc 9.9026 best val_acc 23.478084 te_acc 23.498377\n",
      "median: at LIE n_at 10 e 60 fed_model val loss 2.3034 val acc 9.9026 best val_acc 23.478084 te_acc 23.498377\n",
      "median: at LIE n_at 10 e 70 fed_model val loss 2.3024 val acc 9.9026 best val_acc 23.478084 te_acc 23.498377\n",
      "median: at LIE n_at 10 e 80 fed_model val loss 2.3003 val acc 11.9927 best val_acc 23.478084 te_acc 23.498377\n",
      "median: at LIE n_at 10 e 90 fed_model val loss 2.2872 val acc 10.7346 best val_acc 23.478084 te_acc 23.498377\n",
      "median: at LIE n_at 10 e 100 fed_model val loss 2.2615 val acc 11.9521 best val_acc 23.478084 te_acc 23.498377\n",
      "median: at LIE n_at 10 e 110 fed_model val loss 2.1710 val acc 18.8109 best val_acc 23.478084 te_acc 23.498377\n",
      "median: at LIE n_at 10 e 120 fed_model val loss 2.2929 val acc 11.2013 best val_acc 23.478084 te_acc 23.498377\n",
      "median: at LIE n_at 10 e 130 fed_model val loss 2.2425 val acc 17.5325 best val_acc 23.478084 te_acc 23.498377\n",
      "median: at LIE n_at 10 e 140 fed_model val loss 2.0697 val acc 20.6169 best val_acc 23.478084 te_acc 23.498377\n",
      "median: at LIE n_at 10 e 150 fed_model val loss 2.0068 val acc 24.4724 best val_acc 24.655032 te_acc 23.417208\n",
      "median: at LIE n_at 10 e 160 fed_model val loss 2.1702 val acc 17.0455 best val_acc 24.655032 te_acc 23.417208\n",
      "median: at LIE n_at 10 e 170 fed_model val loss 2.1062 val acc 18.2427 best val_acc 24.655032 te_acc 23.417208\n",
      "median: at LIE n_at 10 e 180 fed_model val loss 2.1527 val acc 17.0252 best val_acc 24.655032 te_acc 23.417208\n",
      "median: at LIE n_at 10 e 190 fed_model val loss 2.1549 val acc 18.1615 best val_acc 26.623377 te_acc 25.974026\n",
      "median: at LIE n_at 10 e 200 fed_model val loss 2.1521 val acc 15.7873 best val_acc 26.724838 te_acc 25.974026\n",
      "median: at LIE n_at 10 e 210 fed_model val loss 1.9879 val acc 24.7565 best val_acc 26.724838 te_acc 25.974026\n",
      "median: at LIE n_at 10 e 220 fed_model val loss 2.0891 val acc 22.9302 best val_acc 27.414773 te_acc 27.577110\n",
      "median: at LIE n_at 10 e 230 fed_model val loss 2.0298 val acc 20.0487 best val_acc 27.840909 te_acc 27.150974\n",
      "median: at LIE n_at 10 e 240 fed_model val loss 2.2171 val acc 22.9302 best val_acc 29.971591 te_acc 28.835227\n",
      "median: at LIE n_at 10 e 250 fed_model val loss 2.0667 val acc 24.4318 best val_acc 29.971591 te_acc 28.835227\n",
      "median: at LIE n_at 10 e 260 fed_model val loss 1.9424 val acc 28.0032 best val_acc 30.519481 te_acc 29.910714\n",
      "median: at LIE n_at 10 e 270 fed_model val loss 2.0305 val acc 24.2086 best val_acc 33.258929 te_acc 32.893669\n",
      "median: at LIE n_at 10 e 280 fed_model val loss 1.9563 val acc 28.1859 best val_acc 33.989448 te_acc 33.258929\n",
      "median: at LIE n_at 10 e 290 fed_model val loss 1.8659 val acc 32.5487 best val_acc 33.989448 te_acc 33.258929\n",
      "median: at LIE n_at 10 e 300 fed_model val loss 2.0042 val acc 26.9683 best val_acc 33.989448 te_acc 33.258929\n",
      "median: at LIE n_at 10 e 310 fed_model val loss 1.9918 val acc 25.4464 best val_acc 33.989448 te_acc 33.258929\n",
      "median: at LIE n_at 10 e 320 fed_model val loss 1.8775 val acc 29.1193 best val_acc 35.693994 te_acc 35.166396\n",
      "median: at LIE n_at 10 e 330 fed_model val loss 1.9366 val acc 28.9976 best val_acc 35.693994 te_acc 35.166396\n",
      "median: at LIE n_at 10 e 340 fed_model val loss 1.8127 val acc 30.5601 best val_acc 35.856331 te_acc 35.044643\n",
      "median: at LIE n_at 10 e 350 fed_model val loss 1.9437 val acc 28.9367 best val_acc 39.529221 te_acc 38.737825\n",
      "median: at LIE n_at 10 e 360 fed_model val loss 1.6641 val acc 38.2914 best val_acc 39.529221 te_acc 38.737825\n",
      "median: at LIE n_at 10 e 370 fed_model val loss 1.7498 val acc 35.0446 best val_acc 39.529221 te_acc 38.737825\n",
      "median: at LIE n_at 10 e 380 fed_model val loss 1.7538 val acc 33.6039 best val_acc 39.529221 te_acc 38.737825\n",
      "median: at LIE n_at 10 e 390 fed_model val loss 1.8503 val acc 32.9748 best val_acc 39.529221 te_acc 38.737825\n",
      "median: at LIE n_at 10 e 400 fed_model val loss 1.6608 val acc 38.9205 best val_acc 39.549513 te_acc 38.758117\n",
      "median: at LIE n_at 10 e 410 fed_model val loss 1.7677 val acc 34.4156 best val_acc 41.010552 te_acc 41.193182\n",
      "median: at LIE n_at 10 e 420 fed_model val loss 1.6563 val acc 37.8450 best val_acc 41.984578 te_acc 42.370130\n",
      "median: at LIE n_at 10 e 430 fed_model val loss 1.6382 val acc 38.9002 best val_acc 42.897727 te_acc 43.547078\n",
      "median: at LIE n_at 10 e 440 fed_model val loss 1.5709 val acc 42.0049 best val_acc 42.897727 te_acc 43.547078\n",
      "median: at LIE n_at 10 e 450 fed_model val loss 1.5789 val acc 40.7873 best val_acc 43.262987 te_acc 43.607955\n",
      "median: at LIE n_at 10 e 460 fed_model val loss 1.7898 val acc 38.3726 best val_acc 43.831169 te_acc 44.602273\n",
      "median: at LIE n_at 10 e 470 fed_model val loss 1.5720 val acc 41.5584 best val_acc 44.480519 te_acc 44.338474\n",
      "median: at LIE n_at 10 e 480 fed_model val loss 1.7293 val acc 36.8709 best val_acc 45.961851 te_acc 46.144481\n",
      "median: at LIE n_at 10 e 490 fed_model val loss 1.5975 val acc 39.3872 best val_acc 46.672078 te_acc 45.616883\n",
      "median: at LIE n_at 10 e 500 fed_model val loss 1.5394 val acc 43.7906 best val_acc 46.672078 te_acc 45.616883\n",
      "median: at LIE n_at 10 e 510 fed_model val loss 1.6285 val acc 42.7963 best val_acc 46.672078 te_acc 45.616883\n",
      "median: at LIE n_at 10 e 520 fed_model val loss 1.5768 val acc 42.1875 best val_acc 48.112825 te_acc 47.443182\n",
      "median: at LIE n_at 10 e 530 fed_model val loss 1.5999 val acc 41.2541 best val_acc 49.188312 te_acc 49.837662\n",
      "median: at LIE n_at 10 e 540 fed_model val loss 1.4963 val acc 44.3588 best val_acc 49.188312 te_acc 49.837662\n",
      "median: at LIE n_at 10 e 550 fed_model val loss 1.4541 val acc 47.0576 best val_acc 49.188312 te_acc 49.837662\n",
      "median: at LIE n_at 10 e 560 fed_model val loss 1.4533 val acc 47.1388 best val_acc 49.188312 te_acc 49.837662\n",
      "median: at LIE n_at 10 e 570 fed_model val loss 1.5707 val acc 44.7240 best val_acc 49.188312 te_acc 49.837662\n",
      "median: at LIE n_at 10 e 580 fed_model val loss 1.5308 val acc 42.0657 best val_acc 50.649351 te_acc 51.623377\n",
      "median: at LIE n_at 10 e 590 fed_model val loss 1.3749 val acc 50.3856 best val_acc 50.872565 te_acc 51.745130\n",
      "median: at LIE n_at 10 e 600 fed_model val loss 1.3894 val acc 49.1680 best val_acc 50.872565 te_acc 51.745130\n",
      "median: at LIE n_at 10 e 610 fed_model val loss 1.3796 val acc 49.5536 best val_acc 52.516234 te_acc 52.495942\n",
      "median: at LIE n_at 10 e 620 fed_model val loss 1.4372 val acc 49.4927 best val_acc 52.516234 te_acc 52.495942\n",
      "median: at LIE n_at 10 e 630 fed_model val loss 1.2963 val acc 53.0844 best val_acc 53.084416 te_acc 54.301948\n",
      "median: at LIE n_at 10 e 640 fed_model val loss 1.3864 val acc 50.0406 best val_acc 53.084416 te_acc 54.301948\n",
      "median: at LIE n_at 10 e 650 fed_model val loss 1.5388 val acc 44.1153 best val_acc 53.084416 te_acc 54.301948\n",
      "median: at LIE n_at 10 e 660 fed_model val loss 1.3616 val acc 51.0552 best val_acc 53.084416 te_acc 54.301948\n",
      "median: at LIE n_at 10 e 670 fed_model val loss 1.4202 val acc 48.6201 best val_acc 53.429383 te_acc 54.180195\n",
      "median: at LIE n_at 10 e 680 fed_model val loss 1.4137 val acc 49.8782 best val_acc 53.429383 te_acc 54.180195\n",
      "median: at LIE n_at 10 e 690 fed_model val loss 1.3843 val acc 51.0755 best val_acc 53.429383 te_acc 54.180195\n",
      "median: at LIE n_at 10 e 700 fed_model val loss 1.3939 val acc 50.2232 best val_acc 54.200487 te_acc 54.809253\n",
      "median: at LIE n_at 10 e 710 fed_model val loss 1.3109 val acc 52.3539 best val_acc 54.200487 te_acc 54.809253\n",
      "median: at LIE n_at 10 e 720 fed_model val loss 1.5366 val acc 44.5211 best val_acc 54.504870 te_acc 54.951299\n",
      "median: at LIE n_at 10 e 730 fed_model val loss 1.4526 val acc 48.4578 best val_acc 54.504870 te_acc 54.951299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median: at LIE n_at 10 e 740 fed_model val loss 1.3391 val acc 51.2581 best val_acc 55.864448 te_acc 55.823864\n",
      "median: at LIE n_at 10 e 750 fed_model val loss 1.4453 val acc 48.7013 best val_acc 55.965909 te_acc 56.493506\n",
      "median: at LIE n_at 10 e 760 fed_model val loss 1.2891 val acc 54.5657 best val_acc 55.965909 te_acc 56.493506\n",
      "median: at LIE n_at 10 e 770 fed_model val loss 1.4783 val acc 47.4026 best val_acc 55.965909 te_acc 56.493506\n",
      "median: at LIE n_at 10 e 780 fed_model val loss 1.3229 val acc 52.1307 best val_acc 55.965909 te_acc 56.493506\n",
      "median: at LIE n_at 10 e 790 fed_model val loss 1.2288 val acc 55.7630 best val_acc 55.965909 te_acc 56.493506\n",
      "median: at LIE n_at 10 e 800 fed_model val loss 1.4126 val acc 50.1826 best val_acc 56.818182 te_acc 57.183442\n",
      "median: at LIE n_at 10 e 810 fed_model val loss 1.2332 val acc 55.9862 best val_acc 56.818182 te_acc 57.183442\n",
      "median: at LIE n_at 10 e 820 fed_model val loss 1.2230 val acc 56.2500 best val_acc 56.818182 te_acc 57.183442\n",
      "median: at LIE n_at 10 e 830 fed_model val loss 1.2931 val acc 53.7744 best val_acc 56.818182 te_acc 57.183442\n",
      "median: at LIE n_at 10 e 840 fed_model val loss 1.2291 val acc 55.8442 best val_acc 56.818182 te_acc 57.183442\n",
      "median: at LIE n_at 10 e 850 fed_model val loss 1.4101 val acc 49.7768 best val_acc 58.461851 te_acc 58.258929\n",
      "median: at LIE n_at 10 e 860 fed_model val loss 1.3026 val acc 53.6729 best val_acc 58.461851 te_acc 58.258929\n",
      "median: at LIE n_at 10 e 870 fed_model val loss 1.2986 val acc 53.5511 best val_acc 58.461851 te_acc 58.258929\n",
      "median: at LIE n_at 10 e 880 fed_model val loss 1.4785 val acc 48.6404 best val_acc 58.461851 te_acc 58.258929\n",
      "median: at LIE n_at 10 e 890 fed_model val loss 1.2171 val acc 56.6153 best val_acc 58.461851 te_acc 58.258929\n",
      "median: at LIE n_at 10 e 900 fed_model val loss 1.3783 val acc 53.4497 best val_acc 58.461851 te_acc 58.258929\n",
      "median: at LIE n_at 10 e 910 fed_model val loss 1.3789 val acc 49.7565 best val_acc 58.644481 te_acc 58.725649\n",
      "median: at LIE n_at 10 e 920 fed_model val loss 1.2558 val acc 54.0179 best val_acc 58.644481 te_acc 58.725649\n",
      "median: at LIE n_at 10 e 930 fed_model val loss 1.2221 val acc 56.8385 best val_acc 59.111201 te_acc 59.293831\n",
      "median: at LIE n_at 10 e 940 fed_model val loss 1.2601 val acc 54.9919 best val_acc 59.111201 te_acc 59.293831\n",
      "median: at LIE n_at 10 e 950 fed_model val loss 1.5048 val acc 47.9505 best val_acc 60.227273 te_acc 60.673701\n",
      "median: at LIE n_at 10 e 960 fed_model val loss 1.4458 val acc 49.2289 best val_acc 60.227273 te_acc 60.673701\n",
      "median: at LIE n_at 10 e 970 fed_model val loss 1.2555 val acc 57.1023 best val_acc 60.227273 te_acc 60.673701\n",
      "median: at LIE n_at 10 e 980 fed_model val loss 1.2996 val acc 53.2468 best val_acc 60.572240 te_acc 60.288149\n",
      "median: at LIE n_at 10 e 990 fed_model val loss 1.1670 val acc 59.7403 best val_acc 60.572240 te_acc 60.288149\n",
      "New learnin rate  0.25\n",
      "median: at LIE n_at 10 e 1000 fed_model val loss 1.1053 val acc 61.0187 best val_acc 61.018669 te_acc 62.155032\n",
      "median: at LIE n_at 10 e 1010 fed_model val loss 1.0837 val acc 62.4391 best val_acc 62.662338 te_acc 63.372565\n",
      "median: at LIE n_at 10 e 1020 fed_model val loss 1.0877 val acc 62.7435 best val_acc 62.966721 te_acc 63.169643\n",
      "median: at LIE n_at 10 e 1030 fed_model val loss 1.1544 val acc 61.2419 best val_acc 62.966721 te_acc 63.169643\n",
      "median: at LIE n_at 10 e 1040 fed_model val loss 1.1404 val acc 61.5869 best val_acc 62.966721 te_acc 63.169643\n",
      "median: at LIE n_at 10 e 1050 fed_model val loss 1.1351 val acc 61.4651 best val_acc 62.966721 te_acc 63.169643\n",
      "median: at LIE n_at 10 e 1060 fed_model val loss 1.1084 val acc 61.9318 best val_acc 62.966721 te_acc 63.169643\n",
      "median: at LIE n_at 10 e 1070 fed_model val loss 1.1872 val acc 59.9026 best val_acc 62.966721 te_acc 63.169643\n",
      "median: at LIE n_at 10 e 1080 fed_model val loss 1.0866 val acc 63.4537 best val_acc 63.453734 te_acc 63.331981\n",
      "median: at LIE n_at 10 e 1090 fed_model val loss 1.1329 val acc 62.2768 best val_acc 63.453734 te_acc 63.331981\n",
      "median: at LIE n_at 10 e 1100 fed_model val loss 1.2909 val acc 56.6356 best val_acc 63.453734 te_acc 63.331981\n",
      "median: at LIE n_at 10 e 1110 fed_model val loss 1.2157 val acc 60.0852 best val_acc 63.453734 te_acc 63.331981\n",
      "median: at LIE n_at 10 e 1120 fed_model val loss 1.1612 val acc 61.8912 best val_acc 63.453734 te_acc 63.331981\n",
      "median: at LIE n_at 10 e 1130 fed_model val loss 1.2950 val acc 57.9951 best val_acc 63.453734 te_acc 63.331981\n",
      "median: at LIE n_at 10 e 1140 fed_model val loss 1.1559 val acc 62.2362 best val_acc 63.453734 te_acc 63.331981\n",
      "median: at LIE n_at 10 e 1150 fed_model val loss 1.1533 val acc 62.2159 best val_acc 63.453734 te_acc 63.331981\n",
      "median: at LIE n_at 10 e 1160 fed_model val loss 1.2411 val acc 59.6388 best val_acc 63.453734 te_acc 63.331981\n",
      "median: at LIE n_at 10 e 1170 fed_model val loss 1.1367 val acc 62.5000 best val_acc 63.453734 te_acc 63.331981\n",
      "median: at LIE n_at 10 e 1180 fed_model val loss 1.2341 val acc 60.6940 best val_acc 63.616071 te_acc 62.439123\n",
      "median: at LIE n_at 10 e 1190 fed_model val loss 1.2069 val acc 61.0593 best val_acc 63.616071 te_acc 62.439123\n",
      "median: at LIE n_at 10 e 1199 fed_model val loss 1.1958 val acc 60.1461 best val_acc 63.616071 te_acc 62.439123\n",
      "median: at LIE n_at 10 e 1200 fed_model val loss 1.1528 val acc 62.6218 best val_acc 63.616071 te_acc 62.439123\n"
     ]
    }
   ],
   "source": [
    "batch_size=250\n",
    "resume=0\n",
    "nepochs=1200\n",
    "schedule=[1000]\n",
    "nbatches = user_tr_len//batch_size\n",
    "\n",
    "gamma=.5\n",
    "opt = 'sgd'\n",
    "fed_lr=0.5\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "aggregation='median'\n",
    "multi_k = False\n",
    "candidates = []\n",
    "\n",
    "at_type='LIE'\n",
    "z_values={3:0.69847, 5:0.7054, 8:0.71904, 10:0.72575, 12:0.73891}\n",
    "n_attackers=[10]\n",
    "\n",
    "arch='alexnet'\n",
    "chkpt='./'+aggregation\n",
    "\n",
    "for n_attacker in n_attackers:\n",
    "    epoch_num = 0\n",
    "    best_global_acc = 0\n",
    "    best_global_te_acc = 0\n",
    "\n",
    "#     fed_file='alexnet_checkpoint_%s_%s_%d_%.2f.pth.tar'%(aggregation,at_type,n_attacker,z)\n",
    "#     fed_best_file='alexnet_best_%s_%s_%d_%.2f.pth.tar'%(aggregation,at_type,n_attacker,z)\n",
    "\n",
    "#     if resume:\n",
    "#         fed_checkpoint = chkpt+'/'+fed_file\n",
    "#         assert os.path.isfile(fed_checkpoint), 'Error: no user checkpoint at %s'%(fed_checkpoint)\n",
    "#         checkpoint = torch.load(fed_checkpoint, map_location='cuda:%d'%torch.cuda.current_device())\n",
    "#         fed_model.load_state_dict(checkpoint['state_dict'])\n",
    "#         optimizer_fed.load_state_dict(checkpoint['optimizer'])\n",
    "#         resume = 0\n",
    "#         best_global_acc=checkpoint['best_acc']\n",
    "#         best_global_te_acc=checkpoint['best_te_acc']\n",
    "#         val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "#         epoch_num += checkpoint['epoch']\n",
    "#         print('resuming from epoch %d | val acc %.4f | best acc %.3f | best te acc %.3f'%(epoch_num, val_acc, best_global_acc, best_global_te_acc))\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    r=np.arange(user_tr_len)\n",
    "\n",
    "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
    "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        user_grads=[]\n",
    "        if not epoch_num and epoch_num%nbatches == 0:\n",
    "            np.random.shuffle(r)\n",
    "            for i in range(nusers):\n",
    "                user_tr_data_tensors[i]=user_tr_data_tensors[i][r]\n",
    "                user_tr_label_tensors[i]=user_tr_label_tensors[i][r]\n",
    "\n",
    "        for i in range(n_attacker, nusers):\n",
    "\n",
    "            inputs = user_tr_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "            targets = user_tr_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "            outputs = fed_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            fed_model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            param_grad=[]\n",
    "            for param in fed_model.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "\n",
    "        malicious_grads = user_grads\n",
    "\n",
    "        if epoch_num in schedule:\n",
    "            for param_group in optimizer_fed.param_groups:\n",
    "                param_group['lr'] *= gamma\n",
    "                print('New learnin rate ', param_group['lr'])\n",
    "\n",
    "        if n_attacker > 0:\n",
    "            if at_type == 'lie':\n",
    "                mal_update = lie_attack(malicious_grads, z_values[n_attacker])\n",
    "                malicious_grads = torch.cat((torch.stack([mal_update]*n_attacker), malicious_grads))\n",
    "            elif at_type == 'fang':\n",
    "                agg_grads = torch.mean(malicious_grads, 0)\n",
    "                deviation = torch.sign(agg_grads)\n",
    "                malicious_grads = get_malicious_updates_fang_trmean(malicious_grads, deviation, n_attacker, epoch_num)\n",
    "            elif at_type == 'our-agr':\n",
    "                agg_grads = torch.mean(malicious_grads, 0)\n",
    "                malicious_grads = our_attack_krum(malicious_grads, agg_grads, n_attacker, compression=compression, q_level=q_level, norm=norm)\n",
    "\n",
    "        if not epoch_num : \n",
    "            print(malicious_grads.shape)\n",
    "\n",
    "        if aggregation=='median':\n",
    "            agg_grads=torch.median(malicious_grads,dim=0)[0]\n",
    "\n",
    "        elif aggregation=='average':\n",
    "            agg_grads=torch.mean(malicious_grads,dim=0)\n",
    "\n",
    "        elif aggregation=='trmean':\n",
    "            agg_grads=tr_mean(malicious_grads, n_attacker)\n",
    "\n",
    "        elif aggregation=='krum' or aggregation=='mkrum':\n",
    "            multi_k = True if aggregation == 'mkrum' else False\n",
    "            if epoch_num == 0: print('multi krum is ', multi_k)\n",
    "            agg_grads, krum_candidate = multi_krum(malicious_grads, n_attacker, multi_k=multi_k)\n",
    "\n",
    "        elif aggregation=='bulyan':\n",
    "            agg_grads, krum_candidate=bulyan(malicious_grads, n_attacker)\n",
    "\n",
    "        del user_grads\n",
    "\n",
    "        start_idx=0\n",
    "\n",
    "        optimizer_fed.zero_grad()\n",
    "\n",
    "        model_grads=[]\n",
    "\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "\n",
    "        optimizer_fed.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if is_best:\n",
    "            best_global_te_acc = te_acc\n",
    "\n",
    "        if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "            print('%s: at %s n_at %d e %d fed_model val loss %.4f val acc %.4f best val_acc %f te_acc %f'%(aggregation, at_type, n_attacker, epoch_num, val_loss, val_acc, best_global_acc,best_global_te_acc))\n",
    "\n",
    "        if val_loss > 10:\n",
    "            print('val loss %f too high'%val_loss)\n",
    "            break\n",
    "\n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for our AGR-tailored attack on Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_attack_median(all_updates, model_re, n_attackers, dev_type='unit_vec'):\n",
    "\n",
    "    if dev_type == 'unit_vec':\n",
    "        deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "    elif dev_type == 'sign':\n",
    "        deviation = torch.sign(model_re)\n",
    "    elif dev_type == 'std':\n",
    "        deviation = torch.std(all_updates, 0)\n",
    "\n",
    "    lamda = torch.Tensor([10.0]).cuda() #compute_lambda_our(all_updates, model_re, n_attackers)\n",
    "\n",
    "    threshold_diff = 1e-5\n",
    "    prev_loss = -1\n",
    "    lamda_fail = lamda\n",
    "    lamda_succ = 0\n",
    "    iters = 0 \n",
    "    while torch.abs(lamda_succ - lamda) > threshold_diff:\n",
    "        mal_update = (model_re - lamda * deviation)\n",
    "        mal_updates = torch.stack([mal_update] * n_attackers)\n",
    "        mal_updates = torch.cat((mal_updates, all_updates), 0)\n",
    "\n",
    "        agg_grads = torch.median(mal_updates, 0)[0]\n",
    "        \n",
    "        loss = torch.norm(agg_grads - model_re)\n",
    "        \n",
    "        if prev_loss < loss:\n",
    "            lamda_succ = lamda\n",
    "            lamda = lamda + lamda_fail / 2\n",
    "        else:\n",
    "            lamda = lamda - lamda_fail / 2\n",
    "\n",
    "        lamda_fail = lamda_fail / 2\n",
    "        prev_loss = loss\n",
    "        \n",
    "    mal_update = (model_re - lamda_succ * deviation)\n",
    "    mal_updates = torch.stack([mal_update] * n_attackers)\n",
    "    mal_updates = torch.cat((mal_updates, all_updates), 0)\n",
    "\n",
    "    return mal_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 2472266])\n",
      "median: at our-agr n_at 10 e 0 fed_model val loss 2.3026 val acc 9.6794 best val_acc 9.679383 te_acc 10.064935\n",
      "median: at our-agr n_at 10 e 10 fed_model val loss 2.2910 val acc 14.0219 best val_acc 14.021916 te_acc 14.184253\n",
      "median: at our-agr n_at 10 e 20 fed_model val loss 2.3668 val acc 11.8101 best val_acc 20.819805 te_acc 20.616883\n",
      "median: at our-agr n_at 10 e 30 fed_model val loss 2.2703 val acc 16.6599 best val_acc 20.819805 te_acc 20.616883\n",
      "median: at our-agr n_at 10 e 40 fed_model val loss 2.3029 val acc 10.5114 best val_acc 22.666396 te_acc 23.011364\n",
      "median: at our-agr n_at 10 e 50 fed_model val loss 2.2392 val acc 10.8766 best val_acc 22.666396 te_acc 23.011364\n",
      "median: at our-agr n_at 10 e 60 fed_model val loss 2.1888 val acc 21.3271 best val_acc 22.666396 te_acc 23.011364\n",
      "median: at our-agr n_at 10 e 70 fed_model val loss 2.2922 val acc 14.2451 best val_acc 22.666396 te_acc 23.011364\n",
      "median: at our-agr n_at 10 e 80 fed_model val loss 2.2073 val acc 16.9034 best val_acc 22.666396 te_acc 23.011364\n",
      "median: at our-agr n_at 10 e 90 fed_model val loss 2.3195 val acc 12.2159 best val_acc 23.051948 te_acc 22.747565\n",
      "median: at our-agr n_at 10 e 100 fed_model val loss 2.3297 val acc 9.5779 best val_acc 23.051948 te_acc 22.747565\n",
      "median: at our-agr n_at 10 e 110 fed_model val loss 2.2839 val acc 12.9261 best val_acc 23.051948 te_acc 22.747565\n",
      "median: at our-agr n_at 10 e 120 fed_model val loss 2.2465 val acc 16.0917 best val_acc 23.051948 te_acc 22.747565\n",
      "median: at our-agr n_at 10 e 130 fed_model val loss 2.2696 val acc 15.7265 best val_acc 23.051948 te_acc 22.747565\n",
      "median: at our-agr n_at 10 e 140 fed_model val loss 2.2314 val acc 15.8076 best val_acc 23.051948 te_acc 22.747565\n",
      "median: at our-agr n_at 10 e 150 fed_model val loss 2.2632 val acc 18.5065 best val_acc 23.051948 te_acc 22.747565\n",
      "median: at our-agr n_at 10 e 160 fed_model val loss 2.1607 val acc 18.8312 best val_acc 23.234578 te_acc 22.362013\n",
      "median: at our-agr n_at 10 e 170 fed_model val loss 2.2195 val acc 17.1266 best val_acc 23.944805 te_acc 23.417208\n",
      "median: at our-agr n_at 10 e 180 fed_model val loss 2.1428 val acc 20.0893 best val_acc 23.944805 te_acc 23.417208\n",
      "median: at our-agr n_at 10 e 190 fed_model val loss 2.3507 val acc 7.7516 best val_acc 23.944805 te_acc 23.417208\n",
      "median: at our-agr n_at 10 e 200 fed_model val loss 2.3332 val acc 9.0909 best val_acc 23.944805 te_acc 23.417208\n",
      "median: at our-agr n_at 10 e 210 fed_model val loss 2.3045 val acc 6.9805 best val_acc 23.944805 te_acc 23.417208\n",
      "median: at our-agr n_at 10 e 220 fed_model val loss 2.2202 val acc 15.7062 best val_acc 23.944805 te_acc 23.417208\n",
      "median: at our-agr n_at 10 e 230 fed_model val loss 2.2352 val acc 11.8304 best val_acc 23.944805 te_acc 23.417208\n",
      "median: at our-agr n_at 10 e 240 fed_model val loss 2.2234 val acc 15.4221 best val_acc 23.944805 te_acc 23.417208\n",
      "median: at our-agr n_at 10 e 250 fed_model val loss 2.3396 val acc 8.4821 best val_acc 23.944805 te_acc 23.417208\n",
      "median: at our-agr n_at 10 e 260 fed_model val loss 2.3264 val acc 9.6185 best val_acc 23.944805 te_acc 23.417208\n",
      "median: at our-agr n_at 10 e 270 fed_model val loss 2.3119 val acc 10.9375 best val_acc 23.944805 te_acc 23.417208\n",
      "median: at our-agr n_at 10 e 280 fed_model val loss 2.3077 val acc 9.9026 best val_acc 23.944805 te_acc 23.417208\n",
      "median: at our-agr n_at 10 e 290 fed_model val loss 2.2053 val acc 19.0747 best val_acc 23.944805 te_acc 23.417208\n",
      "median: at our-agr n_at 10 e 300 fed_model val loss 2.2832 val acc 9.9432 best val_acc 23.944805 te_acc 23.417208\n",
      "median: at our-agr n_at 10 e 310 fed_model val loss 2.2452 val acc 13.7378 best val_acc 23.944805 te_acc 23.417208\n",
      "median: at our-agr n_at 10 e 320 fed_model val loss 2.2124 val acc 15.3206 best val_acc 23.944805 te_acc 23.417208\n",
      "median: at our-agr n_at 10 e 330 fed_model val loss 2.1870 val acc 20.4140 best val_acc 23.944805 te_acc 23.417208\n",
      "median: at our-agr n_at 10 e 340 fed_model val loss 2.2105 val acc 20.3531 best val_acc 23.944805 te_acc 23.417208\n",
      "median: at our-agr n_at 10 e 350 fed_model val loss 2.1927 val acc 20.2922 best val_acc 23.944805 te_acc 23.417208\n",
      "median: at our-agr n_at 10 e 360 fed_model val loss 2.1208 val acc 20.0893 best val_acc 23.944805 te_acc 23.417208\n",
      "median: at our-agr n_at 10 e 370 fed_model val loss 2.1561 val acc 18.0195 best val_acc 23.944805 te_acc 23.417208\n",
      "median: at our-agr n_at 10 e 380 fed_model val loss 2.2176 val acc 15.5032 best val_acc 23.944805 te_acc 23.417208\n",
      "median: at our-agr n_at 10 e 390 fed_model val loss 2.2629 val acc 13.6364 best val_acc 23.944805 te_acc 23.417208\n",
      "median: at our-agr n_at 10 e 400 fed_model val loss 2.1274 val acc 21.3474 best val_acc 23.944805 te_acc 23.417208\n",
      "median: at our-agr n_at 10 e 410 fed_model val loss 2.0289 val acc 25.0406 best val_acc 25.040584 te_acc 24.289773\n",
      "median: at our-agr n_at 10 e 420 fed_model val loss 2.1562 val acc 20.5966 best val_acc 25.487013 te_acc 25.649351\n",
      "median: at our-agr n_at 10 e 430 fed_model val loss 2.2673 val acc 17.2687 best val_acc 26.136364 te_acc 26.379870\n",
      "median: at our-agr n_at 10 e 440 fed_model val loss 2.0185 val acc 20.5763 best val_acc 26.136364 te_acc 26.379870\n",
      "median: at our-agr n_at 10 e 450 fed_model val loss 2.2962 val acc 16.7817 best val_acc 26.136364 te_acc 26.379870\n",
      "median: at our-agr n_at 10 e 460 fed_model val loss 2.2462 val acc 15.6656 best val_acc 26.724838 te_acc 25.487013\n",
      "median: at our-agr n_at 10 e 470 fed_model val loss 2.0002 val acc 23.0114 best val_acc 26.846591 te_acc 25.953734\n",
      "median: at our-agr n_at 10 e 480 fed_model val loss 2.0153 val acc 22.4838 best val_acc 26.846591 te_acc 25.953734\n",
      "median: at our-agr n_at 10 e 490 fed_model val loss 1.9430 val acc 29.0990 best val_acc 29.099026 te_acc 27.374188\n",
      "median: at our-agr n_at 10 e 500 fed_model val loss 2.1455 val acc 17.4919 best val_acc 29.099026 te_acc 27.374188\n",
      "median: at our-agr n_at 10 e 510 fed_model val loss 2.3335 val acc 13.3117 best val_acc 29.099026 te_acc 27.374188\n",
      "median: at our-agr n_at 10 e 520 fed_model val loss 2.0157 val acc 23.2752 best val_acc 29.099026 te_acc 27.374188\n",
      "median: at our-agr n_at 10 e 530 fed_model val loss 1.9465 val acc 29.8498 best val_acc 29.849838 te_acc 29.707792\n",
      "median: at our-agr n_at 10 e 540 fed_model val loss 2.1722 val acc 21.9562 best val_acc 29.849838 te_acc 29.707792\n",
      "val loss 11.789267 too high\n"
     ]
    }
   ],
   "source": [
    "batch_size=250\n",
    "resume=0\n",
    "nepochs=1200\n",
    "schedule=[1000]\n",
    "nbatches = user_tr_len//batch_size\n",
    "\n",
    "gamma=.5\n",
    "opt = 'sgd'\n",
    "fed_lr=0.5\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "aggregation='median'\n",
    "multi_k = False\n",
    "candidates = []\n",
    "\n",
    "at_type='our-agr'\n",
    "dev_type = 'std'\n",
    "z_values={3:0.69847, 5:0.7054, 8:0.71904, 10:0.72575, 12:0.73891}\n",
    "n_attackers=[10]\n",
    "\n",
    "arch='alexnet'\n",
    "chkpt='./'+aggregation\n",
    "\n",
    "for n_attacker in n_attackers:\n",
    "    epoch_num = 0\n",
    "    best_global_acc = 0\n",
    "    best_global_te_acc = 0\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    r=np.arange(user_tr_len)\n",
    "\n",
    "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
    "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        user_grads=[]\n",
    "        if not epoch_num and epoch_num%nbatches == 0:\n",
    "            np.random.shuffle(r)\n",
    "            for i in range(nusers):\n",
    "                user_tr_data_tensors[i]=user_tr_data_tensors[i][r]\n",
    "                user_tr_label_tensors[i]=user_tr_label_tensors[i][r]\n",
    "\n",
    "        for i in range(n_attacker, nusers):\n",
    "\n",
    "            inputs = user_tr_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "            targets = user_tr_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "            outputs = fed_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            fed_model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            param_grad=[]\n",
    "            for param in fed_model.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "\n",
    "        malicious_grads = user_grads\n",
    "\n",
    "        if epoch_num in schedule:\n",
    "            for param_group in optimizer_fed.param_groups:\n",
    "                param_group['lr'] *= gamma\n",
    "                print('New learnin rate ', param_group['lr'])\n",
    "\n",
    "        if n_attacker > 0:\n",
    "            if at_type == 'lie':\n",
    "                mal_update = lie_attack(malicious_grads, z_values[n_attacker])\n",
    "                malicious_grads = torch.cat((torch.stack([mal_update]*n_attacker), malicious_grads))\n",
    "            elif at_type == 'fang':\n",
    "                agg_grads = torch.mean(malicious_grads, 0)\n",
    "                deviation = torch.sign(agg_grads)\n",
    "                malicious_grads = get_malicious_updates_fang_trmean(malicious_grads, deviation, n_attacker, epoch_num)\n",
    "            elif at_type == 'our-agr':\n",
    "                agg_grads = torch.mean(malicious_grads, 0)\n",
    "                malicious_grads = our_attack_median(malicious_grads, agg_grads, n_attacker, dev_type=dev_type)\n",
    "\n",
    "        if not epoch_num : \n",
    "            print(malicious_grads.shape)\n",
    "\n",
    "        if aggregation=='median':\n",
    "            agg_grads=torch.median(malicious_grads,dim=0)[0]\n",
    "\n",
    "        elif aggregation=='average':\n",
    "            agg_grads=torch.mean(malicious_grads,dim=0)\n",
    "\n",
    "        elif aggregation=='trmean':\n",
    "            agg_grads=tr_mean(malicious_grads, n_attacker)\n",
    "\n",
    "        elif aggregation=='krum' or aggregation=='mkrum':\n",
    "            multi_k = True if aggregation == 'mkrum' else False\n",
    "            if epoch_num == 0: print('multi krum is ', multi_k)\n",
    "            agg_grads, krum_candidate = multi_krum(malicious_grads, n_attacker, multi_k=multi_k)\n",
    "\n",
    "        elif aggregation=='bulyan':\n",
    "            agg_grads, krum_candidate=bulyan(malicious_grads, n_attacker)\n",
    "\n",
    "        del user_grads\n",
    "\n",
    "        start_idx=0\n",
    "\n",
    "        optimizer_fed.zero_grad()\n",
    "\n",
    "        model_grads=[]\n",
    "\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "\n",
    "        optimizer_fed.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if is_best:\n",
    "            best_global_te_acc = te_acc\n",
    "\n",
    "        if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "            print('%s: at %s n_at %d e %d fed_model val loss %.4f val acc %.4f best val_acc %f te_acc %f'%(aggregation, at_type, n_attacker, epoch_num, val_loss, val_acc, best_global_acc,best_global_te_acc))\n",
    "\n",
    "        if val_loss > 10:\n",
    "            print('val loss %f too high'%val_loss)\n",
    "            break\n",
    "\n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for our first AGR-agnostic attack - Min-max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MIN-MAX attack\n",
    "'''\n",
    "def our_attack_dist(all_updates, model_re, n_attackers, dev_type='unit_vec'):\n",
    "\n",
    "    if dev_type == 'unit_vec':\n",
    "        deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "    elif dev_type == 'sign':\n",
    "        deviation = torch.sign(model_re)\n",
    "    elif dev_type == 'std':\n",
    "        deviation = torch.std(all_updates, 0)\n",
    "\n",
    "    lamda = torch.Tensor([10.0]).float().cuda()\n",
    "    # print(lamda)\n",
    "    threshold_diff = 1e-5\n",
    "    lamda_fail = lamda\n",
    "    lamda_succ = 0\n",
    "    \n",
    "    distances = []\n",
    "    for update in all_updates:\n",
    "        distance = torch.norm((all_updates - update), dim=1) ** 2\n",
    "        distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)\n",
    "    \n",
    "    max_distance = torch.max(distances)\n",
    "    del distances\n",
    "\n",
    "    while torch.abs(lamda_succ - lamda) > threshold_diff:\n",
    "        mal_update = (model_re - lamda * deviation)\n",
    "        distance = torch.norm((all_updates - mal_update), dim=1) ** 2\n",
    "        max_d = torch.max(distance)\n",
    "        \n",
    "        if max_d <= max_distance:\n",
    "            # print('successful lamda is ', lamda)\n",
    "            lamda_succ = lamda\n",
    "            lamda = lamda + lamda_fail / 2\n",
    "        else:\n",
    "            lamda = lamda - lamda_fail / 2\n",
    "\n",
    "        lamda_fail = lamda_fail / 2\n",
    "\n",
    "    mal_update = (model_re - lamda_succ * deviation)\n",
    "    \n",
    "    return mal_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malicious_grads shape  torch.Size([50, 2472266])\n",
      "median: at min-max n_at 10 | e 0 fed_model val loss 2.3030 val acc 9.9432 best val_acc 9.943182 te_acc 9.253247\n",
      "median: at min-max n_at 10 | e 10 fed_model val loss 2.2905 val acc 12.6420 best val_acc 14.894481 te_acc 16.193182\n",
      "median: at min-max n_at 10 | e 20 fed_model val loss 2.2952 val acc 11.8912 best val_acc 19.805195 te_acc 19.947240\n",
      "median: at min-max n_at 10 | e 30 fed_model val loss 2.3011 val acc 12.7232 best val_acc 21.530032 te_acc 21.875000\n",
      "median: at min-max n_at 10 | e 40 fed_model val loss 2.1932 val acc 16.7208 best val_acc 21.530032 te_acc 21.875000\n",
      "median: at min-max n_at 10 | e 50 fed_model val loss 2.3039 val acc 10.8969 best val_acc 21.530032 te_acc 21.875000\n",
      "median: at min-max n_at 10 | e 60 fed_model val loss 2.2325 val acc 14.8133 best val_acc 21.530032 te_acc 21.875000\n",
      "median: at min-max n_at 10 | e 70 fed_model val loss 2.1430 val acc 18.7703 best val_acc 21.530032 te_acc 21.875000\n",
      "median: at min-max n_at 10 | e 80 fed_model val loss 2.1694 val acc 17.2078 best val_acc 21.530032 te_acc 21.875000\n",
      "median: at min-max n_at 10 | e 90 fed_model val loss 2.3117 val acc 9.4359 best val_acc 21.530032 te_acc 21.875000\n",
      "median: at min-max n_at 10 | e 100 fed_model val loss 2.3301 val acc 11.7695 best val_acc 21.530032 te_acc 21.875000\n",
      "median: at min-max n_at 10 | e 110 fed_model val loss 2.1379 val acc 17.6136 best val_acc 21.530032 te_acc 21.875000\n",
      "median: at min-max n_at 10 | e 120 fed_model val loss 2.2091 val acc 17.0455 best val_acc 21.530032 te_acc 21.875000\n",
      "median: at min-max n_at 10 | e 130 fed_model val loss 2.1618 val acc 18.9326 best val_acc 21.530032 te_acc 21.875000\n",
      "median: at min-max n_at 10 | e 140 fed_model val loss 2.2820 val acc 12.5203 best val_acc 21.530032 te_acc 21.875000\n",
      "median: at min-max n_at 10 | e 150 fed_model val loss 2.1813 val acc 17.0455 best val_acc 21.530032 te_acc 21.875000\n",
      "median: at min-max n_at 10 | e 160 fed_model val loss 2.1540 val acc 20.5154 best val_acc 21.530032 te_acc 21.875000\n",
      "median: at min-max n_at 10 | e 170 fed_model val loss 2.2403 val acc 13.7987 best val_acc 22.422890 te_acc 21.712662\n",
      "median: at min-max n_at 10 | e 180 fed_model val loss 2.1300 val acc 20.6778 best val_acc 22.422890 te_acc 21.712662\n",
      "median: at min-max n_at 10 | e 190 fed_model val loss 2.6202 val acc 10.0649 best val_acc 23.782468 te_acc 23.173701\n",
      "median: at min-max n_at 10 | e 200 fed_model val loss 2.3074 val acc 13.3726 best val_acc 23.782468 te_acc 23.173701\n",
      "median: at min-max n_at 10 | e 210 fed_model val loss 2.2184 val acc 17.7963 best val_acc 23.782468 te_acc 23.173701\n",
      "median: at min-max n_at 10 | e 220 fed_model val loss 2.1522 val acc 17.1672 best val_acc 23.782468 te_acc 23.173701\n",
      "median: at min-max n_at 10 | e 230 fed_model val loss 2.0810 val acc 20.0690 best val_acc 23.782468 te_acc 23.173701\n",
      "median: at min-max n_at 10 | e 240 fed_model val loss 2.0491 val acc 21.3068 best val_acc 23.782468 te_acc 23.173701\n",
      "median: at min-max n_at 10 | e 250 fed_model val loss 2.1786 val acc 18.5268 best val_acc 23.782468 te_acc 23.173701\n",
      "median: at min-max n_at 10 | e 260 fed_model val loss 2.0674 val acc 25.0000 best val_acc 25.000000 te_acc 23.762175\n",
      "median: at min-max n_at 10 | e 270 fed_model val loss 2.1462 val acc 17.3093 best val_acc 25.121753 te_acc 23.904221\n",
      "median: at min-max n_at 10 | e 280 fed_model val loss 2.1698 val acc 15.6250 best val_acc 25.121753 te_acc 23.904221\n",
      "median: at min-max n_at 10 | e 290 fed_model val loss 2.2169 val acc 16.3352 best val_acc 25.121753 te_acc 23.904221\n",
      "median: at min-max n_at 10 | e 300 fed_model val loss 2.5495 val acc 4.8701 best val_acc 25.121753 te_acc 23.904221\n",
      "median: at min-max n_at 10 | e 310 fed_model val loss 2.3063 val acc 9.6388 best val_acc 25.121753 te_acc 23.904221\n",
      "median: at min-max n_at 10 | e 320 fed_model val loss 2.2087 val acc 17.4513 best val_acc 25.121753 te_acc 23.904221\n",
      "median: at min-max n_at 10 | e 330 fed_model val loss 2.2663 val acc 12.5203 best val_acc 25.121753 te_acc 23.904221\n",
      "median: at min-max n_at 10 | e 340 fed_model val loss 2.1308 val acc 17.5933 best val_acc 25.121753 te_acc 23.904221\n",
      "median: at min-max n_at 10 | e 350 fed_model val loss 2.1521 val acc 17.0455 best val_acc 25.121753 te_acc 23.904221\n",
      "median: at min-max n_at 10 | e 360 fed_model val loss 2.1918 val acc 17.5528 best val_acc 25.121753 te_acc 23.904221\n",
      "median: at min-max n_at 10 | e 370 fed_model val loss 2.0680 val acc 22.2200 best val_acc 25.121753 te_acc 23.904221\n",
      "median: at min-max n_at 10 | e 380 fed_model val loss 2.2244 val acc 22.2200 best val_acc 25.121753 te_acc 23.904221\n",
      "median: at min-max n_at 10 | e 390 fed_model val loss 2.3459 val acc 9.7808 best val_acc 25.121753 te_acc 23.904221\n",
      "median: at min-max n_at 10 | e 400 fed_model val loss 2.3074 val acc 9.8823 best val_acc 25.121753 te_acc 23.904221\n",
      "median: at min-max n_at 10 | e 410 fed_model val loss 2.2832 val acc 9.6997 best val_acc 25.121753 te_acc 23.904221\n",
      "median: at min-max n_at 10 | e 420 fed_model val loss 2.1375 val acc 19.1964 best val_acc 25.121753 te_acc 23.904221\n",
      "median: at min-max n_at 10 | e 430 fed_model val loss 2.0886 val acc 23.6607 best val_acc 25.121753 te_acc 23.904221\n",
      "median: at min-max n_at 10 | e 440 fed_model val loss 2.3914 val acc 12.0942 best val_acc 25.121753 te_acc 23.904221\n",
      "median: at min-max n_at 10 | e 450 fed_model val loss 2.3014 val acc 15.1989 best val_acc 25.121753 te_acc 23.904221\n",
      "median: at min-max n_at 10 | e 460 fed_model val loss 2.0431 val acc 20.4951 best val_acc 25.121753 te_acc 23.904221\n",
      "median: at min-max n_at 10 | e 470 fed_model val loss 2.0138 val acc 24.1680 best val_acc 27.171266 te_acc 26.014610\n",
      "median: at min-max n_at 10 | e 480 fed_model val loss 2.1918 val acc 15.9497 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 490 fed_model val loss 2.0710 val acc 20.0893 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 500 fed_model val loss 2.0115 val acc 24.4318 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 510 fed_model val loss 2.0032 val acc 24.5739 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 520 fed_model val loss 2.4352 val acc 9.9026 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 530 fed_model val loss 2.3298 val acc 9.9229 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 540 fed_model val loss 2.3114 val acc 9.9432 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 550 fed_model val loss 2.3055 val acc 9.9229 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 560 fed_model val loss 2.3035 val acc 9.6794 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 570 fed_model val loss 2.3030 val acc 9.6794 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 580 fed_model val loss 2.2965 val acc 10.1258 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 590 fed_model val loss 2.2777 val acc 13.6364 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 600 fed_model val loss 2.1742 val acc 17.1063 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 610 fed_model val loss 2.1305 val acc 18.2021 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 620 fed_model val loss 2.3074 val acc 9.5373 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 630 fed_model val loss 2.2610 val acc 11.2622 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 640 fed_model val loss 2.9856 val acc 11.0998 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 650 fed_model val loss 2.1528 val acc 16.0106 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 660 fed_model val loss 2.3777 val acc 11.3839 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 670 fed_model val loss 2.0647 val acc 17.5731 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 680 fed_model val loss 2.1171 val acc 17.4716 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 690 fed_model val loss 2.3469 val acc 9.3344 best val_acc 28.226461 te_acc 27.232143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median: at min-max n_at 10 | e 700 fed_model val loss 2.2635 val acc 15.7265 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 710 fed_model val loss 2.2825 val acc 13.0885 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 720 fed_model val loss 2.1243 val acc 20.2719 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 730 fed_model val loss 2.1638 val acc 19.2776 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 740 fed_model val loss 2.1576 val acc 17.9586 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 750 fed_model val loss 2.0607 val acc 21.6112 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 760 fed_model val loss 2.0455 val acc 22.6258 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 770 fed_model val loss 2.1343 val acc 18.4659 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 780 fed_model val loss 2.1502 val acc 17.8571 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 790 fed_model val loss 2.0378 val acc 20.5966 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 800 fed_model val loss 2.3725 val acc 10.4302 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 810 fed_model val loss 2.3244 val acc 11.5666 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 820 fed_model val loss 2.3074 val acc 10.4099 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 830 fed_model val loss 2.3034 val acc 9.9432 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 840 fed_model val loss 2.3028 val acc 9.9432 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 850 fed_model val loss 2.3026 val acc 10.5925 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 860 fed_model val loss 2.3027 val acc 9.9432 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 870 fed_model val loss 2.3027 val acc 9.6388 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 880 fed_model val loss 2.3027 val acc 9.6388 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 890 fed_model val loss 2.3027 val acc 9.6388 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 900 fed_model val loss 2.3027 val acc 9.6388 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 910 fed_model val loss 2.3027 val acc 9.6388 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 920 fed_model val loss 2.3027 val acc 9.6388 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 930 fed_model val loss 2.3027 val acc 9.6388 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 940 fed_model val loss 2.3027 val acc 9.6388 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 950 fed_model val loss 2.3027 val acc 9.6388 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 960 fed_model val loss 2.3027 val acc 9.6388 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 970 fed_model val loss 2.3027 val acc 9.6388 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 980 fed_model val loss 2.3028 val acc 9.6388 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 990 fed_model val loss 2.3027 val acc 9.6388 best val_acc 28.226461 te_acc 27.232143\n",
      "New learnin rate  0.25\n",
      "median: at min-max n_at 10 | e 1000 fed_model val loss 2.3027 val acc 9.6388 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 1010 fed_model val loss 2.3027 val acc 9.6388 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 1020 fed_model val loss 2.3027 val acc 9.6388 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 1030 fed_model val loss 2.3027 val acc 9.6388 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 1040 fed_model val loss 2.3027 val acc 9.6388 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 1050 fed_model val loss 2.3027 val acc 9.6388 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 1060 fed_model val loss 2.3027 val acc 9.6388 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 1070 fed_model val loss 2.3027 val acc 9.6388 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 1080 fed_model val loss 2.3027 val acc 9.6388 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 1090 fed_model val loss 2.3027 val acc 9.6388 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 1100 fed_model val loss 2.3027 val acc 9.6388 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 1110 fed_model val loss 2.3027 val acc 9.6388 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 1120 fed_model val loss 2.3027 val acc 9.6388 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 1130 fed_model val loss 2.3027 val acc 9.6388 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 1140 fed_model val loss 2.3027 val acc 9.6388 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 1150 fed_model val loss 2.3027 val acc 9.6388 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 1160 fed_model val loss 2.3027 val acc 9.6388 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 1170 fed_model val loss 2.3027 val acc 9.6388 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 1180 fed_model val loss 2.3027 val acc 9.6388 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 1190 fed_model val loss 2.3027 val acc 9.6388 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 1199 fed_model val loss 2.3027 val acc 9.6388 best val_acc 28.226461 te_acc 27.232143\n",
      "median: at min-max n_at 10 | e 1200 fed_model val loss 2.3027 val acc 9.6388 best val_acc 28.226461 te_acc 27.232143\n"
     ]
    }
   ],
   "source": [
    "batch_size=250\n",
    "resume=0\n",
    "nepochs=1200\n",
    "schedule=[1000]\n",
    "nbatches = user_tr_len//batch_size\n",
    "\n",
    "gamma=.5\n",
    "opt = 'sgd'\n",
    "fed_lr=0.5\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "aggregation='median'\n",
    "multi_k = False\n",
    "candidates = []\n",
    "\n",
    "at_type='min-max'\n",
    "dev_type ='std'\n",
    "z=0\n",
    "n_attackers=[10]\n",
    "\n",
    "arch='alexnet'\n",
    "chkpt='./'+aggregation\n",
    "\n",
    "for n_attacker in n_attackers:\n",
    "    candidates = []\n",
    "\n",
    "    epoch_num = 0\n",
    "    best_global_acc = 0\n",
    "    best_global_te_acc = 0\n",
    "\n",
    "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
    "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    r=np.arange(user_tr_len)\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        user_grads=[]\n",
    "        if not epoch_num and epoch_num%nbatches == 0:\n",
    "            np.random.shuffle(r)\n",
    "            for i in range(nusers):\n",
    "                user_tr_data_tensors[i]=user_tr_data_tensors[i][r]\n",
    "                user_tr_label_tensors[i]=user_tr_label_tensors[i][r]\n",
    "\n",
    "        for i in range(n_attacker, nusers):\n",
    "\n",
    "            inputs = user_tr_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "            targets = user_tr_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "            outputs = fed_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            fed_model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            param_grad=[]\n",
    "            for param in fed_model.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "\n",
    "        malicious_grads = user_grads\n",
    "\n",
    "        if epoch_num in schedule:\n",
    "            for param_group in optimizer_fed.param_groups:\n",
    "                param_group['lr'] *= gamma\n",
    "                print('New learnin rate ', param_group['lr'])\n",
    "\n",
    "        if n_attacker > 0:\n",
    "            if at_type == 'lie':\n",
    "                malicious_grads = get_malicious_updates_lie(malicious_grads, n_attacker, z, epoch_num)\n",
    "            elif at_type == 'fang':\n",
    "                agg_grads = torch.mean(malicious_grads, 0)\n",
    "                deviation = torch.sign(agg_grads)\n",
    "                malicious_grads = get_malicious_updates_fang(malicious_grads, agg_grads, deviation, n_attacker)\n",
    "            elif at_type == 'our-agr':\n",
    "                agg_grads = torch.mean(malicious_grads, 0)\n",
    "                mal_update = our_attack_median(malicious_grads, agg_grads, n_attacker, dev_type)\n",
    "            elif at_type == 'min-max':\n",
    "                agg_grads = torch.mean(malicious_grads, 0)\n",
    "                mal_update = our_attack_dist(malicious_grads, agg_grads, n_attacker, dev_type)\n",
    "            elif at_type == 'min-sum':\n",
    "                agg_grads = torch.mean(malicious_grads, 0)\n",
    "                mal_update = our_attack_score(malicious_grads, agg_grads, n_attacker, dev_type)\n",
    "\n",
    "            mal_updates = torch.stack([mal_update] * n_attacker)\n",
    "            malicious_grads = torch.cat((mal_updates, user_grads), 0)\n",
    "\n",
    "        if epoch_num==0: print('malicious_grads shape ', malicious_grads.shape)\n",
    "\n",
    "        if aggregation=='median':\n",
    "            agg_grads=torch.median(malicious_grads,dim=0)[0]\n",
    "\n",
    "        elif aggregation=='average':\n",
    "            agg_grads=torch.mean(malicious_grads,dim=0)\n",
    "\n",
    "        elif aggregation=='krum' or aggregation=='mkrum':\n",
    "            multi_k = True if aggregation == 'mkrum' else False\n",
    "            if epoch_num == 0: print('multi krum is ', multi_k)\n",
    "            agg_grads, krum_candidate = multi_krum(malicious_grads, n_attacker, multi_k=multi_k)\n",
    "\n",
    "        elif aggregation=='bulyan':\n",
    "            agg_grads,bulyan_candidate=bulyan(malicious_grads, n_attacker)\n",
    "\n",
    "        del user_grads\n",
    "\n",
    "        start_idx=0\n",
    "\n",
    "        optimizer_fed.zero_grad()\n",
    "\n",
    "        model_grads=[]\n",
    "\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "\n",
    "        optimizer_fed.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if is_best:\n",
    "            best_global_te_acc = te_acc\n",
    "\n",
    "        if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "            print('%s: at %s n_at %d | e %d fed_model val loss %.4f val acc %.4f best val_acc %f te_acc %f'%(aggregation, at_type, n_attacker, epoch_num, val_loss, val_acc, best_global_acc,best_global_te_acc))\n",
    "\n",
    "        if val_loss > 1000:\n",
    "            print('val loss %f too high'%val_loss)\n",
    "            break\n",
    "            \n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for our second AGR-agnostic attack - Min-sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MIN-SUM attack\n",
    "'''\n",
    "\n",
    "def our_attack_score(all_updates, model_re, n_attackers, dev_type='unit_vec'):\n",
    "\n",
    "    if dev_type == 'unit_vec':\n",
    "        deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir\n",
    "    elif dev_type == 'sign':\n",
    "        deviation = torch.sign(model_re)\n",
    "    elif dev_type == 'std':\n",
    "        deviation = torch.std(all_updates, 0)\n",
    "    \n",
    "    lamda = torch.Tensor([10.0]).float().cuda()\n",
    "    # print(lamda)\n",
    "    threshold_diff = 1e-5\n",
    "    lamda_fail = lamda\n",
    "    lamda_succ = 0\n",
    "    \n",
    "    distances = []\n",
    "    for update in all_updates:\n",
    "        distance = torch.norm((all_updates - update), dim=1) ** 2\n",
    "        distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)\n",
    "    \n",
    "    scores = torch.sum(distances, dim=1)\n",
    "    min_score = torch.min(scores)\n",
    "    del distances\n",
    "\n",
    "    while torch.abs(lamda_succ - lamda) > threshold_diff:\n",
    "        mal_update = (model_re - lamda * deviation)\n",
    "        distance = torch.norm((all_updates - mal_update), dim=1) ** 2\n",
    "        score = torch.sum(distance)\n",
    "        \n",
    "        if score <= min_score:\n",
    "            # print('successful lamda is ', lamda)\n",
    "            lamda_succ = lamda\n",
    "            lamda = lamda + lamda_fail / 2\n",
    "        else:\n",
    "            lamda = lamda - lamda_fail / 2\n",
    "\n",
    "        lamda_fail = lamda_fail / 2\n",
    "\n",
    "    # print(lamda_succ)\n",
    "    mal_update = (model_re - lamda_succ * deviation)\n",
    "    \n",
    "    return mal_update\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malicious_grads shape  torch.Size([50, 2472266])\n",
      "median: at min-sum n_at 10 | e 0 fed_model val loss 2.3024 val acc 9.8011 best val_acc 9.801136 te_acc 10.491071\n",
      "median: at min-sum n_at 10 | e 10 fed_model val loss 2.2846 val acc 16.3758 best val_acc 18.790584 te_acc 19.419643\n",
      "median: at min-sum n_at 10 | e 20 fed_model val loss 2.3058 val acc 9.9432 best val_acc 22.321429 te_acc 21.672078\n",
      "median: at min-sum n_at 10 | e 30 fed_model val loss 2.2668 val acc 9.9432 best val_acc 22.321429 te_acc 21.672078\n",
      "median: at min-sum n_at 10 | e 40 fed_model val loss 2.2428 val acc 15.5438 best val_acc 22.321429 te_acc 21.672078\n",
      "median: at min-sum n_at 10 | e 50 fed_model val loss 2.2125 val acc 16.8222 best val_acc 22.321429 te_acc 21.672078\n",
      "median: at min-sum n_at 10 | e 60 fed_model val loss 2.1811 val acc 18.6485 best val_acc 22.321429 te_acc 21.672078\n",
      "median: at min-sum n_at 10 | e 70 fed_model val loss 2.2008 val acc 16.2946 best val_acc 22.321429 te_acc 21.672078\n",
      "median: at min-sum n_at 10 | e 80 fed_model val loss 2.2853 val acc 12.3174 best val_acc 22.321429 te_acc 21.672078\n",
      "median: at min-sum n_at 10 | e 90 fed_model val loss 2.3253 val acc 13.2914 best val_acc 22.564935 te_acc 21.590909\n",
      "median: at min-sum n_at 10 | e 100 fed_model val loss 2.1557 val acc 17.7151 best val_acc 22.564935 te_acc 21.590909\n",
      "median: at min-sum n_at 10 | e 110 fed_model val loss 2.0774 val acc 22.5244 best val_acc 22.564935 te_acc 21.590909\n",
      "median: at min-sum n_at 10 | e 120 fed_model val loss 2.2661 val acc 19.2370 best val_acc 23.579545 te_acc 22.747565\n",
      "median: at min-sum n_at 10 | e 130 fed_model val loss 2.1916 val acc 17.0657 best val_acc 23.579545 te_acc 22.747565\n",
      "median: at min-sum n_at 10 | e 140 fed_model val loss 2.1477 val acc 17.8977 best val_acc 23.924513 te_acc 23.336039\n",
      "median: at min-sum n_at 10 | e 150 fed_model val loss 2.0557 val acc 22.0982 best val_acc 24.634740 te_acc 23.153409\n",
      "median: at min-sum n_at 10 | e 160 fed_model val loss 2.0819 val acc 21.6518 best val_acc 24.675325 te_acc 24.391234\n",
      "median: at min-sum n_at 10 | e 170 fed_model val loss 2.0204 val acc 24.8985 best val_acc 27.029221 te_acc 26.765422\n",
      "median: at min-sum n_at 10 | e 180 fed_model val loss 2.0884 val acc 22.1388 best val_acc 27.029221 te_acc 26.765422\n",
      "median: at min-sum n_at 10 | e 190 fed_model val loss 2.0207 val acc 27.3133 best val_acc 27.313312 te_acc 26.826299\n",
      "median: at min-sum n_at 10 | e 200 fed_model val loss 2.0785 val acc 21.4489 best val_acc 28.591721 te_acc 27.556818\n",
      "median: at min-sum n_at 10 | e 210 fed_model val loss 2.5702 val acc 15.8279 best val_acc 28.591721 te_acc 27.556818\n",
      "median: at min-sum n_at 10 | e 220 fed_model val loss 2.8535 val acc 15.5844 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 230 fed_model val loss 2.3986 val acc 5.9659 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 240 fed_model val loss 2.3064 val acc 11.7492 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 250 fed_model val loss 2.2369 val acc 17.6542 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 260 fed_model val loss 2.2593 val acc 14.0828 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 270 fed_model val loss 2.1486 val acc 17.8369 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 280 fed_model val loss 2.0750 val acc 20.8198 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 290 fed_model val loss 2.1241 val acc 19.2979 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 300 fed_model val loss 2.1677 val acc 17.0455 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 310 fed_model val loss 2.0950 val acc 19.8458 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 320 fed_model val loss 2.0801 val acc 22.5852 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 330 fed_model val loss 2.0839 val acc 18.9935 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 340 fed_model val loss 2.1463 val acc 16.8628 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 350 fed_model val loss 2.0769 val acc 23.9854 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 360 fed_model val loss 1.9919 val acc 25.8929 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 370 fed_model val loss 2.0437 val acc 21.7938 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 380 fed_model val loss 2.4443 val acc 10.0649 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 390 fed_model val loss 2.3573 val acc 9.9026 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 400 fed_model val loss 2.3202 val acc 9.9026 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 410 fed_model val loss 2.3090 val acc 9.8011 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 420 fed_model val loss 2.3050 val acc 9.8011 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 430 fed_model val loss 2.3036 val acc 9.8011 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 440 fed_model val loss 2.3030 val acc 9.6388 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 450 fed_model val loss 2.3028 val acc 9.6388 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 460 fed_model val loss 2.3027 val acc 10.3693 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 470 fed_model val loss 2.3027 val acc 10.3693 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 480 fed_model val loss 2.3026 val acc 10.3693 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 490 fed_model val loss 2.3026 val acc 10.3693 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 500 fed_model val loss 2.3026 val acc 10.3693 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 510 fed_model val loss 2.3026 val acc 10.3693 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 520 fed_model val loss 2.3026 val acc 10.3693 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 530 fed_model val loss 2.3026 val acc 10.3693 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 540 fed_model val loss 2.3026 val acc 10.3693 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 550 fed_model val loss 2.3026 val acc 10.3693 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 560 fed_model val loss 2.3026 val acc 10.3693 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 570 fed_model val loss 2.3026 val acc 10.3693 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 580 fed_model val loss 2.3026 val acc 10.3693 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 590 fed_model val loss 2.3026 val acc 10.3693 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 600 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 610 fed_model val loss 2.3026 val acc 10.3693 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 620 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 630 fed_model val loss 2.3026 val acc 10.3693 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 640 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 650 fed_model val loss 2.3026 val acc 10.3693 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 660 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 670 fed_model val loss 2.3026 val acc 10.3693 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 680 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 690 fed_model val loss 2.3026 val acc 10.3693 best val_acc 28.733766 te_acc 27.658279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median: at min-sum n_at 10 | e 700 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 710 fed_model val loss 2.3026 val acc 10.3693 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 720 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 730 fed_model val loss 2.3026 val acc 10.3693 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 740 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 750 fed_model val loss 2.3026 val acc 10.3693 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 760 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 770 fed_model val loss 2.3026 val acc 10.3693 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 780 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 790 fed_model val loss 2.3026 val acc 10.3693 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 800 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 810 fed_model val loss 2.3026 val acc 10.3693 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 820 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 830 fed_model val loss 2.3026 val acc 10.3693 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 840 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 850 fed_model val loss 2.3026 val acc 10.3693 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 860 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 870 fed_model val loss 2.3026 val acc 10.3693 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 880 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 890 fed_model val loss 2.3026 val acc 10.3693 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 900 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 910 fed_model val loss 2.3026 val acc 10.3693 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 920 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 930 fed_model val loss 2.3026 val acc 10.3693 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 940 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 950 fed_model val loss 2.3026 val acc 10.3693 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 960 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 970 fed_model val loss 2.3026 val acc 10.3693 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 980 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 990 fed_model val loss 2.3026 val acc 10.3693 best val_acc 28.733766 te_acc 27.658279\n",
      "New learnin rate  0.25\n",
      "median: at min-sum n_at 10 | e 1000 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 1010 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 1020 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 1030 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 1040 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 1050 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 1060 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 1070 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 1080 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 1090 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 1100 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 1110 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 1120 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 1130 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 1140 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 1150 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 1160 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 1170 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 1180 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 1190 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 1199 fed_model val loss 2.3027 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n",
      "median: at min-sum n_at 10 | e 1200 fed_model val loss 2.3026 val acc 9.9432 best val_acc 28.733766 te_acc 27.658279\n"
     ]
    }
   ],
   "source": [
    "batch_size=250\n",
    "resume=0\n",
    "nepochs=1200\n",
    "schedule=[1000]\n",
    "nbatches = user_tr_len//batch_size\n",
    "\n",
    "gamma=.5\n",
    "opt = 'sgd'\n",
    "fed_lr=0.5\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "aggregation='median'\n",
    "\n",
    "at_type='min-sum'\n",
    "dev_type ='std'\n",
    "z=0\n",
    "n_attackers=[10]\n",
    "\n",
    "arch='alexnet'\n",
    "chkpt='./'+aggregation\n",
    "\n",
    "for n_attacker in n_attackers:\n",
    "    candidates = []\n",
    "\n",
    "    epoch_num = 0\n",
    "    best_global_acc = 0\n",
    "    best_global_te_acc = 0\n",
    "\n",
    "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
    "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    r=np.arange(user_tr_len)\n",
    "\n",
    "    while epoch_num <= nepochs:\n",
    "        user_grads=[]\n",
    "        if not epoch_num and epoch_num%nbatches == 0:\n",
    "            np.random.shuffle(r)\n",
    "            for i in range(nusers):\n",
    "                user_tr_data_tensors[i]=user_tr_data_tensors[i][r]\n",
    "                user_tr_label_tensors[i]=user_tr_label_tensors[i][r]\n",
    "\n",
    "        for i in range(n_attacker, nusers):\n",
    "\n",
    "            inputs = user_tr_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "            targets = user_tr_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "            outputs = fed_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            fed_model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            param_grad=[]\n",
    "            for param in fed_model.parameters():\n",
    "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
    "\n",
    "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
    "\n",
    "        malicious_grads = user_grads\n",
    "\n",
    "        if epoch_num in schedule:\n",
    "            for param_group in optimizer_fed.param_groups:\n",
    "                param_group['lr'] *= gamma\n",
    "                print('New learnin rate ', param_group['lr'])\n",
    "\n",
    "        if n_attacker > 0:\n",
    "            if at_type == 'lie':\n",
    "                malicious_grads = get_malicious_updates_lie(malicious_grads, n_attacker, z, epoch_num)\n",
    "            elif at_type == 'fang':\n",
    "                agg_grads = torch.mean(malicious_grads, 0)\n",
    "                deviation = torch.sign(agg_grads)\n",
    "                malicious_grads = get_malicious_updates_fang(malicious_grads, agg_grads, deviation, n_attacker)\n",
    "            elif at_type == 'our-agr':\n",
    "                agg_grads = torch.mean(malicious_grads, 0)\n",
    "                mal_update = our_attack_median(malicious_grads, agg_grads, n_attacker, dev_type)\n",
    "            elif at_type == 'min-max':\n",
    "                agg_grads = torch.mean(malicious_grads, 0)\n",
    "                mal_update = our_attack_dist(malicious_grads, agg_grads, n_attacker, dev_type)\n",
    "            elif at_type == 'min-sum':\n",
    "                agg_grads = torch.mean(malicious_grads, 0)\n",
    "                mal_update = our_attack_score(malicious_grads, agg_grads, n_attacker, dev_type)\n",
    "\n",
    "            mal_updates = torch.stack([mal_update] * n_attacker)\n",
    "            malicious_grads = torch.cat((mal_updates, user_grads), 0)\n",
    "\n",
    "        if epoch_num==0: print('malicious_grads shape ', malicious_grads.shape)\n",
    "\n",
    "        if aggregation=='median':\n",
    "            agg_grads=torch.median(malicious_grads,dim=0)[0]\n",
    "\n",
    "        elif aggregation=='average':\n",
    "            agg_grads=torch.mean(malicious_grads,dim=0)\n",
    "\n",
    "        elif aggregation=='krum' or aggregation=='mkrum':\n",
    "            multi_k = True if aggregation == 'mkrum' else False\n",
    "            if epoch_num == 0: print('multi krum is ', multi_k)\n",
    "            agg_grads, krum_candidate = multi_krum(malicious_grads, n_attacker, multi_k=multi_k)\n",
    "\n",
    "        elif aggregation=='bulyan':\n",
    "            agg_grads,bulyan_candidate=bulyan(malicious_grads, n_attacker)\n",
    "\n",
    "        del user_grads\n",
    "\n",
    "        start_idx=0\n",
    "\n",
    "        optimizer_fed.zero_grad()\n",
    "\n",
    "        model_grads=[]\n",
    "\n",
    "        for i, param in enumerate(fed_model.parameters()):\n",
    "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
    "            start_idx=start_idx+len(param.data.view(-1))\n",
    "            param_=param_.cuda()\n",
    "            model_grads.append(param_)\n",
    "\n",
    "        optimizer_fed.step(model_grads)\n",
    "\n",
    "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
    "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
    "\n",
    "        is_best = best_global_acc < val_acc\n",
    "\n",
    "        best_global_acc = max(best_global_acc, val_acc)\n",
    "\n",
    "        if is_best:\n",
    "            best_global_te_acc = te_acc\n",
    "\n",
    "        if epoch_num%10==0 or epoch_num==nepochs-1:\n",
    "            print('%s: at %s n_at %d | e %d fed_model val loss %.4f val acc %.4f best val_acc %f te_acc %f'%(aggregation, at_type, n_attacker, epoch_num, val_loss, val_acc, best_global_acc,best_global_te_acc))\n",
    "\n",
    "        if val_loss > 1000:\n",
    "            print('val loss %f too high'%val_loss)\n",
    "            break\n",
    "            \n",
    "        epoch_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
